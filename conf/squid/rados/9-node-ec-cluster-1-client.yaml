# Config is to be used for suits tier-3_rados_test_5+2_ecpools.yaml
# Config is to be used for suits tier-3_rados_test_6+2_ecpools.yaml
# This Ceph cluster configuration includes:
# - 3 monitors, 3 managers, 8 OSDs, 3 MDS, 3 RGWs,
# - 1 installer, 1 admin, 1 client, 4 node-exporters, and 5 crash daemons.
globals:
  - ceph-cluster:
      name: ceph
      node1:
        role:
          - osd
          - _admin
          - mon
          - mgr
          - installer
          - rgw
          - mds
        no-of-volumes: 3
        disk-size: 15

      node2:
        role:
          - osd
          - mon
          - mgr
          - mds
          - rgw
        no-of-volumes: 3
        disk-size: 15

      node3:
        role:
          - osd
          - mon
          - mgr
          - mds
          - rgw
        no-of-volumes: 3
        disk-size: 15

      node4:
        role:
          - osd
          - node-exporter
          - crash
        no-of-volumes: 3
        disk-size: 15

      node5:
        role:
          - osd
          - node-exporter
          - crash
        no-of-volumes: 3
        disk-size: 15

      node6:
        role:
          - osd
          - node-exporter
          - crash
        no-of-volumes: 3
        disk-size: 15

      node7:
        role:
          - client

      node8:
        role:
          - osd
          - node-exporter
          - crash
        no-of-volumes: 3
        disk-size: 15

      node9:
        role:
          - mon
          - mgr
          - rgw
          - osd
          - crash
        no-of-volumes: 3
        disk-size: 15

      node10:
        role:
          - osd-bak
          - node-exporter
          - crash
        no-of-volumes: 3
        disk-size: 15
