import random
import string
import time
import traceback

from ceph.ceph import CommandFailed
from tests.cephfs.cephfs_mirroring.cephfs_mirroring_utils import CephfsMirroringUtils
from tests.cephfs.cephfs_utilsV1 import FsUtils
from utility.log import Log

log = Log(__name__)


def run(ceph_cluster, **kw):
    """
    CEPH-83580026 - Validate multiple snapshots Synchronization on Mirror setup

    Scenarios Covered :
    Sceneario 1 :
        1. Enable Mirroring
        2. Create snapshot i.e, snap1 and validate the sync. it should have synced the snapshot snap1 to remote cluster
            and files should be present in snapshot
        3. Add more files and directories
        4. Create snapshot i.e snap2 and validate the sync.  it should have synced the snapshot snap2 to remote cluster
            and files should be present in snapshot
        5. Disable the mirroring
        6. Delete Snapshot snap1
        7. Enable Mirroring and create a snapshot with the same name as snap1
        8. As snap1 is already synced. we need to check the behaviour of the sync again works and get the new files

    Args:
        ceph_cluster: The Ceph cluster to perform the mirroring tests on.
        **kw: Additional keyword arguments.

    Returns:
        int: 0 if the test is successful, 1 if there's an error.

    Raises:
        Exception: Any unexpected exceptions that might occur during the test.
    """

    try:
        config = kw.get("config")
        ceph_cluster_dict = kw.get("ceph_cluster_dict")
        fs_util_ceph1 = FsUtils(ceph_cluster_dict.get("ceph1"))
        fs_util_ceph2 = FsUtils(ceph_cluster_dict.get("ceph2"))
        fs_mirroring_utils = CephfsMirroringUtils(
            ceph_cluster_dict.get("ceph1"), ceph_cluster_dict.get("ceph2")
        )
        build = config.get("build", config.get("rhbuild"))
        source_clients = ceph_cluster_dict.get("ceph1").get_ceph_objects("client")
        target_clients = ceph_cluster_dict.get("ceph2").get_ceph_objects("client")
        cephfs_mirror_node = ceph_cluster_dict.get("ceph1").get_ceph_objects(
            "cephfs-mirror"
        )

        log.info("checking Pre-requisites")
        if not source_clients or not target_clients:
            log.info(
                "This test requires a minimum of 1 client node on both ceph1 and ceph2."
            )
            return 1
        log.info("Preparing Clients...")
        fs_util_ceph1.prepare_clients(source_clients, build)
        fs_util_ceph2.prepare_clients(target_clients, build)
        fs_util_ceph1.auth_list(source_clients)
        fs_util_ceph2.auth_list(target_clients)
        source_fs = "cephfs_nw_1"
        target_fs = "cephfs_rem_1"
        target_user = "mirror_remote_nw"
        target_site_name = "remote_site_nw"
        fs_details = fs_util_ceph1.get_fs_info(source_clients[0], source_fs)
        if not fs_details:
            fs_util_ceph1.create_fs(client=source_clients[0], vol_name=source_fs)
            fs_util_ceph2.wait_for_mds_process(source_clients[0], source_fs)
        fs_details = fs_util_ceph2.get_fs_info(target_clients[0], target_fs)
        if not fs_details:
            fs_util_ceph2.create_fs(client=target_clients[0], vol_name=target_fs)
            fs_util_ceph2.wait_for_mds_process(target_clients[0], target_fs)
        log.info("Deploy CephFS Mirroring Configuration")
        token = fs_mirroring_utils.deploy_cephfs_mirroring(
            source_fs,
            source_clients[0],
            target_fs,
            target_clients[0],
            target_user,
            target_site_name,
        )
        log.info(token)

        log.info("Create Subvolumes for adding Data")
        subvolumegroup_list = [
            {"vol_name": source_fs, "group_name": "subvolgroup_1"},
        ]
        for subvolumegroup in subvolumegroup_list:
            fs_util_ceph1.create_subvolumegroup(source_clients[0], **subvolumegroup)

        subvolume_list = [
            {
                "vol_name": source_fs,
                "subvol_name": "subvol_1",
                "group_name": "subvolgroup_1",
                "size": "5368709120",
            },
            {
                "vol_name": source_fs,
                "subvol_name": "subvol_2",
                "group_name": "subvolgroup_1",
                "size": "5368709120",
            },
        ]
        for subvolume in subvolume_list:
            fs_util_ceph1.create_subvolume(source_clients[0], **subvolume)

        mounting_dir = "".join(
            random.choice(string.ascii_lowercase + string.digits)
            for _ in list(range(10))
        )
        log.info("Mount 1 subvolume on kernel and 1 subvloume on Fuse â†’ Client1")
        kernel_mounting_dir_1 = f"/mnt/cephfs_kernel{mounting_dir}_1/"
        mon_node_ips = fs_util_ceph1.get_mon_node_ips()
        log.info("Get the path of subvolume1 on  filesystem")
        subvol_path1, rc = source_clients[0].exec_command(
            sudo=True,
            cmd=f"ceph fs subvolume getpath {source_fs} subvol_1 subvolgroup_1",
        )
        index = subvol_path1.find("subvol_1/")
        if index != -1:
            subvol1_path = subvol_path1[: index + len("subvol_1/")]
        else:
            subvol1_path = subvol_path1
        log.info(subvol1_path)
        fs_util_ceph1.kernel_mount(
            [source_clients[0]],
            kernel_mounting_dir_1,
            ",".join(mon_node_ips),
            extra_params=f",fs={source_fs}",
        )
        fuse_mounting_dir_1 = f"/mnt/cephfs_fuse{mounting_dir}_1/"
        subvol_path2, rc = source_clients[0].exec_command(
            sudo=True,
            cmd=f"ceph fs subvolume getpath {source_fs} subvol_2 subvolgroup_1",
        )
        index = subvol_path2.find("subvol_2/")
        if index != -1:
            subvol2_path = subvol_path2[: index + len("subvol_2/")]
        else:
            subvol2_path = subvol_path2
        log.info(subvol2_path)
        fs_util_ceph1.fuse_mount(
            [source_clients[0]],
            fuse_mounting_dir_1,
            extra_params=f" --client_fs {source_fs}",
        )
        #
        log.info("Add subvolumes for mirroring to remote location")
        fs_mirroring_utils.add_path_for_mirroring(
            source_clients[0], source_fs, subvol1_path
        )
        fs_mirroring_utils.add_path_for_mirroring(
            source_clients[0], source_fs, subvol2_path
        )
        md5sum = "md5sum_script.sh"
        target_clients[0].upload_file(
            sudo=True,
            src="tests/cephfs/cephfs_mirror_upgrade/md5sum_script.sh",
            dst=f"/root/{md5sum}",
        )
        source_clients[0].upload_file(
            sudo=True,
            src="tests/cephfs/cephfs_mirror_upgrade/md5sum_script.sh",
            dst=f"/root/{md5sum}",
        )
        snap1 = "snap_k1_mul"
        snap2 = "snap_f1_mul"
        source_clients[0].exec_command(
            sudo=True,
            cmd=f"mkdir -p {kernel_mounting_dir_1}/{subvol1_path}/{mounting_dir};"
            f"python3 /home/cephuser/smallfile/smallfile_cli.py "
            f"--operation create --threads 1 --file-size 1 "
            f"--files 5 --files-per-dir 1 --dirs-per-dir 2 --top "
            f"{kernel_mounting_dir_1}/{subvol1_path}/{mounting_dir}",
        )

        source_clients[0].exec_command(
            sudo=True,
            cmd=f"mkdir {kernel_mounting_dir_1}{subvol1_path}.snap/{snap1}",
        )
        fsid = fs_mirroring_utils.get_fsid(cephfs_mirror_node[0])
        log.info(f"fsid on ceph cluster : {fsid}")
        daemon_names = fs_mirroring_utils.get_daemon_name(source_clients[0])

        asok_files = fs_mirroring_utils.get_asok_file(
            cephfs_mirror_node, fsid, daemon_names
        )
        log.info(f"Admin Socket file of cephfs-mirror daemon : {asok_files}")
        filesystem_id = fs_mirroring_utils.get_filesystem_id_by_name(
            source_clients[0], source_fs
        )
        log.info(f"filesystem id of {source_fs} is : {filesystem_id}")
        peer_uuid = fs_mirroring_utils.get_peer_uuid_by_name(
            source_clients[0], source_fs
        )
        log.info(f"peer uuid of {source_fs} is : {peer_uuid}")

        for node in cephfs_mirror_node:
            node.exec_command(sudo=True, cmd="yum install -y ceph-common --nogpgcheck")

        fs_mirroring_utils.validate_snapshot_sync_status(
            cephfs_mirror_node,
            source_fs,
            snap1,
            fsid,
            asok_files,
            filesystem_id,
            peer_uuid,
        )
        out, rc = fs_mirroring_utils.list_and_verify_remote_snapshots_and_data_checksum(
            target_clients[0],
            "client.admin",
            subvol1_path,
            source_clients[0],
            kernel_mounting_dir_1,
            target_fs,
        )
        if not out:
            raise CommandFailed(f"{rc}")

        source_clients[0].exec_command(
            sudo=True,
            cmd=f"mkdir -p {fuse_mounting_dir_1}/{subvol2_path}/{mounting_dir};"
            f"python3 /home/cephuser/smallfile/smallfile_cli.py "
            f"--operation create --threads 1 --file-size 1 "
            f"--files 5 --files-per-dir 1 --dirs-per-dir 2 --top "
            f"{fuse_mounting_dir_1}/{subvol2_path}/{mounting_dir}",
        )
        peer_uuid = fs_mirroring_utils.get_peer_uuid_by_name(
            source_clients[0], source_fs
        )
        source_clients[0].exec_command(
            sudo=True,
            cmd=f"mkdir {fuse_mounting_dir_1}{subvol2_path}.snap/{snap2}",
        )
        log.info(
            "=" * 50
            + "\nDisconnect the mirroring by removing the peer connection.\n"
            + "=" * 50
        )
        fs_mirroring_utils.remove_snapshot_mirror_peer(
            source_clients[0], source_fs, peer_uuid
        )
        out, _ = source_clients[0].exec_command(
            f"ceph fs snapshot mirror peer_list {source_fs} --format json-pretty"
        )
        log.info(out)
        if out:
            log.error("Peer list not removed")
        source_clients[0].exec_command(
            sudo=True,
            cmd=f"ceph fs subvolume snapshot rm {source_fs} subvol_1 {snap1} --group_name subvolgroup_1",
        )
        log.info("Import token again")
        log.info(token)
        fs_mirroring_utils.import_peer_bootstrap(source_fs, token, source_clients[0])
        max_duration_seconds = 5 * 60
        start_time = time.time()
        while True:
            peer_uuid_new = fs_mirroring_utils.get_peer_uuid_by_name(
                source_clients[0], source_fs
            )
            if peer_uuid == peer_uuid_new:
                log.info("Peer UUID has not changed even after new import ")
                time.sleep(30)

                # Check if the maximum duration has been reached
                elapsed_time = time.time() - start_time
                if elapsed_time >= max_duration_seconds:
                    log.info("Maximum duration reached. Breaking out of the loop.")
                    break
            else:
                break

        fs_mirroring_utils.validate_snapshot_sync_status(
            cephfs_mirror_node,
            source_fs,
            snap2,
            fsid,
            asok_files,
            filesystem_id,
            peer_uuid_new,
        )
        peer_uuid = peer_uuid_new
        out, rc = fs_mirroring_utils.list_and_verify_remote_snapshots_and_data_checksum(
            target_clients[0],
            "client.admin",
            subvol2_path,
            source_clients[0],
            fuse_mounting_dir_1,
            target_fs,
        )
        if not out:
            raise CommandFailed(f"{rc}")
        log.info(
            f"{snap2} was synced with out any errors even when mirror node bootstrap token is removed and added back"
        )
        source_clients[0].exec_command(
            sudo=True,
            cmd=f"mkdir -p {kernel_mounting_dir_1}/{subvol1_path}/{mounting_dir}/{mounting_dir}_1;"
            f"python3 /home/cephuser/smallfile/smallfile_cli.py "
            f"--operation create --threads 1 --file-size 1 "
            f"--files 5 --files-per-dir 1 --dirs-per-dir 2 --top "
            f"{kernel_mounting_dir_1}/{subvol1_path}/{mounting_dir}/{mounting_dir}_1",
        )
        source_clients[0].exec_command(
            sudo=True,
            cmd=f"mkdir -p {kernel_mounting_dir_1}/{subvol1_path}/{mounting_dir}_1;"
            f"python3 /home/cephuser/smallfile/smallfile_cli.py "
            f"--operation create --threads 1 --file-size 1 "
            f"--files 5 --files-per-dir 1 --dirs-per-dir 2 --top "
            f"{kernel_mounting_dir_1}/{subvol1_path}/{mounting_dir}_1",
        )

        source_clients[0].exec_command(
            sudo=True,
            cmd=f"ceph fs subvolume snapshot create {source_fs} subvol_1 {snap1} --group_name subvolgroup_1",
        )

        fs_mirroring_utils.validate_snapshot_sync_status(
            cephfs_mirror_node,
            source_fs,
            f"{snap1}",
            fsid,
            asok_files,
            filesystem_id,
            peer_uuid,
        )
        time.sleep(60)
        out, rc = fs_mirroring_utils.list_and_verify_remote_snapshots_and_data_checksum(
            target_clients[0],
            "client.admin",
            subvol1_path,
            source_clients[0],
            kernel_mounting_dir_1,
            target_fs,
        )
        if not out:
            raise CommandFailed(f"{rc}")
        log.info(
            f"{snap1} was synced with out any errors even when {snap1} was recreated on source cluster."
        )

        return 0
    except Exception as e:
        log.error(e)
        log.error(traceback.format_exc())
        return 1
    finally:
        log.info("Clean up the system")
        log.info("Delete the snapshots")
        snapshots_to_delete = [
            f"{kernel_mounting_dir_1}{subvol1_path}.snap/snap_k1*",
            f"{fuse_mounting_dir_1}{subvol2_path}.snap/snap_f1*",
        ]
        for snapshot_path in snapshots_to_delete:
            source_clients[0].exec_command(
                sudo=True, cmd=f"rmdir {snapshot_path}", check_ec=False
            )

        log.info("Unmount the paths")
        paths_to_unmount = [kernel_mounting_dir_1, fuse_mounting_dir_1]
        for path in paths_to_unmount:
            source_clients[0].exec_command(sudo=True, cmd=f"umount -l {path}")

        log.info("Remove paths used for mirroring")
        paths_to_remove = [subvol1_path, subvol2_path]
        for path in paths_to_remove:
            fs_mirroring_utils.remove_path_from_mirroring(
                source_clients[0], source_fs, path
            )

        log.info("Destroy CephFS Mirroring setup.")
        fs_mirroring_utils.destroy_cephfs_mirroring(
            source_fs,
            source_clients[0],
            target_fs,
            target_clients[0],
            target_user,
            peer_uuid,
        )

        log.info("Remove Subvolumes")
        for subvolume in subvolume_list:
            fs_util_ceph1.remove_subvolume(
                source_clients[0],
                **subvolume,
            )

        log.info("Remove Subvolume Group")
        for subvolumegroup in subvolumegroup_list:
            fs_util_ceph1.remove_subvolumegroup(source_clients[0], **subvolumegroup)

        log.info("Delete the mounted paths")
        source_clients[0].exec_command(sudo=True, cmd=f"rm -rf {kernel_mounting_dir_1}")
        source_clients[0].exec_command(sudo=True, cmd=f"rm -rf {fuse_mounting_dir_1}")
        for client in [source_clients[0], target_clients[0]]:
            client.exec_command(
                sudo=True,
                cmd="ceph config set mon mon_allow_pool_delete true",
                check_ec=False,
            )
        fs_util_ceph1.remove_fs(source_clients[0], source_fs, validate=False)
        fs_util_ceph1.remove_fs(target_clients[0], target_fs, validate=False)
