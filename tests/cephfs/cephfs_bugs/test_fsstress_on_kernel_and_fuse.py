import random
import string
import time
import traceback

from tests.cephfs.cephfs_utilsV1 import FsUtils as FsUtilsV1
from tests.cephfs.lib.cephfs_common_lib import CephFSCommonUtils
from utility.log import Log

log = Log(__name__)


def run(ceph_cluster, **kw):
    """
    CEPH-83575623 - Run fsstress.sh repeatedly on fuse and kernel clients for a while and validate no crash is seen.

    Procedure :
    1. Create 2 Subvolumes - each mounted on kernel and fuse.
    2. Run fsstress for few iterations(10).
    3. Check for Ceph Health Status to validate if there is no Health Warn or Health Error status.

    Clean-up:
    1. Remove files from mountpoint, Unmount subvolumes.
    3. ceph fs subvolume rm <vol_name> <subvol_name> [--group_name <subvol_group_name>]
    4. ceph fs subvolumegroup rm <vol_name> <group_name>
    """

    try:
        tc = "CEPH-83575623"
        log.info(f"Running cephfs {tc} test case")
        test_data = kw.get("test_data")
        fs_util_v1 = FsUtilsV1(ceph_cluster, test_data=test_data)
        cephfs_common_utils = CephFSCommonUtils(ceph_cluster)
        erasure = (
            FsUtilsV1.get_custom_config_value(test_data, "erasure")
            if test_data
            else False
        )
        config = kw.get("config")
        build = config.get("build", config.get("rhbuild"))
        clients = ceph_cluster.get_ceph_objects("client")
        log.info("checking Pre-requisites")
        if len(clients) < 1:
            log.error(
                f"This test requires minimum 1 client nodes.This has only {len(clients)} clients"
            )
            return 1
        log.info("Installing required packages for make")
        clients[0].exec_command(
            sudo=True, cmd='dnf groupinstall "Development Tools" -y'
        )
        fs_util_v1.prepare_clients(clients, build)
        fs_util_v1.auth_list(clients)
        default_fs = "cephfs" if not erasure else "cephfs-ec"
        # if "cephfs" does not exsit, create it
        fs_details = fs_util_v1.get_fs_info(clients[0], default_fs)
        if not fs_details:
            fs_util_v1.create_fs(clients[0], default_fs)
        subvolume_group_name = "subvol_group1"
        subvolume_name = "subvol"
        subvolumegroup = {
            "vol_name": default_fs,
            "group_name": subvolume_group_name,
        }
        fs_util_v1.create_subvolumegroup(clients[0], **subvolumegroup)
        subvolume_list = [
            {
                "vol_name": default_fs,
                "subvol_name": f"{subvolume_name}_1",
                "group_name": subvolume_group_name,
            },
            {
                "vol_name": default_fs,
                "subvol_name": f"{subvolume_name}_2",
                "group_name": subvolume_group_name,
            },
        ]
        for subvolume in subvolume_list:
            fs_util_v1.create_subvolume(clients[0], **subvolume)

        mounting_dir = "".join(
            random.choice(string.ascii_lowercase + string.digits)
            for _ in list(range(10))
        )
        log.info("Mount 1 subvolume on kernel and 1 subvolume on Fuse â†’ Client1")

        kernel_mounting_dir_1 = f"/mnt/cephfs_kernel{mounting_dir}_1/"
        mon_node_ips = fs_util_v1.get_mon_node_ips()
        subvol_path_kernel, rc = clients[0].exec_command(
            sudo=True,
            cmd=f"ceph fs subvolume getpath {default_fs} {subvolume_name}_1 {subvolume_group_name}",
        )
        fs_util_v1.kernel_mount(
            [clients[0]],
            kernel_mounting_dir_1,
            ",".join(mon_node_ips),
            sub_dir=f"{subvol_path_kernel.strip()}",
            extra_params=f",fs={default_fs}",
        )

        fuse_mounting_dir_1 = f"/mnt/cephfs_fuse{mounting_dir}_1/"
        subvol_path_fuse, rc = clients[0].exec_command(
            sudo=True,
            cmd=f"ceph fs subvolume getpath {default_fs} {subvolume_name}_2 {subvolume_group_name}",
        )
        fs_util_v1.fuse_mount(
            [clients[0]],
            fuse_mounting_dir_1,
            extra_params=f" -r {subvol_path_fuse.strip()} --client_fs {default_fs}",
        )

        log.info("Verify Cluster is healthy before test")
        if cephfs_common_utils.wait_for_healthy_ceph(clients[0], 300):
            log.error("Cluster health is not OK even after waiting for 300secs")
            return 1

        log.info("Run fsstress for few iterations on fuse and kernel mounts.")
        fsstress_url = "https://raw.githubusercontent.com/ceph/ceph/main/qa/workunits/suites/fsstress.sh"

        def run_commands(client, commands):
            for command in commands:
                client.exec_command(sudo=True, cmd=command)

        directories = [kernel_mounting_dir_1, fuse_mounting_dir_1]
        for directory in directories:
            commands = [
                f"mkdir -p {directory}fsstress/",
                f"cd {directory}fsstress/ && wget {fsstress_url}",
                f"chmod 777 {directory}fsstress/fsstress.sh",
            ]
            run_commands(clients[0], commands)
        iterations = 50
        log.info(
            f"run fsstress on kernel and fuse in parallel for {iterations} iterations"
        )
        for i in range(iterations):
            log.info("Iteration: {}".format(i + 1))
            for directory in [kernel_mounting_dir_1, fuse_mounting_dir_1]:
                clients[0].exec_command(
                    sudo=True, cmd=f"sh {directory}fsstress/fsstress.sh"
                )
                time.sleep(10)
                clients[0].exec_command(
                    sudo=True, cmd=f"rm -rf {directory}fsstress/fsstress/"
                )
            time.sleep(10)
            # Just capturing the health status and not failing it
            fs_util_v1.get_ceph_health_status(clients[0], validate=False)

        log.info(
            "Check for the Ceph Health to see if there are any deadlock bw unlink and rename."
        )
        if cephfs_common_utils.wait_for_healthy_ceph(clients[0], 300):
            log.error("Cluster health is not OK even after waiting for 300secs")
            return 1
        return 0
    except Exception as e:
        log.error(e)
        log.error(traceback.format_exc())
        return 1
    finally:
        log.error("Clean up the system")
        fs_util_v1.client_clean_up(
            "umount", kernel_clients=[clients[0]], mounting_dir=kernel_mounting_dir_1
        )

        fs_util_v1.client_clean_up(
            "umount", fuse_clients=[clients[0]], mounting_dir=fuse_mounting_dir_1
        )

        for subvolume in subvolume_list:
            fs_util_v1.remove_subvolume(clients[0], **subvolume)

        fs_util_v1.remove_subvolumegroup(
            clients[0], default_fs, subvolume_group_name, force=True
        )
