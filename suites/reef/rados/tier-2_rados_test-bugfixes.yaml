# Suite contains  tier-2 rados bug verification automation
#===============================================================================================
#------------------------------------------------------------------------------------------
#----- Tier-2 - Bug verification  automation   ------
#------------------------------------------------------------------------------------------
# Conf: conf/reef/rados/7-node-cluster.yaml
# Bugs:
#     1. https://bugzilla.redhat.com/show_bug.cgi?id=2233800
#     2. https://bugzilla.redhat.com/show_bug.cgi?id=1900127
#     3. https://bugzilla.redhat.com/show_bug.cgi?id=2229651
#     4. https://bugzilla.redhat.com/show_bug.cgi?id=2011756
#     5. https://bugzilla.redhat.com/show_bug.cgi?id=2264053
#     6. https://bugzilla.redhat.com/show_bug.cgi?id=2264054
#     7. https://bugzilla.redhat.com/show_bug.cgi?id=2264052
#     8. https://bugzilla.redhat.com/show_bug.cgi?id=2260306
#     9. https://bugzilla.redhat.com/show_bug.cgi?id=2237038
#     10.https://bugzilla.redhat.com/show_bug.cgi?id=1892173
#     11.https://bugzilla.redhat.com/show_bug.cgi?id=2174954
#     12.https://bugzilla.redhat.com/show_bug.cgi?id=2151501
#     13.https://bugzilla.redhat.com/show_bug.cgi?id=2315595
#     14.https://bugzilla.redhat.com/show_bug.cgi?id=2326179
#     15.https://bugzilla.redhat.com/show_bug.cgi?id=2213766
#===============================================================================================
tests:
  - test:
      name: setup install pre-requisistes
      desc: Setup phase to deploy the required pre-requisites for running the tests.
      module: install_prereq.py
      abort-on-fail: true

  - test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                registry-url: registry.redhat.io
                mon-ip: node1
                allow-fqdn-hostname: true
                orphan-initial-daemons: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: shell
              args:          # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
          - config:
              command: apply
              service: mds
              base_cmd_args:          # arguments to ceph orch
                verbose: true
              pos_args:
                - cephfs              # name of the filesystem
              args:
                placement:
                  nodes:
                    - node2
                    - node6
                  limit: 2            # no of daemons
                  sep: " "            # separator to be used for placements
      destroy-cluster: false
      abort-on-fail: true

  - test:
      name: Configure client admin
      desc: Configures client admin node on cluster
      module: test_client.py
      polarion-id:
      config:
        command: add
        id: client.1                      # client Id (<type>.<Id>)
        node: node7                       # client node
        install_packages:
          - ceph-common
        copy_admin_keyring: true          # Copy admin keyring to node
        caps:                             # authorize client capabilities
          mon: "allow *"
          osd: "allow *"
          mds: "allow *"
          mgr: "allow *"

  - test:
      name: Enable logging to file
      module: rados_prep.py
      config:
        log_to_file: true
      desc: Change config options to enable logging to file

  - test:
      name: Verification of the Mon ballooning
      module: test_rados_ballooning_client.py
      polarion-id: CEPH-83584004
      desc: Verify MON ballooning client

# moved prior to Inconsistent object test due to chances of an OSD being down
  - test:
      name: OSD restart when bluefs_shared_alloc_size is lower than bluestore_min_alloc_size
      module: test_bug_fixes.py
      config:
        lower_bluefs_shared_alloc_size: true
      polarion-id: CEPH-83591092
      desc: verify OSD resiliency when 'bluefs_shared_alloc_size' is below 'bluestore_min_alloc_size'

  - test:
      name: Inconsistent objects in  EC pool functionality check
      desc: Scub and deep-scrub on  inconsistent objects in EC pool
      module: test_osd_ecpool_inconsistency_scenario.py
      polarion-id: CEPH-83586175
      config:
        ec_pool:
          create: true
          pool_name: ecpool
          pg_num: 1
          k: 2
          m: 2
          plugin: jerasure
          disable_pg_autoscale: true
        inconsistent_obj_count: 4
        debug_enable: False
        delete_pool:
          - ecpool
      comments:  Active bugs 2277111,2335727 and 2335762

  - test:
      name: Inconsistent objects in replicated pool functionality check
      desc: Scub and deep-scrub on  inconsistent objects in Replicated pool
      module: test_osd_replicated_inconsistency_scenario.py
      polarion-id: CEPH-83586175
      config:
        replicated_pool:
          create: true
          pool_name: replicated_pool
          pg_num: 1
          disable_pg_autoscale: true
        inconsistent_obj_count: 4
        debug_enable: False
        delete_pool:
          - replicated_pool
      comments: Active bug 2316244

  - test:
      name: Verification of ceph config show & get
      module: test_bug_fixes.py
      config:
        test-config-show-get: true
      polarion-id: CEPH-83590689
      desc: Verify ceph config show & get outputs

  #Moved to the prior to the slow OSD heartbeat test
  - test:
      name: Verification the recovery flag functionality
      desc: Verify the pg_num and recovery status
      module: test_norecovery_flag_functionality.py
      polarion-id: CEPH-83591783
      config:
        replicated_pool:
          create: true
          pool_name: recovery_pool
      delete_pool:
        - recovery_pool

  - test:
      name: Verification of Slow OSD heartbeat
      module: test_bug_fixes.py
      config:
        slow-osd-heartbeat: true
      polarion-id: CEPH-83590688
      desc: Generate Slow OSD heartbeats by inducing network delay

  - test:
      name: Client connection over v1 port
      module: test_v1client.py
      polarion-id: CEPH-83594645
      desc: Ensure client connection over v1 port does not crash

  # Below test is currently failing, BZ raised #2315596
  # will be un-commented once fix is available
  # - test:
  #     name: Mon connection scores testing
  #     desc: Verification of mon connection scores in different scenarios
  #     module: test_mon_connection_scores.py
  #     polarion-id: CEPH-83602911
  #     comments: Active bug 2151501

  - test:
      name: Verify config values via cephadm-ansible
      module: test_cephadm_ansible.py
      polarion-id: CEPH-83602686
      desc: Ensure configuration values are accessible via cephadm-ansible module
