# Test suite containing all generic rados test cases


tests:
  - test:
      name: setup install pre-requisistes
      desc: Setup phase to deploy the required pre-requisites for running the tests.
      module: install_prereq.py
      abort-on-fail: true

  - test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                rhcs-version: 6.1
                release: rc
                mon-ip: node1
                registry-url: registry.redhat.io
                allow-fqdn-hostname: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: shell
              args:          # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
          - config:
              command: apply
              service: mds
              base_cmd_args:          # arguments to ceph orch
                verbose: true
              pos_args:
                - cephfs              # name of the filesystem
              args:
                placement:
                  nodes:
                    - node2
                    - node6
                  limit: 2            # no of daemons
                  sep: " "            # separator to be used for placements
      destroy-cluster: false
      abort-on-fail: true

  - test:
      name: Configure client admin
      desc: Configures client admin node on cluster
      module: test_client.py
      polarion-id:
      config:
        command: add
        id: client.1                      # client Id (<type>.<Id>)
        node: node7                       # client node
        install_packages:
          - ceph-common
          - ceph-base
        copy_admin_keyring: true          # Copy admin keyring to node
        caps:                             # authorize client capabilities
          mon: "allow *"
          osd: "allow *"
          mds: "allow *"
          mgr: "allow *"
      abort-on-fail: true

  - test:
      name: Enable logging to file
      module: rados_prep.py
      config:
        log_to_file: true
      desc: Change config options to enable logging to file

# 0. All components basic sanity tests

  - test:
      name: rbd-io
      module: rbd_faster_exports.py
      config:
        rep-pool-only: True
        rep_pool_config:
          pool: rbd_rep_pool
          image: rbd_rep_image
          size: 10G
        io-total: 100M
      desc: Perform export during read/write,resizing,flattening,lock operations

  - test:
      name: rgw sanity tests
      module: sanity_rgw.py
      config:
          script-name: test_multitenant_user_access.py
          config-file-name: test_multitenant_access.yaml
          timeout: 300
      desc: Perform rgw tests

  - test:
      abort-on-fail: false
      desc: "cephfs basic operations"
      module: cephfs_basic_tests.py
      name: cephfs-basics
      polarion-id: "CEPH-11293"

  - test:
      name: nfs-ganesha_with_cephfs
      module: nfs-ganesha_basics.py
      desc: Configure nfs-ganesha on nfs server,do mount on any client and do IOs
      polarion-id: CEPH-83574439
      abort-on-fail: false

# Upgrade to latest
  - test:
      name: Upgrade ceph cluster
      desc: Upgrade cluster to latest version and check health warn
      module: test_upgrade_warn.py
      polarion-id: CEPH-83574934

# Starting with RADOS basic tests
# 1. Configuration tests

  - test:
      name: Configure email alerts
      module: rados_prep.py
      polarion-id: CEPH-83574472
      config:
        email_alerts:
          smtp_host: smtp.corp.redhat.com
          smtp_sender: ceph-iad2-c01-lab.mgr@redhat.com
          smtp_port: 25
          interval: 10
          smtp_destination:
            - pdhiran@redhat.com
          smtp_from_name: Rados Sanity Cluster Alerts
      desc: Configure email alerts on ceph cluster

  - test:
      name: Test configuration Assimilation
      module: test_config_assimilation.py
      polarion-id: CEPH-83573480
      comments: RFE - 2277761
      config:
        cluster_conf_path: "conf/reef/rados/test-confs/cluster-configs"
        Verify_config_parameters:
          configurations:
            - config-1:
                section: "mon"
                name: "mon_cluster_log_to_syslog"
                value: "true"
            - config-2:
                section: "osd"
                name: "debug_osd"
                value: "5/5"
            - config-3:
                section: "mgr"
                name: "mgr_stats_period"
                value: "10"
            - config-4:
                section: "mgr"
                name: "debug_mgr"
                value: "5/5"
            - config-5:
                section: "mds"
                name: "mds_op_history_size"
                value: "40"
      desc: Verify config assimilation into ceph mon configuration database

  - test:
      name: Monitor configuration - section and masks changes
      module: rados_prep.py
      polarion-id: CEPH-83573477
      config:
        Verify_config_parameters:
          configurations:
            - config-1:
                section: "osd"
                name: "osd_max_backfills"
                value: "8"
                location_type: "class"
                location_value: "hdd"
            - config-2:
                section: "osd"
                name: "osd_recovery_max_active"
                value: "8"
                location_type: "host"
                location_value: "host"
            - config-3:
                section: "global"
                name: "debug_mgr"
                value: "10/10"
            - config-4:
                section: "osd"
                name: "osd_max_scrubs"
                value: "5"
            - config-5:
                section: "osd.1"
                name: "osd_max_scrubs"
                value: "3"
            - config-6:
                section: "mds"
                name: "mds_op_history_size"
                value: "40"
            - config-7:
                section: "client.rgw"
                name: "rgw_lc_debug_interval"
                value: "1"
            - config-8:
                section: "global"
                name: "debug_mgr"
                value: "10/10"
            - config-9:
                section: "osd.2"
                name: "debug_ms"
                value: "10/10"
      desc: Verify config changes for section & masks like device class, host etc

  - test:
      name: Monitor configuration - msgrv2 compression modes
      module: rados_prep.py
      polarion-id: CEPH-83574961
      config:
        Verify_config_parameters:
          configurations:
            - config-1:
                section: "mon"
                name: "ms_osd_compress_mode"
                value: "force"
            - config-2:
                section: "mon"
                name: "ms_osd_compress_min_size"
                value: "512"
            - config-3:
                section: "mon"
                name: "ms_osd_compress_mode"
                value: "none"
            - config-4:
                section: "mon"
                name: "ms_osd_compress_min_size"
                value: "1024"
      desc: Verify the health status of the cluster by randomly changing the compression configuration values

  - test:
      name: Config checks
      module: rados_prep.py
      polarion-id: CEPH-83574529
      config:
        cluster_configuration_checks:
          configure: true
          disable_check_list:
            - osd_mtu_size
            - osd_linkspeed
            - kernel_security
          enable_check_list:
            - kernel_security
            - osd_linkspeed
      desc: Enable Cluster Configuration checks

  - test:
      name: config source changes log
      module: test_mon_config_history.py
      polarion-id: CEPH-83573479
      desc: Config sources - Verify config source changes in the log

  - test:
      name: config source changes reset
      module: test_mon_config_reset.py
      polarion-id: CEPH-83573478
      desc: Config sources - Verify config source changes and reset config

  - test:
      name: Verification of ceph config show & get
      module: test_bug_fixes.py
      config:
        test-config-show-get: true
      polarion-id: CEPH-83590689
      desc: Verify ceph config show & get outputs

  - test:
      name: osd_memory_target param set at OSD level
      module: test_osd_memory_target.py
      desc: Verification of osd_memory_target parameter set at OSD level
      polarion-id: CEPH-83580882
      config:
        osd_level: true

  - test:
      name: osd_memory_target param set at host level
      module: test_osd_memory_target.py
      desc: Verification of osd_memory_target parameter set at host level
      polarion-id: CEPH-83580881
      config:
        host_level: true


# 2. Replicated pool tests
  - test:
      name: Replicated pool LC
      module: rados_prep.py
      polarion-id: CEPH-83571632
      config:
        replicated_pool:
          create: true
          pool_name: test_re_pool
          pg_num: 16
          size: 2
          disable_pg_autoscale: true
          rados_write_duration: 50
          rados_read_duration: 30
        set_pool_configs:
          pool_name: test_re_pool
          configurations:
            pg_num: 32
            pgp_num: 32
            pg_autoscale_mode: 'on'
            compression_mode: aggressive
            compression_algorithm: zlib
        delete_pools:
          - test_re_pool
      desc: Create replicated pools and run IO

  - test:
      name: Compression algorithms
      module: rados_prep.py
      polarion-id: CEPH-83571669
      config:
        replicated_pool:
          create: true
          pool_name: re_pool_compress
          pg_num: 32
          rados_write_duration: 10
          rados_read_duration: 10
        enable_compression:
          pool_name: re_pool_compress
          rados_write_duration: 50
          rados_read_duration: 50
          configurations:
            - config-1:
                compression_mode: force
                compression_algorithm: snappy
                compression_required_ratio: 0.3
                compression_min_blob_size: 1B
                byte_size: 10KB
            - config-2:
                compression_mode: passive
                compression_algorithm: zlib
                compression_required_ratio: 0.7
                compression_min_blob_size: 10B
                byte_size: 100KB
            - config-3:
                compression_mode: aggressive
                compression_algorithm: zstd
                compression_required_ratio: 0.5
                compression_min_blob_size: 1KB
                byte_size: 100KB
        delete_pools:
          - re_pool_compress
      desc: Enable/disable different compression algorithms.

  - test:
      name: Ceph PG Autoscaler
      module: rados_prep.py
      polarion-id: CEPH-83573412
      config:
        replicated_pool:
          create: true
          pool_name: rep_test_pool
          rados_write_duration: 50
          rados_read_duration: 50
          pg_num: 32
        configure_pg_autoscaler:
          default_mode: warn
          mon_target_pg_per_osd: 128
          pool_config:
            pool_name: rep_test_pool
            pg_autoscale_mode: "on"
            pg_num_min: 16
            target_size_ratio: 0.4
        delete_pools:
          - rep_test_pool
      desc: Ceph PG autoscaler CLI validation

  - test:
      name: autoscaler flags
      module: test_pg_autoscale_flag.py
      polarion-id: CEPH-83574794
      config:
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
        create_re_pool: true
        create_ec_pool: false
      desc: verify autoscaler flags functionality
      comments: active Bugs 2252788, 2283358

  - test:
      name: Decreasing the replica count
      module: rados_prep.py
      polarion-id: CEPH-9261
      config:
        replicated_pool:
          create: true
          pool_name: test_re_pool-1
          pg_num: 16
          size: 3
          rados_write_duration: 100
          rados_read_duration: 50
        set_pool_configs:
          pool_name: test_re_pool-1
          configurations:
            size: 2
            pg_num: 32
            pgp_num: 32
      desc: Decreasing the replica count of the pool and observe the behaviour

  - test:
      name: Compression test - replicated pool
      module: pool_tests.py
      polarion-id: CEPH-83571673
      config:
        Compression_tests:
          verify_compression_ratio_set: true          # TC : CEPH-83571672
          pool_type: replicated
          pool_config:
            pool-1: test_compression_repool-1
            pool-2: test_compression_repool-2
            rados_write_duration: 200
            byte_size: 400KB
            pg_num: 32
          compression_config:
            compression_mode: aggressive
            compression_algorithm: snappy
            compression_required_ratio: 0.6
            compression_min_blob_size: 1B
            byte_size: 10KB
      desc: Verification of the effect of compression on replicated pools

  - test:
      name: Verify degraded pool behaviour at min_size
      module: pool_tests.py
      polarion-id: CEPH-9185
      config:
        Verify_degraded_pool_min_size_behaviour:
          pool_config:
            pool-1:
              pool_type: replicated
              pool_name: pool_degraded_test
              pg_num: 1
              disable_pg_autoscale: true
      desc: On a degraded cluster verify that clients can read and write data into pools with min_size OSDs
      abort-on-fail: false

  - test:
      name: Inconsistent object pg check
      desc: Inconsistent object pg check
      module: test_osd_inconsistency_pg.py
      polarion-id: CEPH-9924
      config:
        verify_osd_omap_entries:
          configurations:
            pool-1:
              pool_name: Inconsistent_pool
              pool_type: replicated
              pg_num: 1
          omap_config:
            obj_start: 0
            obj_end: 5
            num_keys_obj: 10
        delete_pool: true

  - test:
      name: Inconsistent object pg check using pool snapshot for RE pools
      desc: Inconsistent object pg check using pool snapshot for RE pools
      module: test_osd_snap_inconsistency_pg.py
      polarion-id: CEPH-9942
      config:
        verify_osd_omap_entries:
          configurations:
            pool-1:
              pool_name: Inconsistent_snap_pool_re
              pool_type: replicated
              pg_num: 1
          omap_config:
            obj_start: 0
            obj_end: 5
            num_keys_obj: 10
        delete_pool: true

  - test:
      name: Inconsistent object secondary pg check using pool snapshot
      desc: Inconsistent object pg check using pool snapshot for RE pools for secondary OSD in PG
      module: test_osd_snap_inconsistency_pg.py
      polarion-id: CEPH-83571452
      config:
        test_secondary: true
        verify_osd_omap_entries:
          configurations:
            pool-1:
              pool_name: Inconsistent_snap_pool
              pool_type: replicated
              pg_num: 1
          omap_config:
            obj_start: 0
            obj_end: 5
            num_keys_obj: 10
        delete_pool: true

# 3. EC pool tests

  - test:
      name: autoscaler flags
      module: test_pg_autoscale_flag.py
      polarion-id: CEPH-83574794
      config:
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
        create_ec_pool: true
        create_re_pool: false
      desc: verify autoscaler flags functionality
      comments: active Bugs 2252788, 2283358

  - test:
      name: EC Pool Recovery Improvement
      module: pool_tests.py
      polarion-id: CEPH-83573852
      config:
        ec_pool_recovery_improvement:
          create: true
          pool_name: ec_pool_recovery
          k: 2
          m: 2
          pg_num: 32
          plugin: jerasure
          rados_write_duration: 100
          rados_read_duration: 50
          osd_max_backfills: 16
          osd_recovery_max_active: 16
          delete_pool: true
      desc: Verify Recovery of EC pool with only "k" shards available

  - test:
      name: EC pool LC
      module: rados_prep.py
      polarion-id: CEPH-83571632
      config:
        ec_pool:
          create: true
          pool_name: test_ec_pool
          pg_num: 64
          k: 2
          m: 2
          plugin: jerasure
          disable_pg_autoscale: true
          rados_write_duration: 50
          rados_read_duration: 30
        set_pool_configs:
          pool_name: test_ec_pool
          configurations:
            pg_num: 32
            pgp_num: 32
            pg_autoscale_mode: 'on'
            compression_mode: force
            compression_algorithm: snappy
        delete_pools:
          - test_ec_pool
      desc: Create, modify & delete EC pools and run IO

  - test:
      name: EC pool with Overwrites
      module: rados_prep.py
      polarion-id: CEPH-83571730
      config:
        ec_pool:
          create: true
          pool_name: ec_pool_overwrite
          app_name: rbd
          pg_num: 32
          erasure_code_use_overwrites: "true"
          k: 2
          m: 2
          plugin: jerasure
          rados_write_duration: 50
          rados_read_duration: 30
          test_overwrites_pool: true
          metadata_pool: re_pool_overwrite
          image_name: image_ec_pool
          image_size: 100M
        delete_pools:
          - ec_pool_overwrite
          - re_pool_overwrite
      desc: EC pool with Overwrites & create RBD pool

  - test:
      name: Inconsistent objects in  EC pool functionality check
      desc: Scub and deep-scrub on  inconsistent objects in EC pool
      module: test_osd_ecpool_inconsistency_scenario.py
      polarion-id: CEPH-83586175
      config:
        ec_pool:
          create: true
          pool_name: ecpool
          pg_num: 1
          k: 2
          m: 2
          plugin: jerasure
          disable_pg_autoscale: true
        inconsistent_obj_count: 4
        delete_pool:
          - ecpool

  - test:
      name: EC pool with clay profile for RBD
      module: test_clay_ecpool.py
      polarion-id: CEPH-83574881
      config:
        clay_pool:
          profile_name: clay_profile1
          pool_name: clay_ec_pool1
          k: 4
          m: 3
          d: 6
          plugin: clay
          app_name: rbd
          erasure_code_use_overwrites: "true"
          crush-failure-domain: osd
          force: true
          test_overwrites_pool: true
          delete_pools: true
          image_name: test_clay_image1
          image_size: 50G
          metadata_pool: clay_meta_pool1
      desc: Create and delete CLAY profile EC pool with RBD Images

  - test:
      name: Pool tests with clay profile for RBD Images
      module: test_clay_ecpool.py
      polarion-id: CEPH-83574880
      config:
        clay_pool:
          profile_name: clay_profile2
          pool_name: clay_ec_pool2
          k: 4
          m: 2
          d: 5
          plugin: clay
          app_name: rbd
          erasure_code_use_overwrites: "true"
          crush-failure-domain: osd
          force: true
          test_overwrites_pool: true
          delete_pools: true
          image_name: test_clay_image2
          image_size: 50G
          metadata_pool: clay_meta_pool2
          test_tier3_system_tests: true
          test_compression:
            configurations:
              - config-1:
                  compression_mode: force
                  compression_algorithm: snappy
                  compression_required_ratio: 0.3
                  compression_min_blob_size: 1B
                  byte_size: 10KB
              - config-2:
                  compression_mode: passive
                  compression_algorithm: zlib
                  compression_required_ratio: 0.7
                  compression_min_blob_size: 10B
                  byte_size: 100KB
              - config-3:
                  compression_mode: aggressive
                  compression_algorithm: zstd
                  compression_required_ratio: 0.5
                  compression_min_blob_size: 1KB
                  byte_size: 100KB
      desc: Perform tests on EC pools having a RBD Image

  - test:
      name: Taking snaps during rebalance
      module: test_osd_rebalance_snap.py
      desc: Taking snaps while data migration is in progress
      polarion-id: CEPH-9235
      config:
        create_pools:
          - create_pool:
              snapshot: true
              num_snaps: 2
              pool_name: repool_1
              pg_num: 1
              rados_put: true
              byte_size: 1024
              pool_type: replicated
              osd_max_backfills: 16
              osd_recovery_max_active: 16

        delete_pools:
          - repool_1

  - test:
      name: Compression test - EC pool
      module: pool_tests.py
      polarion-id: CEPH-83571674
      config:
        Compression_tests:
          pool_type: erasure
          pool_config:
            pool-1: test_compression_ecpool-1
            pool-2: test_compression_ecpool-2
            rados_write_duration: 300
            byte_size: 10KB
            pg_num: 32
            k: 2
            m: 2
            plugin: jerasure
            crush-failure-domain: host
          compression_config:
            compression_mode: aggressive
            compression_algorithm: snappy
            compression_required_ratio: 0.7
            compression_min_blob_size: 1B
            byte_size: 10KB
      desc: Verification of the effect of compression on erasure coded pools

# 4. pool independent tests

  - test:
      name: Ceph balancer plugin
      module: rados_prep.py
      polarion-id: CEPH-83573247
      config:
        configure_balancer:
          configure: true
          balancer_mode: crush-compat
          target_max_misplaced_ratio: 0.04
          sleep_interval: 30
      desc: Ceph balancer plugins CLI validation in crush-compat mode

  - test:
      name: Ceph balancer test
      module: rados_prep.py
      polarion-id: CEPH-83573251
      config:
        configure_balancer:
          configure: true
          balancer_mode: upmap
          target_max_misplaced_ratio: 0.05
          sleep_interval: 60
      desc: Ceph balancer plugins CLI validation in upmap mode

  - test:
      name: Verification the recovery flag functionality
      desc: Verify the pg_num and recovery status
      module: test_norecovery_flag_functionality.py
      polarion-id: CEPH-83591783
      config:
        replicated_pool:
          create: true
          pool_name: recovery_pool
      delete_pool:
        - recovery_pool

  - test:
      name: Verify dups trimming
      module: test_pg_dups_trimming.py
      polarion-id: CEPH-83575116
      config:
        pool_configs:
            - type: replicated
              conf: sample-pool-1
            - type: replicated
              conf: sample-pool-2
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
        verify_inflation: false
      desc: Verify that the duplicates in the PG log entries are trimmed regularly

  - test:
      name: Test Reads Balancer
      module: test_reads_balancer.py
      desc: Testing reads balancer online tool functionality in RHCS
      polarion-id: CEPH-83576078
      config:
        negative_scenarios: true
        online_command_verification: true
        create_pools:
          - create_pool:
              pool_name: rpool_1
              pg_num: 64
              byte_size: 256
              pool_type: replicated
              rados_write_duration: 50
              rados_read_duration: 30
          - create_pool:
              pool_name: rpool_2
              pg_num: 128
              pool_type: replicated
              rados_write_duration: 50
              rados_read_duration: 30
              byte_size: 256
          - create_pool:
              pool_name: rpool_3
              pg_num: 32
              rados_write_duration: 50
              rados_read_duration: 30
              byte_size: 256
              pool_type: replicated
          - create_pool:
              create: true
              pool_name: ecpool_test_1
              pool_type: erasure
              pg_num: 32
              k: 2
              m: 2
              rados_write_duration: 50
              rados_read_duration: 30
              byte_size: 256
        delete_pools:
          - rpool_1
          - rpool_2
          - rpool_3
          - ecpool_test_1

  - test:
      name: Test Reads Balancer
      module: test_reads_balancer.py
      desc: Testing reads balancer offline tool functionality in RHCS
      polarion-id: CEPH-83576079
      config:
        offline_command_verification: true
        create_pools:
          - create_pool:
              pool_name: rpool_1
              pg_num: 64
              byte_size: 256
              pool_type: replicated
              rados_write_duration: 50
              rados_read_duration: 30
          - create_pool:
              pool_name: rpool_2
              pg_num: 128
              pool_type: replicated
              rados_write_duration: 50
              rados_read_duration: 30
              byte_size: 256
          - create_pool:
              pool_name: rpool_3
              pg_num: 32
              rados_write_duration: 50
              rados_read_duration: 30
              byte_size: 256
              pool_type: replicated
          - create_pool:
              create: true
              pool_name: ecpool_test_2
              pool_type: erasure
              pg_num: 32
              k: 2
              m: 2
              rados_write_duration: 50
              rados_read_duration: 30
              byte_size: 256
        delete_pools:
          - rpool_1
          - rpool_2
          - rpool_3
          - ecpool_test_2

  - test:
      name: Verify PG split and merge
      module: test_pg_split.py
      desc: Verify PG splitting and merging
      polarion-id: CEPH-11674
      config:
        create_pools:
          - create_pool:
              pool_name: pool1
              pg_num: 32
              rados_put: true
              num_objs: 200
              byte_size: 1024
              pool_type: replicated
        delete_pools:
          - pool1

  - test:
      name: Verify restart osd during PG split
      module: test_pg_split.py
      desc: Verify restart osd when PG split in progress
      polarion-id: CEPH-11667
      config:
        create_pools:
          - create_pool:
              restart_osd: true
              pool_name: pool2
              pg_num: 32
              rados_put: true
              num_objs: 200
              byte_size: 1024
              pool_type: replicated
        delete_pools:
          - pool2

  - test:
      name: Verify PG split and merge with network delay
      module: test_pg_split.py
      desc: Verify PG splitting and merging with network delay
      polarion-id: CEPH-83571705
      config:
        create_pools:
          - create_pool:
              pool_name: pool5
              pg_num: 32
              rados_put: true
              num_objs: 200
              byte_size: 1024
              pool_type: replicated
        add_network_delay: true
        delete_pools:
          - pool5

  - test:
      name: Verify delete object during PG split
      module: test_pg_split.py
      desc: Verify delete object when PG split in progress
      polarion-id: CEPH-11671
      config:
        create_pools:
          - create_pool:
              del_obj: true
              pool_name: pool3
              pg_num: 32
              rados_put: true
              num_objs: 200
              objs_to_del: 20
              byte_size: 1024
              pool_type: replicated
        delete_pools:
          - pool3

  - test:
      name: Verify premerge PGS during PG split
      module: test_pg_split.py
      desc: Verify if there are premerge PGs when split is in progress
      polarion-id: CEPH-83573526
      config:
        create_pools:
          - create_pool:
              check_premerge_pgs: true
              pool_name: pool3
              pg_num: 32
              rados_put: true
              num_objs: 200
              byte_size: 1024
              pool_type: replicated
        delete_pools:
          - pool3

  - test:
      name: OSD compaction with failures
      module: test_osd_compaction.py
      polarion-id: CEPH-11681
      config:
        omap_config:
          pool_name: re_pool_large_omap
          large_warn: true
          obj_start: 0
          obj_end: 5
          normal_objs: 400
          num_keys_obj: 200001
        bench_config:
          pool_name: re_pool_bench
          pg_num: 128
          pgp_num: 128
      desc: Perform OSD compaction with & without failures

  - test:
      name: Pg autoscaler bulk flag
      module: pool_tests.py
      polarion-id: CEPH-83573412
      desc: Ceph PG autoscaler bulk flag tests
      config:
        test_autoscaler_bulk_feature: true
        pool_name: test_bulk_features
        delete_pool: true

  - test:
      name: PG number maximum limit check
      module: pool_tests.py
      desc: Check the pg_num maximut limit is <=128
      polarion-id: CEPH-83574909
      config:
        verify_pg_num_limit:
          pool_name: pool_num_chk
          delete_pool: true

  - test:
      name: OSD min-alloc size and fragmentation checks
      module: rados_prep.py
      polarion-id: CEPH-83573808
      config:
        Verify_osd_alloc_size:
          allocation_size: 4096
      desc: Verify the minimum allocation size for OSDs along with fragmentation scores.

  - test:
      name: Migrate data bw pools.
      module: test_data_migration_bw_pools.py
      polarion-id: CEPH-83574768
      config:
        pool-1-type: replicated
        pool-2-type: replicated
        pool-1-conf: sample-pool-1
        pool-2-conf: sample-pool-2
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
      desc: Migrating data between different pools. Scenario-1. RE -> RE

  - test:
      name: Migrate data bw pools.
      module: test_data_migration_bw_pools.py
      polarion-id: CEPH-83574768
      config:
        pool-1-type: replicated
        pool-2-type: erasure
        pool-1-conf: sample-pool-1
        pool-2-conf: sample-pool-3
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
      desc: Migrating data between different pools. Scenario-2. RE -> EC

  - test:
      name: Migrate data bw pools.
      module: test_data_migration_bw_pools.py
      polarion-id: CEPH-83574768
      config:
        pool-1-type: erasure
        pool-2-type: replicated
        pool-1-conf: sample-pool-3
        pool-2-conf: sample-pool-3
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
      desc: Migrating data between different pools. Scenario-3. EC -> RE

  - test:
      name: Migrate data bw pools.
      module: test_data_migration_bw_pools.py
      polarion-id: CEPH-83574768
      config:
        pool-1-type: erasure
        pool-2-type: erasure
        pool-1-conf: sample-pool-2
        pool-2-conf: sample-pool-3
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
      desc: Migrating data between different pools. Scenario-4. Ec -> EC

  - test:
      name: Autoscaler test - pool pg_num_min
      module: pool_tests.py
      polarion-id: CEPH-83573425
      config:
        verify_pg_num_min:
          configurations:
            pool-1:
              pool_name: ec_pool_2
              pool_type: erasure
              pg_num: 32
              k: 8
              m: 3
              plugin: jerasure
              crush-failure-domain: osd
              pg_num_min: 16
              rados_write_duration: 50
              rados_read_duration: 50
              delete_pool: true
            pool-2:
              pool_type: replicated
              pool_name: re_pool_2
              pg_num: 64
              pg_num_min: 32
              rados_write_duration: 50
              rados_read_duration: 50
              delete_pool: true
      desc: Specifying pool bounds on pool Pgs - Verify pg_num_min

  - test:
      name: client pg access
      module: test_client_pg_access.py
      polarion-id: CEPH-83571713
      config:
        verify_client_pg_access:
          num_snapshots: 20
          configurations:
            pool-1:
              pool_name: ec_pool_4
              pool_type: erasure
              pg_num: 1
              k: 8
              m: 3
              disable_pg_autoscale: true
            pool-2:
              pool_type: replicated
              pool_name: re_pool_4
              pg_num: 1
              disable_pg_autoscale: true
      desc: many clients clients accessing same PG with bluestore as backend

  - test:
      name: Verify cluster behaviour during PG autoscaler warn
      module: pool_tests.py
      polarion-id:  CEPH-83573413
      config:
        verify_pool_warnings:
          pool_configs:
            - type: replicated
              conf: sample-pool-1
            - type: erasure
              conf: sample-pool-2
          pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
      desc: Verify alerts for large number of Objs per OSD during PG autoscaler warn

  - test:
      name: Verify scrub logs
      module: test_scrub_log.py
      polarion-id: CEPH-83575403
      config:
        pool_configs:
            - type: replicated
              conf: sample-pool-2
            - type: erasure
              conf: sample-pool-2
        pool_configs_path: "conf/reef/rados/test-confs/pool-configurations.yaml"
      desc: Verify that scrub & deep-scrub logs are captured in OSD logs

  - test:
      name: Verify pool behaviour at min_size
      module: pool_tests.py
      polarion-id: CEPH-9167
      config:
        Verify_pool_min_size_behaviour:
          pool_name: test-pool-3
      desc: Verify that Clients can read and write data into pools with min_size OSDs

  - test:
      name: Autoscaler test - pool target size ratio
      module: pool_tests.py
      polarion-id: CEPH-83573424
      config:
        verify_pool_target_ratio:
          configurations:
            pool-1:
              pool_name: ec_pool_1
              pool_type: erasure
              pg_num: 32
              k: 2
              m: 2
              plugin: jerasure
              crush-failure-domain: osd
              target_size_ratio: 0.8
              rados_write_duration: 50
              rados_read_duration: 50
              delete_pool: true
            pool-2:
              pool_type: replicated
              pool_name: re_pool_1
              pg_num: 32
              target_size_ratio: 0.8
              rados_write_duration: 50
              rados_read_duration: 50
              delete_pool: true
      desc: Specifying pool bounds on pool Pgs - Verify target_size_ratio

  - test:
      name: Verify Ceph df stats
      desc: Verify Ceph df stats after creating and deleting objects from a pool
      module: test_cephdf.py
      polarion-id: CEPH-83571666
      config:
        verify_cephdf_stats:
          create_pool: true
          pool_name: test-ceph-df
          obj_nums:
            - 5
            - 20
            - 50
          delete_pool: true
      destroy-cluster: false

# 5. OSD tests

  - test:
      name: Verification of recovery from Slow OSD heartbeat
      module: test_bug_fixes.py
      config:
        slow-osd-heartbeat-baremetal: true
      polarion-id: CEPH-83590688
      desc: Verify auto removal of Slow OSD heartbeat

  - test:
      name: OSD restart when bluefs_shared_alloc_size is lower than bluestore_min_alloc_size
      module: test_bug_fixes.py
      config:
        lower_bluefs_shared_alloc_size: true
      polarion-id: CEPH-83591092
      desc: verify OSD resiliency when 'bluefs_shared_alloc_size' is below 'bluestore_min_alloc_size'

  - test:
      name: Verification of Slow OSD heartbeat
      module: test_bug_fixes.py
      config:
        slow-osd-heartbeat: true
      polarion-id: CEPH-83590688
      desc: Generate Slow OSD heartbeats by inducing network delay

  - test:
      name: Cluster behaviour when OSDs are full
      desc: Test PG autoscaling and rebalancing when OSDs are near-full, backfill-full and completely full
      module: test_osd_full.py
      polarion-id: CEPH-83571715
      config:
        pg_autoscaling:
          pool_config:
            pool-1:
              pool_type: replicated
              pool_name: re_pool_3
              pg_num: 1
              disable_pg_autoscale: true
            # EC pool will be added later

  - test:
      name: OSD behaviour when marked Out
      module: pool_tests.py
      polarion-id: CEPH-83591786
      config:
        Verify_osd_in_out_behaviour: true
      desc: Verify OSD behaviour when it is marked in and out of cluster


# 6. Mon/ Mgr daemon tests

  - test:
      name: Mon election strategies
      polarion-id: CEPH-83573627
      module: test_election_strategies.py
      desc: Change Mon election strategies and verify status

  - test:
      name: Verification of the Mon ballooning
      module: test_rados_ballooning_client.py
      polarion-id: CEPH-83584004
      desc: Verify MON ballooning client

  - test:
      name: MGR regression test
      module: test_mgr_daemon.py
      polarion-id: CEPH-83586116
      desc: MGR regression testing

  - test:
      name: Automatic trimming of Mon DB
      module: customer_scenarios.py
      polarion-id: CEPH-83574466
      config:
        mondb_trim_config:
          paxos_service_trim_min: 10
          paxos_service_trim_max: 100
          osd_op_complaint_time: 0.000001
          osd_max_backfills: 10
          osd_recovery_max_active: 10
      desc: Verification of mon DB trimming during various cluster operations

  - test:
      name: verify mon parameter 'mon_pg_warn_max_object_skew'
      polarion-id: CEPH-83575440
      module: test_mon_pg_warn.py
      desc: verify auto-increment of 'mon_pg_warn_max_object_skew' MON parameter

  - test:
      name: Mon target for PG num
      module: pool_tests.py
      polarion-id: CEPH-83573423
      desc: Verification of mon_target_pg_per_osd option globally
      config:
        verify_mon_target_pg_per_osd:
          section: "global"
          name: "mon_target_pg_per_osd"
          value: "150"

  - test:
      name: Change Mon weight for Mgr
      polarion-id: CEPH-83588304
      module: test_mon_mgr_weight.py
      desc: Verify Mgr stability when mon-weight is modified

  - test:
      name: mon replacement test
      polarion-id: CEPH-9407
      module: test_mon_addition_removal.py
      desc: Replace a Healthy Mon with a new MON

  - test:
      name: mon system tests
      polarion-id: CEPH-9388
      module: test_mon_system_operations.py
      desc: Performing system tests with mon daemons

# 7. Misc component tests

  - test:
      name: Limit PG log size
      polarion-id: CEPH-83573252
      module: test_pg_log_limit.py
      desc: Verify PG log growth limit using pglog_hardlimit flag

  - test:
      name: Mute ceph health alerts
      polarion-id: CEPH-83573854
      module: mute_alerts.py
      desc: Mute health alerts

  - test:
      name: prometheus metadata
      module: test_brownfield.py
      desc: Fetch and verify ceph version for various daemons post upgrade
      polarion-id: CEPH-83581346
      config:
       prometheus_metrics:
         daemons:
           - mon
           - osd
           - mgr

  - test:
      name: ceph osd df stats
      module: test_osd_df.py
      desc: Mark osd out and inspect stats change in ceph osd df
      polarion-id: CEPH-10787
      config:
        run_iteration: 3
        create_pool: true
        pool_name: test-osd-df
        write_iteration: 4
        delete_pool: true

  - test:
      name: ObjectStore block stats verification
      module: test_objectstore_block.py
      desc: Reduce data from an object and verify the decrease in blocks
      polarion-id: CEPH-83571714
      config:
        create_pool: true
        pool_name: test-objectstore
        write_iteration: 3
        delete_pool: true


  - test:
      name: Limit slow request details to cluster log
      module: test_slow_op_requests.py
      desc: Limit slow request details to cluster log
      polarion-id: CEPH-83574884
      config:
        create_pools:
          - create_pool:
              pool_name: pool1
              pg_num: 64
              rados_write_duration: 1000
              byte_size: 1024
              pool_type: replicated
              osd_max_backfills: 16
              osd_recovery_max_active: 16
          - create_pool:
              pool_name: pool2
              pg_num: 64
              rados_write_duration: 1000
              byte_size: 1024
              pool_type: replicated
              osd_max_backfills: 16
              osd_recovery_max_active: 16
          - create_pool:
              pool_name: pool3
              pg_num: 64
              rados_write_duration: 1000
              byte_size: 1024
              pool_type: replicated
              osd_max_backfills: 16
              osd_recovery_max_active: 16
        delete_pools:
          - pool1
          - pool2
          - pool3

  - test:
      name: BlueStore Checksum algorithms
      module: test_bluestore_configs.py
      polarion-id: CEPH-83571646
      config:
        checksums:
          - none
          - crc32c
          - crc32c_16
          - crc32c_8
          - xxhash32
          - xxhash64
      desc: Verify the different applicable BlueStore Checksum algorithms

  - test:
      name: BlueStore cache size tuning
      module: test_bluestore_configs.py
      polarion-id: CEPH-83571675
      config:
        bluestore_cache: true
      desc: Verify tuning of BlueStore cache size for HDDs and SSDs

  - test:
      name: ceph-objectstore-tool utility
      module: test_objectstoretool_workflows.py
      polarion-id: CEPH-83581811
      desc: Verify ceph-objectstore-tool functionalities


  - test:
      name: ceph-bluestore-tool utility
      module: test_bluestoretool_workflows.py
      polarion-id: CEPH-83571692
      desc: Verify ceph-bluestore-tool functionalities

  - test:
      name: OMAP feature
      desc: Testing omap features
      module: test_scrub_omap.py
      polarion-id: CEPH-83572661
      config:
        verify_osd_omap_entries:
          configurations:
            pool-1:
              pool_name: Test
              pool_type: replicated
              pg_num: 1
          omap_config:
            obj_start: 0
            obj_end: 200
            num_keys_obj: 300000
        verify_cluster_health: true

  - test:
      name: concurrent and parallel IOPS on an object
      desc: Perform concurrent and parallel IOPs on one object
      module: test_object_ops.py
      config:
        obj_sizes:
          - 45
          - 80
          - 23
          - 107
      polarion-id: CEPH-83571710

  - test:
      name: Inconsistent OSD epoch
      desc: list-inconsistent requires the correct epoch
      module: test_inconsistency_osd_epoch.py
      polarion-id: CEPH-9947
      config:
        verify_osd_omap_entries:
          configurations:
            pool_name: test_pool_4
            pool_type: replicated
          omap_config:
            obj_start: 0
            obj_end: 50
            num_keys_obj: 100
        delete_pool: true

  - test:
      name: Omap creations on objects
      module: test_omap_entries.py
      polarion-id: CEPH-83571702
      config:
        # Pool created to  verify the Bug#2249003
        crash_config:
          pool_name: re_pool_crash
        # EC pool config is commented out because omaps can be written only on RE pools
        omap_config:
          small_omap:
            pool_name: re_pool_small_omap
            pg_num: 1
            pg_num_max: 1
            large_warn: false
            obj_start: 0
            obj_end: 5
            normal_objs: 400
            num_keys_obj: 200000
          large_omap:
            pool_name: re_pool_large_omap
            pg_num: 1
            pg_num_max: 1
            large_warn: true
            obj_start: 0
            obj_end: 5
            normal_objs: 400
            num_keys_obj: 200001
      desc: Large number of omap creation on objects and OSD resiliency

  - test:
      name: Ceph MSGRV2 paramter check in 7.x
      desc: MSGRV2 parameter check in 7.x
      polarion-id: CEPH-83574890
      module: test_config_parameter_chk.py
      config:
        scenario: msgrv2_6x
        ini-file: conf/reef/rados/test-confs/rados_config_parameters.ini

  - test:
      name: Mclock sleep parameter check
      desc: Mclock sleep parameter check
      polarion-id: CEPH-83574903
      module: test_config_parameter_chk.py
      config:
        scenario: mclock_sleep
        ini-file: conf/reef/rados/test-confs/rados_config_parameters.ini

  - test:
      name: Mclock default,reservation,limit and weight parameter check
      desc: Mclock default,reservation,limit and weight parameter check
      polarion-id: CEPH-83574902
      module: test_config_parameter_chk.py
      config:
        scenario: mclock_chg_chk
        ini-file: conf/reef/rados/test-confs/rados_config_parameters.ini

  - test:
      name: Check RocksDB compression default value
      desc: Check that RocksDB compression value is kLZ4Compression
      polarion-id: CEPH-83582326
      module: test_config_parameter_chk.py
      config:
        scenario: rocksdb_compression


  - test:
      name: Automatic trimming of osdmaps
      desc: check for periodic trimming of osdmaps
      module: test_osdmap_trim.py
      polarion-id: CEPH-10046

  - test:
      name: Trimming of onodes
      desc: check for the onode trimming in the cluster
      module: test_osd_onode_trimming.py
      polarion-id: CEPH-83575269

  - test:
      name: Crushtool to change crush maps
      module: test_crushtool_workflows.py
      polarion-id: CEPH-83572695
      config:
        add_buckets:
          DC1: datacenter
          DC2: datacenter
          tiebreaker: datacenter
          test-host-1: host
          test-host-2: host
        bin_tests:
          - show-statistics
          - show-mappings
          - show-utilization-all
      desc: Verify Crushtool to change crush maps ( -ve Tests )

  - test:
      name: crash warning upon daemon crash
      module: test_crash_daemon.py
      polarion-id: CEPH-83573855
      desc: Verify crash warning in ceph health upon crashing a daemon
      comments: Active BZ-2253394

  - test:
      name: Test ceph osd command arguments
      desc: Provide invalid values as argument to different ceph osd commands
      module: test_osd_args.py
      polarion-id: CEPH-10417
      config:
        cmd_list:
          - "reweight-by-pg"
          - "reweight-by-utilization"
          - "test-reweight-by-pg"
          - "test-reweight-by-utilization"

  - test:
      name: Verify Ceph df MAX_AVAIL
      desc: MAX_AVAIL value should not change to 0 upon addition of OSD with weight 0
      module: test_cephdf.py
      polarion-id: CEPH-10312
      config:
        verify_cephdf_max_avail:
          create_pool: true
          pool_name: test-max-avail
          obj_nums: 5
          delete_pool: true

# Scrubbing
  - test:
      name: Preempt scrub messages checks
      desc: Checking preempt messages in the OSDs
      module: test_rados_preempt_scrub.py
      polarion-id: CEPH-83572916
      config:
        verify_osd_preempt:
          configurations:
            pool-1:
              pool_name: preempt_pool
              pool_type: replicated
              pg_num: 16
        delete_pool: true
      comments: Active BZ-2269089

  - test:
      name: verify scrub chunk max
      polarion-id: CEPH-10792
      module: test_scrub_chunk_max.py
      config:
        delete_pool: true
      desc: Scrub Chunk max validation

  - test:
      name: Scrub enhancement
      module: test_scrub_enhancement.py
      desc: Verify scrub enhancement feature
      polarion-id: CEPH-83575885
      config:
        create_pools:
          - create_pool:
              pool_name: scrub_pool
              pg_num: 1
              pg_num_max: 1
              pool_type: replicated
        delete_pools:
          - scrub_pool

  - test:
      name: CPU and Memory check during scheduled scrub
      module: test_scrub_cpu_memory_usage.py
      desc: Verify CPU and Memory check during scheduled scrub
      polarion-id: CEPH-9369
      config:
        create_pools:
          - create_pool:
              pool_name: scheduled_scrub
              pg_num: 32
              pg_num_min: 32
              pool_type: replicated
              rados_write_duration: 120
              rados_read_duration: 120
              byte_size: 1KB
        delete_pools:
          - scheduled_scrub

  - test:
      name: Default scheduled scrub
      polarion-id: CEPH-9361
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "default"
      delete_pools:
        - scrub_pool

  - test:
      name: Begin Time = End Time
      polarion-id: CEPH-9362
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "begin_end_time_equal"
      delete_pools:
        - scrub_pool

  - test:
      name: Begin time > End time
      polarion-id: CEPH-9363
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "beginTime gt endTime"
      delete_pools:
        - scrub_pool

  - test:
      name: Begin Time >End time<current
      polarion-id: CEPH-9365
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "beginTime gt endTime lt currentTime"
      delete_pools:
        - scrub_pool

  - test:
      name: Begin Time & End time > current
      polarion-id: CEPH-9368
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "beginTime and endTime gt currentTime"
      delete_pools:
        - scrub_pool

  - test:
      name: Decrease scrub time
      polarion-id: CEPH-9371
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "decreaseTime"
      delete_pools:
        - scrub_pool

  - test:
      name: Unsetting scrubbing
      polarion-id: CEPH-9374
      module: scheduled_scrub_scenarios.py
      desc: Scheduled scrub validation
      config:
        replicated_pool:
          create: true
          pool_name: scrub_pool
          pg_num: 1
          disable_pg_autoscale: true
        scenario: "unsetScrub"
      delete_pools:
        - scrub_pool
