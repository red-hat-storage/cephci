# Test suite for for baremetal rgw-multisite deployment and testing multisite sync at scale.
#
# This suite deploys a Longevity environment on baremetal machines. It is a multisite environment with each site having a storage capacity of 617Tb.
# each site has around 4 osd hosts and a total of 16 osds.
# A single realm (India) spanning across two RHCS clusters. It has a
# zonegroup (shared) which also spans across the clusters. There exists a master (primary)
# and secondary (secondary) zone within this group. The master zone is part of the pri
# cluster whereas the sec zone is part of the sec datacenter (cluster).

# The deployment is evaluated by running IOs across the environments.
# tested with conf file: conf/baremetal/extensa_clara_multisite_1admin_4node_1client.yaml

tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: clara001
                    allow-fqdn-hostname: true
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: clara013
                    allow-fqdn-hostname: true
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
      desc: Bootstrap clusters using cephadm.
      polarion-id: CEPH-83573386
      destroy-cluster: false
      module: test_cephadm.py
      name: Bootstrap clusters

# deploy more mons and mgrs
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
      desc: RHCS deploy mons and mgrs
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy mons and mgrs

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa001
                    - "/dev/sda"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa001
                    - "/dev/sdc"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa001
                    - "/dev/sdd"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa001
                    - "/dev/sde"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa010
                    - "/dev/sda"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa010
                    - "/dev/sdc"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa010
                    - "/dev/sdd"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa010
                    - "/dev/sde"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa011
                    - "/dev/sda"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa011
                    - "/dev/sdc"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa011
                    - "/dev/sdd"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa011
                    - "/dev/sde"
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa012
                    - "/dev/sda"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa012
                    - "/dev/sdc"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa012
                    - "/dev/sdd"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa012
                    - "/dev/sde"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa013
                    - "/dev/sdb"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa013
                    - "/dev/sdc"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa013
                    - "/dev/sdd"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa013
                    - "/dev/sde"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa014
                    - "/dev/sdb"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa014
                    - "/dev/sdc"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa014
                    - "/dev/sdd"
              - config:
                  command: add
                  service: osd
                  pos_args:
                    - extensa014
                    - "/dev/sde"
      desc: RHCS OSD deployment
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_daemon.py
      name: Add OSD services

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri.io
                  args:
                    port: 8080
                    placement:
                      nodes:
                        - clara001
                        - clara011
                        - clara012
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri.sync
                  args:
                    port: 8080
                    placement:
                      nodes:
                        - extensa001
                        - extensa010
                        - extensa011
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec.io
                  args:
                    port: 8080
                    placement:
                      nodes:
                        - clara013
                        - clara014
                        - clara015
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec.sync
                  args:
                    port: 8080
                    placement:
                      nodes:
                        - extensa012
                        - extensa013
                        - extensa014
      desc: RHCS rgws deploy using cephadm
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_cephadm.py
      name: rgws deploy using cephadm

  - test:
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - clara001
                    - service_type: grafana
                      placement:
                        nodes:
                          - clara001
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - clara013
                    - service_type: grafana
                      placement:
                        nodes:
                          - clara013
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.pri
            node:
              - clara001
              - extensa011
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.sec
            node:
              - clara013
              - extensa014
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      polarion-id: CEPH-83573758
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:extensa001}:8080,http://{node_ip:extensa010}:8080,http://{node_ip:extensa011}:8080 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:extensa001}:8080,http://{node_ip:extensa010}:8080,http://{node_ip:extensa011}:8080 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key test123 --secret test123 --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key test123 --secret test123"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.shared.pri.sync rgw_realm india"
              - "ceph config set client.rgw.shared.pri.sync rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.pri.sync rgw_zone primary"
              - "ceph config set client.rgw.shared.pri.io rgw_realm india"
              - "ceph config set client.rgw.shared.pri.io rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.pri.io rgw_zone primary"
              - "ceph config set client.rgw.shared.pri.io rgw_run_sync_thread false"
              - "ceph orch restart {service_name:shared.pri.io}"
              - "ceph orch restart {service_name:shared.pri.sync}"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://extensa001:8080 --access-key test123 --secret test123 --default"
              - "radosgw-admin period pull --url http://extensa001:8080 --access-key test123 --secret test123"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:extensa012}:8080,http://{node_ip:extensa013}:8080,http://{node_ip:extensa014}:8080 --access-key test123 --secret test123"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.shared.sec.sync rgw_realm india"
              - "ceph config set client.rgw.shared.sec.sync rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.sec.sync rgw_zone secondary"
              - "ceph config set client.rgw.shared.sec.io rgw_realm india"
              - "ceph config set client.rgw.shared.sec.io rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.sec.io rgw_zone secondary"
              - "ceph config set client.rgw.shared.sec.io rgw_run_sync_thread false"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10362

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - extensa001
            rgw_endpoints:
              - "extensa001:8080"
              - "extensa010:8080"
              - "extensa011:8080"
        ceph-sec:
          config:
            haproxy_clients:
              - extensa012
            rgw_endpoints:
              - "extensa012:8080"
              - "extensa013:8080"
              - "extensa014:8080"
      desc: Configure HAproxy for sync rgw
      module: haproxy.py
      name: Configure HAproxy for sync rgw
      polarion-id: CEPH-83572703

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin zonegroup modify --endpoints=http://extensa001:5000 --rgw-zonegroup shared --rgw-realm india --access-key test123 --secret-key test123"
              - "radosgw-admin zone modify --endpoints=http://extensa001:5000 --rgw-zonegroup shared --rgw-realm india --rgw-zone primary --access-key test123 --secret-key test123"
              - "radosgw-admin period update --commit"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin zone modify --rgw-zone secondary --endpoints=http://extensa012:5000 --access-key test123 --secret-key test123"
              - "radosgw-admin period update --commit"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10362

  - test:
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph versions"
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "radosgw-admin user list"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph versions"
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "radosgw-admin user list"
      desc: Retrieve the configured environment details
      polarion-id: CEPH-83575227
      module: exec.py
      name: get shared realm info

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - clara001
              - clara011
            rgw_endpoints:
              - "clara001:8080"
              - "clara011:8080"
              - "clara012:8080"
        ceph-sec:
          config:
            haproxy_clients:
              - clara013
              - clara014
            rgw_endpoints:
              - "clara013:8080"
              - "clara014:8080"
              - "clara015:8080"
      desc: Configure HAproxy for io rgw
      module: haproxy.py
      name: Configure HAproxy for io rgw
      polarion-id: CEPH-83572703

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            controllers:
              - clara001
            drivers:
              count: 6
              hosts:
                - clara001
                - clara011
                - clara012
        ceph-sec:
          config:
            controllers:
              - clara013
            drivers:
              count: 6
              hosts:
                - clara013
                - clara014
                - clara015
      desc: Start COS Bench controller and driver
      module: cosbench.py
      name: deploy cosbench
