# This test suite deploys the required environment for OCS CI to validate with external
# Ceph cluster. Only cluster deployment is validated and the rest of validation occurs
# in OCS / ODF CI.

# conf-file conf/reef/integrations/ocs_rgw_ssl_openstack_conf.yaml
---
tests:

  - test:
      abort-on-fail: true
      desc: "Install prerequisites for RHCeph cluster deployment."
      module: install_prereq.py
      name: "test suite setup"

  - test:
      abort-on-fail: true
      config:
        args:
          mon-ip: node1
        command: bootstrap
        service: cephadm
      desc: "Bootstrap the cluster with minimal configuration."
      destroy-cluster: false
      module: test_bootstrap.py
      name: "Test cluster deployment using cephadm"

  - test:
      abort-on-fail: true
      config:
        args:
          attach_ip_address: true
          labels: apply-all-labels
        command: add_hosts
        service: host
      desc: "Adding hosts to the cluster using labels and IP information."
      destroy-cluster: false
      module: test_host.py
      name: "Test host add with labels and IP"

  - test:
      abort-on-fail: true
      config:
        args:
          all-available-devices: true
        command: apply
        service: osd
      desc: "Deploying OSD daemons using all-available-devices option."
      destroy-cluster: false
      module: test_osd.py
      name: "Test apply osd with all-available-devices option"

  - test:
      abort-on-fail: true
      config:
        steps:
          - config:
              args:
                - "ceph osd pool create rbd"
              command: shell
          - config:
              args:
                - "rbd pool init rbd"
              command: shell
      desc: create rbd pool and initialize
      destroy-cluster: false
      polarion-id: CEPH-83573713
      module: test_cephadm.py
      name: Create rbd pool and initialize

  - test:
      abort-on-fail: true
      config:
        command: apply_spec
        specs:
          - service_type: rgw
            service_id: rgw.ssl
            placement:
              nodes:
                - node2
            spec:
              ssl: false
              rgw_frontend_ssl_certificate: create-cert
      desc: "Deploying RGW endpoint which is SSL terminated."
      destroy-cluster: false
      module: test_rgw.py
      name: "Test rgw ssl endpoint using spec"

  - test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - ceph fs volume create fsvol001 --placement='2 label:mds'
          - ceph fs set fsvol001 max_mds 1
      desc: "Create and configure a volume with 2 MDS limit"
      destroy-cluster: false
      module: exec.py
      name: "Create volume with"

  - test:
      abort-on-fail: true
      config:
        args:
          placement:
            label: mds
        command: apply
        pos_args:
          - fsvol001
        service: mds
      desc: Test deploying MDS daemons on hosts with label mds.
      destroy-cluster: false
      module: test_mds.py
      name: Test OSD daemon deployment with all-available-devices enabled.

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node5
        install_packages:
          - ceph-common
        copy_admin_keyring: true
      desc: Configure the ceph client
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        check_status: true
        commands:
          - "radosgw-admin user create --uid=rgw-admin-ops-user --display_name='rgw-admin-ops-user' --email rgw-admin-ops-user@foo.foo"
      desc: Execute radosgw-admin commands for zone
      module: exec.py
      name: rgw-admin-user

  - test:
      abort-on-fail: true
      desc: "Retrieve the deployed cluster information."
      module: gather_cluster_info.py
      name: "Get ceph cluster details."
