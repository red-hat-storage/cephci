# Suite contains basic tier-4 rados tests
# conf - conf/squid/rados/13-node-cluster.yaml
# RHOS-d run duration: 170 mins
tests:
  - test:
      name: setup install pre-requisistes
      desc: Setup phase to deploy the required pre-requisites for running the tests.
      module: install_prereq.py
      abort-on-fail: true

  - test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                mon-ip: node1
                orphan-initial-daemons: true

  - test:
      name: Add host
      desc: Add new host node with IP address
      module: test_host.py
      config:
        command: add_hosts
        service: host
        args:
          nodes:
            - node1
            - node2
            - node3
            - node4
            - node5
            - node6
            - node8
            - node9
            - node10
            - node11
          attach_ip_address: true
          labels: apply-all-labels
      destroy-cluster: false
      abort-on-fail: true

  - test:
      name: Mgr and Mon deployment
      desc: Add Mgr and Mon daemons
      module: test_cephadm.py
      polarion-id: CEPH-83573746
      abort-on-fail: true
      config:
        steps:
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon

  - test:
      name: OSD deployment
      desc: Add OSD services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83573746
      abort-on-fail: true
      config:
        steps:
          - config:
              command: apply_spec
              service: orch
              validate-spec-services: true
              specs:
                - service_type: osd
                  service_id: osds
                  encrypted: "true"                     # boolean as string
                  placement:
                    label: osd
                  spec:
                    data_devices:
                      all: "true"

  - test:
      name: MDS Service deployment with spec
      desc: Add MDS services using spec file
      module: test_cephadm.py
      polarion-id: CEPH-83574728
      config:
        steps:
          - config:
              command: shell
              args: # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: apply_spec
              service: orch
              validate-spec-services: true
              specs:
                - service_type: mds
                  service_id: cephfs
                  placement:
                    label: mds

  - test:
      name: RGW Service deployment
      desc: RGW Service deployment
      module: test_cephadm.py
      polarion-id: CEPH-83574728
      config:
        steps:
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw

  - test:
      name: Configure client admin
      desc: Configures client admin node on cluster
      module: test_client.py
      polarion-id:
      config:
        command: add
        id: client.1                      # client Id (<type>.<Id>)
        node: node7                       # client node
        install_packages:
          - ceph-common
        copy_admin_keyring: true          # Copy admin keyring to node
        caps:                             # authorize client capabilities
          mon: "allow *"
          osd: "allow *"
          mds: "allow *"
          mgr: "allow *"

  - test:
      name: OSD addition on an unavailable disk
      desc: Add new OSD on an pre-occupied OSD disk
      module: test_add_new_osd.py
      polarion-id: CEPH-83574780

  - test:
      name: replacement of OSD
      module: test_osd_replacement.py
      polarion-id: CEPH-83572702
      desc: Replace an OSD by retaining its ID

  - test:
      name: Test ceph osd command arguments
      desc: Provide invalid values as argument to different ceph osd commands
      module: test_osd_args.py
      polarion-id: CEPH-10417
      config:
        cmd_list:
          - "reweight-by-pg"
          - "reweight-by-utilization"
          - "test-reweight-by-pg"
          - "test-reweight-by-utilization"

  - test:
      name: Verify Ceph df MAX_AVAIL
      desc: MAX_AVAIL value should not change to 0 upon addition of OSD with weight 0
      module: test_cephdf.py
      polarion-id: CEPH-10312
      config:
        verify_cephdf_max_avail:
          create_pool: true
          pool_name: test-max-avail
          obj_nums: 5
          delete_pool: true

# commented due to active bug: https://bugzilla.redhat.com/show_bug.cgi?id=2316351
#  - test:
#      name: Verify MAX_AVAIL variance with OSD size change
#      desc: MAX_AVAIL value update correctly when OSD size changes
#      module: test_cephdf.py
#      polarion-id: CEPH-83595780
#      config:
#        cephdf_max_avail_osd_expand: true

  - test:
      name: Compression algorithms - modes
      module: rados_prep.py
      polarion-id: CEPH-83571670
      config:
        replicated_pool:
          create: true
          pool_name: re_pool_compress
          pg_num: 32
          rados_write_duration: 10
          rados_read_duration: 10
        enable_compression:
          pool_name: re_pool_compress
          rados_write_duration: 50
          rados_read_duration: 50
          configurations:
            - config-1:
                compression_mode: force
                compression_algorithm: snappy
                compression_required_ratio: 0.3
                compression_min_blob_size: 1B
                byte_size: 10KB
            - config-2:
                compression_mode: passive
                compression_algorithm: zlib
                compression_required_ratio: 0.7
                compression_min_blob_size: 10B
                byte_size: 100KB
            - config-3:
                compression_mode: aggressive
                compression_algorithm: zstd
                compression_required_ratio: 0.5
                compression_min_blob_size: 1KB
                byte_size: 100KB
      desc: Enable/disable different compression modes.

  - test:
      name: Compression algorithm tuneables
      module: rados_prep.py
      polarion-id: CEPH-83571671
      config:
        replicated_pool:
          create: true
          pool_name: re_pool_compress
          pg_num: 32
          rados_write_duration: 10
          rados_read_duration: 10
        enable_compression:
          pool_name: re_pool_compress
          rados_write_duration: 50
          rados_read_duration: 50
          configurations:
            - config-1:
                compression_mode: force
                compression_algorithm: snappy
                compression_required_ratio: 0.3
                compression_min_blob_size: 1B
                byte_size: 10KB
            - config-2:
                compression_mode: passive
                compression_algorithm: zlib
                compression_required_ratio: 0.7
                compression_min_blob_size: 10B
                byte_size: 100KB
            - config-3:
                compression_mode: aggressive
                compression_algorithm: zstd
                compression_required_ratio: 0.5
                compression_min_blob_size: 1KB
                byte_size: 100KB
      desc: Verify and alter different compression tunables.

  - test:
      name: Mute ceph health alerts
      polarion-id: CEPH-83573854
      module: mute_alerts.py
      desc: Mute health alerts

  - test:
      name: Replacement of a failed OSD host
      module: test_pool_osd_recovery.py
      polarion-id: CEPH-9408
      config:
        osd_host_fail: True
      desc: Replacement of a failed OSD host with a healthy host

  - test:
      name: mon replacement test
      polarion-id: CEPH-9407
      module: test_mon_addition_removal.py
      desc: Replace a Healthy Mon with a new MON

  - test:
      name: mon system tests
      polarion-id: CEPH-9388
      module: test_mon_system_operations.py
      desc: Performing system tests with mon daemons
