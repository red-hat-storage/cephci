# Conf file : conf/squid/common/ms_3way_5node_1client_rh.yaml
# deployment: suites/squid/common/regression/ms_3way_deploy-and-configure.yaml

---

tests:

# configuring HAproxy for IO RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node2:80
              - node3:80
        ceph-sec:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node2:80
              - node3:80
        ceph-ter:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node2:80
              - node3:80
      desc: "Configure HAproxy for IO rgws"
      module: haproxy.py
      name: "Configure HAproxy for IO rgws"

# configuring HAproxy for sync RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node1
            rgw_endpoints:
              - node4:80
              - node5:80
        ceph-sec:
          config:
            haproxy_clients:
              - node1
            rgw_endpoints:
              - node4:80
              - node5:80
        ceph-ter:
          config:
            haproxy_clients:
              - node1
            rgw_endpoints:
              - node4:80
              - node5:80
      desc: "Configure HAproxy for sync rgws"
      module: haproxy.py
      name: "Configure HAproxy for sync rgws"

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin zonegroup modify --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart rgw.shared.pri.sync"
              - "ceph orch restart rgw.shared.pri.io"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart rgw.shared.sec.sync"
              - "ceph orch restart rgw.shared.sec.io"
        ceph-ter:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone tertiary --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart rgw.shared.ter.sync"
              - "ceph orch restart rgw.shared.ter.io"
      desc: Setting up RGW multisite replication environment with haproxy for sync
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10704

  - test:
      name: get cluster and multisite information from both cluster parallely
      desc: Parallely get cluster and multisite information from both cluster
      module: test_parallel.py
      parallel:
        - test:
            abort-on-fail: true
            clusters:
              ceph-pri:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
                    - "ceph osd dump"
                    - "ceph orch ls"
            desc: Retrieve primary environment details
            module: exec.py
            name: get shared realm info on primary before starting the IO
            polarion-id: CEPH-83575227
        - test:
            abort-on-fail: true
            clusters:
              ceph-sec:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
                    - "ceph osd dump"
                    - "ceph orch ls"
            desc: Retrieve secondary environment details
            module: exec.py
            name: get shared realm info on secondary before starting the IO
            polarion-id: CEPH-83575227
        - test:
            abort-on-fail: true
            clusters:
              ceph-ter:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
                    - "ceph osd dump"
                    - "ceph orch ls"
            desc: Retrieve secondary environment details
            module: exec.py
            name: get shared realm info on secondary before starting the IO
            polarion-id: CEPH-83575227

  # Test work flow

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-ter
      desc: create non-tenanted user and copy user details on tertiary site
      module: sanity_rgw_multisite.py
      name: create non-tenanted user and copy user details on tertiary site
      polarion-id: CEPH-83575199

# configuring vault agent on all the sites

  - test:
      clusters:
        ceph-pri:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-sec:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-ter:
          config:
            install:
              - agent
            run-on-rgw: true
      desc: Setup and configure vault agent
      destroy-cluster: false
      module: install_vault.py
      name: configure vault agent
      polarion-id: CEPH-83575226

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.sec rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.sec.io"
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.pri rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.pri.io"
        ceph-ter:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.ter rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.ter.io"
      desc: Setting vault configs for sse-s3 on multisite tertiary site
      module: exec.py
      name: set sse-s3 vault configs on multisite

#Performing IOs via the tertiary zone

  - test:
      clusters:
        ceph-ter:
          config:
            set-env: true
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml
            run-on-haproxy: true
            monitor-consistency-bucket-stats: true
      desc: test M buckets multipart uploads on tertiary zone
      module: sanity_rgw_multisite.py
      name: test M buckets multipart uploads on tertiary zone
      polarion-id: CEPH-83575433

  - test:
      name: S3CMD object download on primary
      desc: S3CMD object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../s3cmd/test_s3cmd.py
            config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml
            run-on-haproxy: true
            monitor-consistency-bucket-stats: true
            # verify-io-on-site: ["ceph-sec"] as there is an io file issue, once its fixed need to uncomment

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_lifecycle_object_expiration_transition.py
            config-file-name: test_lc_cloud_transition_ibm_retain_false.yaml
      desc: test LC cloud transition to IBM cos with retain false
      polarion-id: CEPH-83581973 #CEPH-83581975
      module: sanity_rgw_multisite.py
      name: test LC cloud transition to IBM cos with retain false

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_sts_using_boto.py
            config-file-name: test_sts_multisite.yaml
      desc: test assume role at secondary, with s3 ops
      polarion-id: CEPH-83575592
      module: sanity_rgw_multisite.py
      name: test assume role at secondary, with s3 ops

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_lifecycle_object_expiration_transition.py
            config-file-name: test_lc_cloud_transition_ibm_retain_true.yaml
      desc: test LC cloud transition to IBM cos with retain true
      polarion-id: CEPH-83581972 #CEPH-83575498
      module: sanity_rgw_multisite.py
      name: test LC cloud transition to IBM cos with retain true

  - test:
      clusters:
        ceph-sec:
          config:
            config-file-name: test_Mbuckets_with_Nobjects.yaml
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: [ "ceph-pri" ]
      desc: Execute M buckets with N objects on secondary cluster
      polarion-id: CEPH-83575435
      module: sanity_rgw_multisite.py
      name: m buckets with n objects

  - test:
      name: test aws4 signature version on primary
      desc: test_Mbuckets_with_Nobjects_aws4 on primary
      polarion-id: CEPH-9637
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_aws4.yaml

  - test:
      name: delete buckets and objects on primary
      desc: test_Mbuckets_with_Nobjects_delete on primary
      polarion-id: CEPH-14237
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_delete.yaml

  - test:
      name: test encryption on primary
      desc: test_Mbuckets_with_Nobjects_enc on primary
      polarion-id: CEPH-11358 # also applies to CEPH-11361
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-sec"]
            config-file-name: test_Mbuckets_with_Nobjects_enc.yaml

  - test:
      name: test sharding on primary
      desc: test_Mbuckets_with_Nobjects_sharding on primary
      polarion-id: CEPH-83573597 # also applies to CEPH-83573593
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_sharding.yaml

  - test:
      name: replace bucket policy on primary
      desc: test_bucket_policy_replace on primary
      polarion-id: CEPH-11215
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_policy_ops.py
            config-file-name: test_bucket_policy_replace.yaml
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: suspend bucket versioning on primary
      desc: test_versioning_objects_suspend on secondary
      polarion-id: CEPH-14263
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_objects_suspend.yaml
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: reverting object to one of the versions on primary
      desc: test_versioning_objects_copy on secondary
      polarion-id: CEPH-9179
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_objects_copy.yaml

  - test:
      name: enable bucket versioning on primary
      desc: test_versioning_enable on secondary
      polarion-id: CEPH-9199
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_enable.yaml
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: ordered listing of bucket with pseudo directories and objects
      desc: measure execution time for ordered listing of bucket with pseudo directories and objects
      polarion-id: CEPH-83573545
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_pseudo_ordered.yaml

  - test:
      name: listing flat unordered buckets on primary
      desc: test_bucket_listing_flat_unordered.yaml on secondary
      polarion-id: CEPH-83573545
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_unordered.yaml

  - test:
      name: listing flat ordered versioned buckets on primary
      desc: test_bucket_listing_flat_ordered_versionsing on secondary
      polarion-id: CEPH-83573545
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_ordered_versionsing.yaml

  - test:
      name: listing flat ordered buckets on secondary
      desc: test_bucket_listing_flat_ordered on secondary
      polarion-id: CEPH-83574826
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_ordered.yaml
            monitor-consistency-bucket-stats: true

  - test:
      name: listing pseudo ordered dir only buckets on secondary
      desc: test_bucket_listing_pseudo_ordered_dir_only on secondary
      polarion-id: CEPH-83573651
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_pseudo_ordered_dir_only.yaml
            monitor-consistency-bucket-stats: true
