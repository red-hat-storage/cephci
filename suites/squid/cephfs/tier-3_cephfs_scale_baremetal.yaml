---
tests:
  -
    test:
      abort-on-fail: true
      desc: "Setup phase to deploy the required pre-requisites for running the tests."
      module: install_prereq.py
      name: "setup install pre-requisistes"
  -
    test:
      abort-on-fail: true
      config:
        steps:
          -
            config:
              args:
                mon-ip: node1
                orphan-initial-daemons: true
                registry-url: registry.redhat.io
                allow-fqdn-hostname: true
                skip-monitoring-stack: true
              base_cmd_args:
                verbose: true
              command: bootstrap
              service: cephadm
          -
            config:
              args:
                attach_ip_address: true
                labels: apply-all-labels
              command: add_hosts
              service: host
          -
            config:
              args:
                placement:
                  label: mgr
              command: apply
              service: mgr
          -
            config:
              args:
                placement:
                  label: mon
              command: apply
              service: mon
          -
            config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          -
            config:
              args:
                placement:
                  label: mds
              base_cmd_args:
                verbose: true
              command: apply
              pos_args:
                - cephfs
              service: mds
      desc: "Execute the cluster deployment workflow with label placement."
      destroy-cluster: false
      module: test_cephadm.py
      name: "cluster deployment"
      polarion-id: CEPH-83573777

  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.1
        install_packages:
        - ceph-common
        node: node201
      desc: Configure the Cephfs client system 1
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.2
        install_packages:
        - ceph-common
        node: node202
      desc: Configure the Cephfs client system 2
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.3
        install_packages:
        - ceph-common
        node: node203
      desc: Configure the Cephfs client system 3
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.4
        install_packages:
        - ceph-common
        node: node204
      desc: Configure the Cephfs client system 4
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.5
        install_packages:
        - ceph-common
        node: node205
      desc: Configure the Cephfs client system 5
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.6
        install_packages:
        - ceph-common
        node: node206
      desc: Configure the Cephfs client system 6
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.7
        install_packages:
        - ceph-common
        node: node207
      desc: Configure the Cephfs client system 7
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.8
        install_packages:
        - ceph-common
        node: node208
      desc: Configure the Cephfs client system 8
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.9
        install_packages:
        - ceph-common
        node: node209
      desc: Configure the Cephfs client system 9
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.10
        install_packages:
        - ceph-common
        node: node210
      desc: Configure the Cephfs client system 10
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.11
        install_packages:
        - ceph-common
        node: node211
      desc: Configure the Cephfs client system 11
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.12
        install_packages:
        - ceph-common
        node: node212
      desc: Configure the Cephfs client system 12
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.13
        install_packages:
        - ceph-common
        node: node213
      desc: Configure the Cephfs client system 13
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.14
        install_packages:
        - ceph-common
        node: node214
      desc: Configure the Cephfs client system 14
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.15
        install_packages:
        - ceph-common
        node: node215
      desc: Configure the Cephfs client system 15
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.16
        install_packages:
        - ceph-common
        node: node216
      desc: Configure the Cephfs client system 16
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.17
        install_packages:
        - ceph-common
        node: node217
      desc: Configure the Cephfs client system 17
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.18
        install_packages:
        - ceph-common
        node: node218
      desc: Configure the Cephfs client system 18
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.19
        install_packages:
        - ceph-common
        node: node219
      desc: Configure the Cephfs client system 19
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.20
        install_packages:
        - ceph-common
        node: node220
      desc: Configure the Cephfs client system 20
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.21
        install_packages:
        - ceph-common
        node: node221
      desc: Configure the Cephfs client system 21
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.22
        install_packages:
        - ceph-common
        node: node222
      desc: Configure the Cephfs client system 22
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.23
        install_packages:
        - ceph-common
        node: node223
      desc: Configure the Cephfs client system 23
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.24
        install_packages:
        - ceph-common
        node: node224
      desc: Configure the Cephfs client system 24
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.25
        install_packages:
        - ceph-common
        node: node225
      desc: Configure the Cephfs client system 25
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.26
        install_packages:
        - ceph-common
        node: node226
      desc: Configure the Cephfs client system 26
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.27
        install_packages:
        - ceph-common
        node: node227
      desc: Configure the Cephfs client system 27
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.28
        install_packages:
        - ceph-common
        node: node228
      desc: Configure the Cephfs client system 28
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.29
        install_packages:
        - ceph-common
        node: node229
      desc: Configure the Cephfs client system 29
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.30
        install_packages:
        - ceph-common
        node: node230
      desc: Configure the Cephfs client system 30
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.31
        install_packages:
        - ceph-common
        node: node231
      desc: Configure the Cephfs client system 31
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.32
        install_packages:
        - ceph-common
        node: node232
      desc: Configure the Cephfs client system 32
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.33
        install_packages:
        - ceph-common
        node: node233
      desc: Configure the Cephfs client system 33
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.34
        install_packages:
        - ceph-common
        node: node234
      desc: Configure the Cephfs client system 34
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.35
        install_packages:
        - ceph-common
        node: node235
      desc: Configure the Cephfs client system 35
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.36
        install_packages:
        - ceph-common
        node: node236
      desc: Configure the Cephfs client system 36
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.37
        install_packages:
        - ceph-common
        node: node237
      desc: Configure the Cephfs client system 37
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.38
        install_packages:
        - ceph-common
        node: node238
      desc: Configure the Cephfs client system 38
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.39
        install_packages:
        - ceph-common
        node: node239
      desc: Configure the Cephfs client system 39
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.40
        install_packages:
        - ceph-common
        node: node240
      desc: Configure the Cephfs client system 40
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.41
        install_packages:
        - ceph-common
        node: node241
      desc: Configure the Cephfs client system 41
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.42
        install_packages:
        - ceph-common
        node: node242
      desc: Configure the Cephfs client system 42
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.43
        install_packages:
        - ceph-common
        node: node243
      desc: Configure the Cephfs client system 43
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.44
        install_packages:
        - ceph-common
        node: node244
      desc: Configure the Cephfs client system 44
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.45
        install_packages:
        - ceph-common
        node: node245
      desc: Configure the Cephfs client system 45
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.46
        install_packages:
        - ceph-common
        node: node246
      desc: Configure the Cephfs client system 46
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.47
        install_packages:
        - ceph-common
        node: node247
      desc: Configure the Cephfs client system 47
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.48
        install_packages:
        - ceph-common
        node: node248
      desc: Configure the Cephfs client system 48
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.49
        install_packages:
        - ceph-common
        node: node249
      desc: Configure the Cephfs client system 49
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.50
        install_packages:
        - ceph-common
        node: node250
      desc: Configure the Cephfs client system 50
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      name: Fill Cluster
      module: test_parallel.py
      parallel:
      - test:
          abort-on-fail: false
          desc: "Fill the cluster with specific percentage"
          name: "Fill Cluster"
          module: test_io.py
          config:
            wait_for_io: True
            cephfs:
              "fill_data": 3
              "num_of_clients": 10
              "io_tool": "smallfile"
              "mount": "fuse"
              "batch_size": 4
              "filesystem": "cephfs"
              "mount_dir": ""
      - test:
          abort-on-fail: false
          desc: "Fill the cluster with specific percentage"
          name: "Fill Cluster"
          module: test_io.py
          config:
            wait_for_io: True
            cephfs:
              "fill_data": 3
              "num_of_clients": 10
              "io_tool": "smallfile"
              "mount": "kernel"
              "batch_size": 4
              "filesystem": "cephfs"
              "mount_dir": ""
      desc: "Fill the cluster with specific percentage"
      abort-on-fail: true

  - test:
      abort-on-fail: false
      desc: "Setup Crash configuration"
      module: cephfs_crash_util.py
      name: cephfs-crash-setup
      config:
        crash_setup: 1
        daemon_list: [ 'mds','osd','mgr','mon' ]

#LargeFile Configs Starts here :
  - test:
      name: Write 100GB of large file on 10 Subvol with 10 Client
      module: cephfs_scale.run_scale_io.py
      config:
        num_of_subvolumes: 10
        num_of_clients: 10
        io_type: largefile
        mount_type: fuse
        cleanup: true
      desc: Write 100GB of large file on 10 Subvol with 10 Client
      abort-on-fail: false

  - test:
      name: Write 100GB of large file on 50 Subvol with 50 Client
      module: cephfs_scale.run_scale_io.py
      config:
        num_of_subvolumes: 50
        num_of_clients: 50
        io_type: largefile
        mount_type: fuse
        cleanup: true
      desc: Write 100GB of large file on 50 Subvol with 50 Client
      abort-on-fail: false

  - test:
      name: Write 100GB of large file on 100 Subvol with 50 Client
      module: cephfs_scale.run_scale_io.py
      config:
        num_of_subvolumes: 100
        num_of_clients: 50
        io_type: largefile
        mount_type: fuse
        cleanup: true
      desc: Write 100GB of large file on 100 Subvol with 50 Client
      abort-on-fail: false

  - test:
        name: Write 100GB of large file on 200 Subvol with 100 Client
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 200
          num_of_clients: 100
          io_type: largefile
          mount_type : fuse
          cleanup: true
        desc: Write 100GB of large file on 200 Subvol with 100 Client
        abort-on-fail: false

  - test:
        name: Write 100GB of large file on 500 Subvol with 100 Client
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 500
          num_of_clients: 100
          io_type: largefile
          mount_type : fuse
          cleanup: true
        desc: Write 100GB of large file on 500 Subvol with 100 Client
        abort-on-fail: false

  - test:
        name: Write 100GB of large file on 1000 Subvol with 100 Client
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 1000
          num_of_clients: 100
          io_type: largefile
          mount_type : fuse
          cleanup: true
        desc: Write 100GB of large file on 1000 Subvol with 100 Client
        abort-on-fail: false

  - test:
      name: Write 100GB of large file on 10 Subvol with 10 Client
      module: cephfs_scale.run_scale_io.py
      config:
        num_of_subvolumes: 10
        num_of_clients: 10
        io_type: largefile
        mount_type: kernel
        cleanup: true
      desc: Write 100GB of large file on 10 Subvol with 10 Client (Kernel)
      abort-on-fail: false

  - test:
      name: Write 100GB of large file on 50 Subvol with 50 Client
      module: cephfs_scale.run_scale_io.py
      config:
        num_of_subvolumes: 50
        num_of_clients: 50
        io_type: largefile
        mount_type: kernel
        cleanup: true
      desc: Write 100GB of large file on 50 Subvol with 50 Client (Kernel)
      abort-on-fail: false

  - test:
      name: Write 100GB of large file on 100 Subvol with 50 Client
      module: cephfs_scale.run_scale_io.py
      config:
        num_of_subvolumes: 100
        num_of_clients: 50
        io_type: largefile
        mount_type: kernel
        cleanup: true
      desc: Write 100GB of large file on 100 Subvol with 50 Client (Kernel)
      abort-on-fail: false

  - test:
        name: Write 100GB of large file on 200 Subvol with 100 Client
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 200
          num_of_clients: 100
          io_type: largefile
          mount_type : kernel
          cleanup: true
        desc: Write 100GB of large file on 200 Subvol with 100 Client
        abort-on-fail: false

  - test:
        name: Write 100GB of large file on 500 Subvol with 100 Client
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 500
          num_of_clients: 100
          io_type: largefile
          mount_type : kernel
          cleanup: true
        desc: Write 100GB of large file on 500 Subvol with 100 Client
        abort-on-fail: false

  - test:
        name: Write 100GB of large file on 1000 Subvol with 100 Client
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 1000
          num_of_clients: 100
          io_type: largefile
          mount_type : kernel
          cleanup: true
        desc: Write 100GB of large file on 1000 Subvol with 100 Client
        abort-on-fail: false

# End of Large File IOs

#SmallFile Configs:
  - test:
        name: Write 100GB of small files on 50 Subvol with 50 Fuse Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 50
          num_of_clients: 50
          io_type: smallfile
          mount_type : fuse
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 50 Subvol with 50 Fuse Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 100 Subvol with 100 Fuse Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 100
          num_of_clients: 100
          io_type: smallfile
          mount_type : fuse
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 100 Subvol with 100 Fuse Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 200 Subvol with 100 Fuse Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 200
          num_of_clients: 100
          io_type: smallfile
          mount_type : fuse
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 200 Subvol with 100 Fuse Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 500 Subvol with 100 Fuse Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 500
          num_of_clients: 100
          io_type: smallfile
          mount_type : fuse
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 500 Subvol with 100 Fuse Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 1000 Subvol with 100 Fuse Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 1000
          num_of_clients: 100
          io_type: smallfile
          mount_type : fuse
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 1000 Subvol with 100 Fuse Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 50 Subvol with 50 kernel Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 50
          num_of_clients: 50
          io_type: smallfile
          mount_type : kernel
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 50 Subvol with 50 kernel Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 100 Subvol with 100 kernel Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 100
          num_of_clients: 100
          io_type: smallfile
          mount_type : kernel
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 100 Subvol with 100 kernel Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 200 Subvol with 100 kernel Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 200
          num_of_clients: 100
          io_type: smallfile
          mount_type : kernel
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 200 Subvol with 100 kernel Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 500 Subvol with 100 kernel Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 500
          num_of_clients: 100
          io_type: smallfile
          mount_type : kernel
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 500 Subvol with 100 kernel Clients and 1 thread
        abort-on-fail: false

  - test:
        name: Write 100GB of small files on 1000 Subvol with 100 kernel Clients and 1 thread
        module: cephfs_scale.run_scale_io.py
        config:
          num_of_subvolumes: 1000
          num_of_clients: 100
          io_type: smallfile
          mount_type : kernel
          smallfile_operation : create
          smallfile_threads : 1
          smallfile_file_size : 1024
          smallfile_files : 100000
          cleanup: true
        desc: Write 100GB of small files on 1000 Subvol with 100 kernel Clients and 1 thread
        abort-on-fail: false


# FIO Configs:
  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 kernel clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 kernel clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 kernel clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 kernel clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 kernel clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 kernel clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 kernel clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 kernel clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: kernel
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 kernel clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 50
        num_of_subvolumes: 50
        cleanup: true
      desc: Write 100GB with fio on 50 Subvols with 50 fuse clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 50 Subvols with 50 fuse clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 100
        cleanup: true
      desc: Write 100GB with fio on 100 Subvols with 100 fuse clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 100 Subvols with 100 fuse clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 200
        cleanup: true
      desc: Write 100GB with fio on 200 Subvols with 100 fuse clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 200 Subvols with 100 fuse clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 500
        cleanup: true
      desc: Write 100GB with fio on 500 Subvols with 100 fuse clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 500 Subvols with 100 fuse clients with 32k bs 16 iodepth

#####

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 4k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 4k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 4K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 4k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 4k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 8k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 8k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 8K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 8k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 8k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 16k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 16k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 16K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 16k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 16k bs 16 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 8
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 32k bs 8 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 32k bs 8 iodepth

  - test:
      abort-on-fail: false
      config:
        fio_block_size: 32K
        fio_depth: 16
        fio_direct: 1
        fio_engine: libaio
        fio_file_size: 100G
        fio_operation: randwrite
        io_type: fio
        mount_type: fuse
        num_of_clients: 100
        num_of_subvolumes: 1000
        cleanup: true
      desc: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 32k bs 16 iodepth
      module: cephfs_scale.run_scale_io.py
      name: Write 100GB with fio on 1000 Subvols with 100 fuse clients with 32k bs 16 iodepth

#####
