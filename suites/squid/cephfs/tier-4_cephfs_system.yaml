---
#=======================================================================================================================
# Tier-level: 4
# Test-Suite: tier-4_cephfs_system.yaml
# Conf file : conf/reef/cephfs/tier-2_cephfs_upgrade.yaml
# Test-Case Covered:
#   CEPH-11261: MON node power failure, with client IO
#   CEPH-11263: MDS node power failure, with client IO
#   CEPH-11262: OSD node power failure, with client IO
#   CEPH-11254: MON node failures ops with client IO
#   CEPH-11256: MDS node failures ops with client IO
#   CEPH-11255: OSD node failures ops with client IO
#   CEPH-11266: mds_node_remove_with_IO
#   CEPH-11311: mds_nfs_node_failure_ops
#   CEPH-11320: snap_rollback_with_node_reboots.py
#   CEPH-11241: shutdown_mds_bring_one_by_one
#   CEPH-11240: mds_shutdown_together_io_continue
#   CEPH-83572891: MDS admin socket dump testing
#   CEPH-83572890: MDS admin socket perf flush log testing
#.  CEPH-83595235: MDS standby replay node failures ops with client IO
#.  CEPH-83591709: MDS Standy-Replay system testing
#   CEPH-83594640: MDS Cache Trimming and Client Caps Recall test
#=======================================================================================================================
tests:
  -
    test:
      abort-on-fail: true
      desc: "Setup phase to deploy the required pre-requisites for running the tests."
      module: install_prereq.py
      name: "setup install pre-requisistes"
  -
    test:
      abort-on-fail: true
      config:
        steps:
          -
            config:
              args:
                mon-ip: node1
                orphan-initial-daemons: true
                registry-url: registry.redhat.io
                allow-fqdn-hostname: true
                skip-monitoring-stack: true
              base_cmd_args:
                verbose: true
              command: bootstrap
              service: cephadm
          -
            config:
              args:
                attach_ip_address: true
                labels: apply-all-labels
              command: add_hosts
              service: host
          -
            config:
              args:
                placement:
                  label: mgr
              command: apply
              service: mgr
          -
            config:
              args:
                placement:
                  label: mon
              command: apply
              service: mon
          -
            config:
              args:
                all-available-devices: true
              command: apply
              service: osd
          -
            config:
              args:
                - ceph
                - fs
                - volume
                - create
                - cephfs
              command: shell
          -
            config:
              args:
                placement:
                  label: mds
              base_cmd_args:
                verbose: true
              command: apply
              pos_args:
                - cephfs
              service: mds
          - config:
              args:
                - ceph
                - fs
                - set
                - cephfs
                - max_mds
                - "2"
              command: shell
        verify_cluster_health: true
      desc: "Execute the cluster deployment workflow with label placement."
      destroy-cluster: false
      module: test_cephadm.py
      name: "cluster deployment"
      polarion-id: CEPH-83573777
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.1
        install_packages:
          - ceph-common
        node: node8
      desc: "Configure the Cephfs client system 1"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.2
        install_packages:
          - ceph-common
        node: node9
      desc: "Configure the Cephfs client system 2"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  - test:
      abort-on-fail: false
      desc: "Fill the cluster with specific percentage"
      module: test_io.py
      name: "Fill the cluster with specific percentage"
      config:
        wait_for_io: True
        cephfs:
          "fill_data": 20
          "io_tool": "smallfile"
          "mount": "fuse"
          "filesystem": "cephfs_1"
          "mount_dir": ""
  - test:
        abort-on-fail: false
        desc: "MDS admin socket dump testing"
        module: cephfs_system.mds_admin_daemon_socket_dump.py
        polarion-id: CEPH-83572891
        name: "MDS admin socket dump testing"
  - test:
        abort-on-fail: false
        desc: "MDS admin socket perf flush log testing"
        module: cephfs_system.mds_admin_daemon_socket_perf_flush_log.py
        polarion-id: CEPH-83572890
        name: "MDS admin socket perf flush log testing"
  - test:
      abort-on-fail: false
      desc: "MON node power failure, with client IO"
      module: cephfs_system.mon_failure_with_client_IO.py
      polarion-id: CEPH-11261
      name: "MON node power failure, with client IO"
      config:
          num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "MDS node power failure, with client IO"
      module: cephfs_system.mds_failure_with_client_IO.py
      polarion-id: CEPH-11263
      name: "MDS node power failure, with client IO"
      config:
        num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "OSD node power failure, with client IO"
      module: cephfs_system.osd_failure_with_client_IO.py
      polarion-id: CEPH-11262
      name: "OSD node power failure, with client IO"
      config:
          num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "MON node failures ops with client IO"
      module: cephfs_system.mon_node_failure_ops.py
      polarion-id: CEPH-11254
      name: "MON node power failure with client IO"
      config:
        num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "MDS node failures ops with client IO"
      module: cephfs_system.mds_node_failure_ops.py
      polarion-id: CEPH-11256
      name: "MDS node power failure with client IO ops"
      config:
        num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "OSD node failures ops with client IO"
      module: cephfs_system.osd_node_failure_ops.py
      polarion-id: CEPH-11255
      name: "OSD node power failure with client IO ops"
      config:
        num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "mds_node_remove_with_IO"
      module: cephfs_system.mds_node_remove_with_IO.py
      polarion-id: CEPH-11266
      name: "mds_node_remove_with_IO"
  - test:
      abort-on-fail: false
      desc: "mds_nfs_node_failure_ops"
      module: cephfs_system.mds_nfs_node_failure_ops.py
      polarion-id: CEPH-11311
      name: "mds_nfs_node_failure_ops.py"
  - test:
      abort-on-fail: false
      desc: "snap_rollback_with_node_reboots.py"
      module: cephfs_system.snap_rollback_with_node_reboots.py
      polarion-id: CEPH-11320
      name: "snap_rollback_with_node_reboots.py"
  - test:
      abort-on-fail: false
      desc: "shutdown_mds_bring_one_by_one"
      module: cephfs_system.shutdown_mds_bring_one_by_one.py
      polarion-id: CEPH-11241
      name: "shutdown_mds_bring_one_by_one.py"
  - test:
      abort-on-fail: false
      desc: "mds_shutdown_together_io_continue"
      module: cephfs_system.mds_shutdown_together_io_continue.py
      polarion-id: CEPH-11240
      name: "mds_shutdown_together_io_continue.py"
  - test:
      abort-on-fail: false
      desc: "StandbyReplay MDS node failures ops with client IO"
      module: cephfs_system.mds_standby_replay_node_failure_ops.py
      polarion-id: CEPH-83595235
      name: "mds_standby_replay_node_failure_ops"
      config:
        num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "MDS Standy-Replay system testing"
      module: cephfs_system.mds_failover_standby_replay_systemic_test.py
      polarion-id: CEPH-83591710
      name: "mds_failover_standby_replay_systemic_test"
  - test:
      abort-on-fail: false
      desc: "MDS Cache Trimming and Client Caps Recall test"
      module: cephfs_system.mds_cache_trimming_caps_recall.py
      polarion-id: CEPH-83594640
      name: "mds_cache_trimming_caps_recall"
  - test:
      abort-on-fail: false
      desc: "Fill the cluster with 95 percentage and delete the contents"
      module: cephfs_system.fill_cluster_full_del.py
      polarion-id: CEPH-11328
      name: "Fill the cluster with 95 percentage and delete the contents"
      config:
        wait_for_io: True
        cephfs:
          "fill_data": 65
          "io_tool": "smallfile"
          "mount": "fuse"
          "filesystem": "cephfs_io"
          "mount_dir": ""
