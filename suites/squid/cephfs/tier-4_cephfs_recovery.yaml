---
#=======================================================================================================================
# Tier-level: 4
# Test-Suite: tier-4_cephfs_recovery.yaml
# Conf file : conf/pacific/cephfs/tier-2_cephfs_9-node-cluster.yaml
# options : --cloud baremetal if required to run on baremetal
# Test-Case Covered:
#	CEPH-83573264 - [Cephfs]Taking the cephfs down with down flag.
#   CEPH-11267 - Execute FS Scrub Commands on cephfs
#   CEPH-83575627 : Verify Snapshot retention works even after service restarts
#   CEPH-11264 - Move data and meta_data pool to different root level bucket in OSD tree.

#=======================================================================================================================
tests:
  -
    test:
      abort-on-fail: true
      desc: "Setup phase to deploy the required pre-requisites for running the tests."
      module: install_prereq.py
      name: "setup install pre-requisistes"
  -
    test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                mon-ip: node1
                orphan-initial-daemons: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: shell
              args:          # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - create
                - cephfs-data-ec
                - "64"
                - erasure
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - create
                - cephfs-metadata
                - "64"
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - set
                - cephfs-data-ec
                - allow_ec_overwrites
                - "true"
          - config:
              command: shell
              args: # arguments to ceph orch
                - ceph
                - fs
                - new
                - cephfs-ec
                - cephfs-metadata
                - cephfs-data-ec
                - --force
          - config:
              command: apply
              service: mds
              base_cmd_args:          # arguments to ceph orch
                verbose: true
              pos_args:
                - cephfs              # name of the filesystem
              args:
                placement:
                  label: mds
          - config:
              args:
                - ceph
                - fs
                - set
                - cephfs
                - max_mds
                - "2"
              command: shell
      destroy-cluster: false
      abort-on-fail: true
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.1
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node8
      desc: "Configure the Cephfs client system 1"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.2
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node9
      desc: "Configure the Cephfs client system 2"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.3
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node10
      desc: "Configure the Cephfs client system 3"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.4
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node11
      desc: "Configure the Cephfs client system 4"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  - test:
      abort-on-fail: false
      desc: "Taking the cephfs down with down flag"
      module: cephfs_recovery.fs_down_flag.py
      polarion-id: CEPH-83573264
      name: "Taking the cephfs down with down flag"
      comments: "Product bug - BZ-2225891"
  - test:
      abort-on-fail: false
      desc: "Execute FS Scrub Commands on cephfs"
      module: cephfs_recovery.fs_scrub.py
      polarion-id: CEPH-11267
      name: "Cephfs Scrubing"
  - test:
      name: snap_retention_service_restart
      module: snapshot_clone.snap_schedule_retention_vol_subvol.py
      polarion-id: CEPH-83575627
      desc: Verify Snapshot retention works even after service restarts
      abort-on-fail: false
      config:
        test_name : snap_retention_service_restart
  - test:
      name: Move data and meta_data pool to different root level bucket in OSD tree.
      module: cephfs_generic.test_data_migrate_bw_buckets.py
      polarion-id: CEPH-11264
      desc: Move data and meta_data pool to different root level bucket in OSD tree.
      abort-on-fail: false
      comments: "Unable to install ceph-base"
  - test:
      abort-on-fail: false
      desc: "cephfs_journal_tool_journal_mode_functional"
      module: cephfs_journal_tool.cephfs_journal_tool_journal_mode.py
      name: journal_tool_journal_mode
      polarion-id: "CEPH-83594249"
  - test:
      abort-on-fail: false
      desc: "cephfs_journal_tool_header_mode_functional"
      module: cephfs_journal_tool.cephfs_journal_tool_header_mode.py
      name: journal_tool_header_mode
      polarion-id: "CEPH-83594266"
  - test:
      abort-on-fail: false
      desc: "cephfs_journal_tool_event_mode_functional"
      module: cephfs_journal_tool.cephfs_journal_tool_event_mode.py
      name: journal_tool_event_mode
      polarion-id: "CEPH-83595482"
      comments: "BZ 2335321"
