#===============================================================================================
#-------------------------------------
#---    Test Suite for Nfs Ganesha ---
#-------------------------------------
# Conf: conf/squid/nfs/1admin-7node-3client.yaml
# Smoke test cases for
#    - Bootstrap
#    - Host management
#    - Configure nfs-ganesha on nfs server,do mount on any client and do IOs
#    - Test NFS cluster and export create
#
#===============================================================================================
tests:

  - test:
      name: Setup Upstream NFS and Ceph
      module: nfs_setup_upstream_with_bootstrap.py
      abort-on-fail: true
  - test:
      abort-on-fail: true
      config:
        args:
          attach_ip_address: true
          labels: apply-all-labels
        command: add_hosts
        service: host
      desc: "Adding hosts to the cluster using labels and IP information."
      destroy-cluster: false
      module: test_host.py
      name: "Test host add with labels and IP"
  - test:
      abort-on-fail: true
      config:
        args:
          all-available-devices: true
        command: apply
        service: osd
      desc: "Deploying OSD daemons using all-available-devices option."
      destroy-cluster: false
      module: test_osd.py
      name: "Test apply osd with all-available-devices option"

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node4
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.2
        node: node5
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.3
        node: node6
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.4
        node: node7
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      name: Verify create file, create hardlink and lookups from nfs clients
      module: upstream_nfs_verify_file_ops_hard_links.py
      desc: Verify create file, create soflink and lookups from nfs clients
      polarion-id: CEPH-83577597
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 3
        file_count: 100

  - test:
      name: Modifying file attributes such as size, modification time, and access time
      module: upstream_nfs_verify_file_modification.py
      desc: Verify create file, create soflink and lookups from nfs clients
      polarion-id: CEPH-83577602
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 3
        file_count: 100

# SKIP : Need to analyse the client mount issue when specified in conf
#  - test:
#     name: Nfs export with client permission
#     module: nfs_client_permission_export.py
#     desc: Test nfs export with authorized client mount
#     polarion-id: CEPH-83578394
#     abort-on-fail: false
#     config:
#       nfs_version: 4.2
#       clients: 2

#  SKIP : Need to analyse the dd getting stuck issue
#  - test:
#     name: Nfs Test File Truncate
#     module: nfs_file_truncate.py
#     desc: Perform file truncate operation from clients
#     polarion-id: CEPH-83577598
#     abort-on-fail: false
#     config:
#       nfs_version: 4.2
#       clients: 2

  - test:
      name: Nfs Verify multiple parallel io and lookups
      module: upstream_nfs_verify_multiple_parallel_io_and_lookups.py
      desc: Perform look ups while multiple parallel io are in progress
      polarion-id: CEPH-83581304
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 4
  - test:
      name: Nfs Verify rm write lookups in parellel from multi clients
      module: upstream_nfs_verify_parallel_rm_write_lookup.py
      desc: Perform lookups rm and write at the same time
      polarion-id: CEPH-83577591
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 4

  - test:
     name: Nfs Test File Append
     module: upstream_nfs_file_append.py
     desc: Perform file append operation from clients
     polarion-id: CEPH-83577599
     abort-on-fail: false
     config:
       nfs_version: 4.2
       clients: 4

  - test:
      name: Nfs Verify read write operation permissions
      module: upstream_nfs_verify_read_write_operations.py
      desc: Perform read write without permissions on nfs share
      polarion-id: CEPH-83575941
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 3
        operation: verify_permission

  - test:
      name: Nfs verify read and write non existing file
      module: upstream_nfs_verify_read_write_operations.py
      desc: Perform read write of no existing file on nfs share
      polarion-id: CEPH-83575927
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 3
        operation: verify_non_existing_file

# SKIP : Skipping as it used ceph nfs command for export conf get
#  - test:
#      name: Nfs verify export unexport while nfs share is in use
#      module: nfs_verify_stress.py
#      desc: Stress by performing admin ops like exports unexports while clients are actively using the NFS shares.
#      polarion-id: CEPH-83575994
#      abort-on-fail: false
#      config:
#        nfs_version: 4.2
#        clients: 3
#
  - test:
      name: Nfs Ganesha copy operations
      module: upstream_test_file_ops_copy.py
      desc: Perform file and dir copy and lookups in parallel
      polarion-id: CEPH-83577595
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 3
        num_files: 1000
        num_dirs: 1000

  - test:
      name: Nfs Ganesha rename operations
      module: upstream_test_file_ops_renames.py
      desc: Perform file and dir renaming and lookups in parallel
      polarion-id: CEPH-83577594
      abort-on-fail: false
      config:
        nfs_version: 4.2
        clients: 3
        num_files: 1000
        num_dirs: 1000

# SKIP : Mount v3 not supported upstream
#  - test:
#      name: Nfs Ganesha test multi mount versions
#      module: nfs_verify_multi_mount_version.py
#      desc: Perform different mount versions on clients and validate the behavior
#      polarion-id: CEPH-83577595
#      abort-on-fail: false
#      config:
#        clients: 3
#        nfs_version: [4.2: 2, 3: 1]  # Out of the 3 clients, 2 will be mounted with v4.2 and other with v3
