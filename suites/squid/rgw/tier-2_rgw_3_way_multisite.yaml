# Test suite for RGW 3-way active active multi-site deployment scenario.
#
# This suite deploys a single realm (India) spanning across two RHCS clusters.
# It has a zonegroup (shared) which also spans across the clusters. There
# exists a
# master zone - pri
# secondary zone - sec
# tertiary zone - ter

# The deployment is evaluated by running IOs across the environments.
# This particular yaml runs the tests on the primary and verifies IOs on the
# secondary site and tertiary site.

# In addition to the above, the data bucket is configured to use EC 2+2
# Sync IO rgws & Client IO rgws are sepearated
# global-conf: conf/squid/rgw/3-way-ms-ec-profile-2+2@4nodes.yaml
---

tests:

  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  args:
                    - "ceph osd erasure-code-profile set rgwec22_4 k=2 m=2"
                    - "crush-failure-domain=host crush-device-class=hdd"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool create primary.rgw.buckets.data 32 32"
                    - "erasure rgwec22_4"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool application enable"
                    - "primary.rgw.buckets.data rgw"
                  command: shell
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri.sync
                  args:
                    placement:
                      count-per-host: 2
                      nodes:
                        - node3
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri.io
                  args:
                    placement:
                      count-per-host: 2
                      nodes:
                        - node4
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  args:
                    - "ceph osd erasure-code-profile set rgwec22_4 k=2 m=2"
                    - "crush-failure-domain=host crush-device-class=hdd"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool create secondary.rgw.buckets.data 32 32"
                    - "erasure rgwec22_4"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool application enable"
                    - "secondary.rgw.buckets.data rgw"
                  command: shell
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec.sync
                  args:
                    placement:
                      count-per-host: 2
                      nodes:
                        - node3
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec.io
                  args:
                    placement:
                      count-per-host: 2
                      nodes:
                        - node4
        ceph-ter:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  args:
                    - "ceph osd erasure-code-profile set rgwec22_4 k=2 m=2"
                    - "crush-failure-domain=host crush-device-class=hdd"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool create tertiary.rgw.buckets.data 32 32"
                    - "erasure rgwec22_4"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool application enable"
                    - "tertiary.rgw.buckets.data rgw"
                  command: shell
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.ter.sync
                  args:
                    placement:
                      count-per-host: 2
                      nodes:
                        - node3
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.ter.io
                  args:
                    placement:
                      count-per-host: 2
                      nodes:
                        - node4
      desc: RHCS cluster deployment using cephadm.
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
      polarion-id: CEPH-10117


  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node4
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node4
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-ter:
          config:
            command: add
            id: client.1
            node: node4
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758

# configuring HAproxy for IO RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node4
            rgw_endpoints:
              - node4:80
              - node4:81

        ceph-sec:
          config:
            haproxy_clients:
              - node4
            rgw_endpoints:
              - node4:80
              - node4:81
        ceph-ter:
          config:
            haproxy_clients:
              - node4
            rgw_endpoints:
              - node4:80
              - node4:81

      desc: "Configure HAproxy for IO rgws"
      module: haproxy.py
      name: "Configure HAproxy for IO rgws"

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node3}:80 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node3}:80 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 3way123 --secret 3way123 --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 3way123 --secret 3way123"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.shared.pri.sync rgw_realm india"
              - "ceph config set client.rgw.shared.pri.sync rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.pri.sync rgw_zone primary"
              - "ceph config set client.rgw.shared.pri.io rgw_realm india"
              - "ceph config set client.rgw.shared.pri.io rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.pri.io rgw_zone primary"
              - "ceph config set client.rgw.shared.pri.io rgw_run_sync_thread false"
              - "ceph orch restart {service_name:shared.pri.io}"
              - "ceph orch restart {service_name:shared.pri.sync}"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node3}:80 --access-key 3way123 --secret 3way123 --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node3}:80 --access-key 3way123 --secret 3way123"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node3}:80 --access-key 3way123 --secret 3way123"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.shared.sec.sync rgw_realm india"
              - "ceph config set client.rgw.shared.sec.sync rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.sec.sync rgw_zone secondary"
              - "ceph config set client.rgw.shared.sec.io rgw_realm india"
              - "ceph config set client.rgw.shared.sec.io rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.sec.io rgw_zone secondary"
              - "ceph config set client.rgw.shared.sec.io rgw_run_sync_thread false"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
        ceph-ter:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node3}:80 --access-key 3way123 --secret 3way123 --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node3}:80 --access-key 3way123 --secret 3way123"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone tertiary --endpoints http://{node_ip:node3}:80 --access-key 3way123 --secret 3way123"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.shared.ter.sync rgw_realm india"
              - "ceph config set client.rgw.shared.ter.sync rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.ter.sync rgw_zone tertiary"
              - "ceph config set client.rgw.shared.ter.io rgw_realm india"
              - "ceph config set client.rgw.shared.ter.io rgw_zonegroup shared"
              - "ceph config set client.rgw.shared.ter.io rgw_zone tertiary"
              - "ceph config set client.rgw.shared.ter.io rgw_run_sync_thread false"
              - "ceph orch restart {service_name:shared.ter.io}"
              - "ceph orch restart {service_name:shared.ter.sync}"
      desc: Setting up 3-way RGW multisite replication environment
      module: exec.py
      name: setup 3-way multisite
      polarion-id: CEPH-10704

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83575227
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on secondary
      polarion-id: CEPH-83575227

  - test:
      abort-on-fail: true
      clusters:
        ceph-ter:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on tertiary
      polarion-id: CEPH-83575227


  # Test work flow

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-ter
      desc: create non-tenanted user and copy user details on tertiary site
      module: sanity_rgw_multisite.py
      name: create non-tenanted user and copy user details on tertiary site
      polarion-id: CEPH-83575199

# configuring vault agent on all the sites

  - test:
      clusters:
        ceph-pri:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-sec:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-ter:
          config:
            install:
              - agent
            run-on-rgw: true
      desc: Setup and configure vault agent
      destroy-cluster: false
      module: install_vault.py
      name: configure vault agent
      polarion-id: CEPH-83575226

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.sec rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.sec.io"
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.pri rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.pri.io"
        ceph-ter:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.ter rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.ter rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.ter.io"
      desc: Setting vault configs for sse-s3 on multisite tertiary site
      module: exec.py
      name: set sse-s3 vault configs on multisite

#Performing IOs via the tertiary zone

  - test:
      clusters:
        ceph-ter:
          config:
            set-env: true
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml
            run-on-haproxy: true
            monitor-consistency-bucket-stats: true
      desc: test M buckets multipart uploads on tertiary zone
      module: sanity_rgw_multisite.py
      name: test M buckets multipart uploads on tertiary zone
      polarion-id: CEPH-83575433

  - test:
      name: S3CMD object download on primary
      desc: S3CMD object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../s3cmd/test_s3cmd.py
            config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml
            run-on-haproxy: true
            monitor-consistency-bucket-stats: true
            # verify-io-on-site: ["ceph-sec"] as there is an io file issue, once its fixed need to uncomment

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_lifecycle_object_expiration_transition.py
            config-file-name: test_lc_cloud_transition_ibm_retain_false.yaml
      desc: test LC cloud transition to IBM cos with retain false
      polarion-id: CEPH-83581973 #CEPH-83581975
      module: sanity_rgw_multisite.py
      name: test LC cloud transition to IBM cos with retain false

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_sts_using_boto.py
            config-file-name: test_sts_multisite.yaml
      desc: test assume role at secondary, with s3 ops
      polarion-id: CEPH-83575592
      module: sanity_rgw_multisite.py
      name: test assume role at secondary, with s3 ops

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_lifecycle_object_expiration_transition.py
            config-file-name: test_lc_cloud_transition_ibm_retain_true.yaml
      desc: test LC cloud transition to IBM cos with retain true
      polarion-id: CEPH-83581972 #CEPH-83575498
      module: sanity_rgw_multisite.py
      name: test LC cloud transition to IBM cos with retain true
