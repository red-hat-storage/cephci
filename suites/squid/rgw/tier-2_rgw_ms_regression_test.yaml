# This suites perform disabling and re enabling re-sharding feature and verifying DBR behavior
# Verfies bucket generation
# global-conf: conf/squid/rgw/rgw_multisite.yaml
tests:

  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri
                  args:
                    placement:
                      nodes:
                        - node5
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec
                  args:
                    placement:
                      nodes:
                        - node5
      desc: RHCS cluster deployment using cephadm.
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zonegroup modify --rgw-realm india --rgw-zonegroup shared --enable-feature resharding"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary"
              - "ceph orch restart {service_name:shared.pri}"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary"
              - "ceph orch restart {service_name:shared.sec}"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10362
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83575227
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on secondary
      polarion-id: CEPH-83575227
  # create user from primary
  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create tenanted user
      module: sanity_rgw_multisite.py
      name: create tenanted user
      polarion-id: CEPH-83574433

  - test:
      name: Delete object version with id null
      desc: test_delete_object_version_id_null
      polarion-id: CEPH-83576431
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../aws/test_delete_version_id_null.py
            config-file-name: ../../aws/configs/test_delete_version_id_null.yaml

# Bucket generation verify
  - test:
      name: Bucket generation verification
      desc: Bucket generation verification
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83574423
      clusters:
        ceph-pri:
          config:
            script-name: test_dynamic_bucket_resharding.py
            config-file-name: test_bucket_generation.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]

  - test:
      name: Test Max Bucket generations
      desc: Test Max Bucket generations
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83574625
      clusters:
        ceph-pri:
          config:
            script-name: test_dynamic_bucket_resharding.py
            config-file-name: test_max_generations.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]

#  Dynamic resharding tests
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_bucket_chown_reshard.yaml
            script-name: test_dynamic_bucket_resharding.py
      desc: Resharding test - Test manual reshard after bucket owner change
      name: Test manual reshard after bucket owner change
      polarion-id: CEPH-83574627
      module: sanity_rgw_multisite.py
  - test:
      name: Disable DBR feature on zonegroup
      desc: Disable DBR feature on zonegroup
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83574626
      clusters:
        ceph-pri:
          config:
            script-name: test_dynamic_bucket_resharding.py
            config-file-name: test_resharding_disable_in_zonegroup.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]

  - test:
      name: Re-enable DBR feature on cluster and Verify
      desc: Re-enable DBR feature on cluster an verify
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83573596
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_check_sharding_enabled_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dbr_tenanted_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
      desc: Test dynamic resharding on greenfield deployment tenanted user
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster tenanted user
      polarion-id: CEPH-83573596
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dbr_versioning_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
      desc: Test dynamic resharding on versioned bucket
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster
      polarion-id: CEPH-83573596
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dbr_versioning_tenanted_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
      desc: Test dynamic resharding on versioned bucket and tenanted user
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on versioned bucket and tenanted user Primary cluster
      polarion-id: CEPH-83573596
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_dynamic_resharding_quota_exceed.yaml
            script-name: test_dynamic_bucket_resharding.py
      desc: test exceeding quota limit on dynamically resharded bucket able to access bucket on secondary
      name: test exceeding quota limit on dynamically resharded bucket able to access bucket on secondary
      polarion-id: CEPH-83574669
      module: sanity_rgw_multisite.py
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_sync_disable_enable.yaml
      desc: Test sync enable and disable with manual resharding
      module: sanity_rgw_multisite.py
      name: Manual Resharding tests on Primary cluster
      polarion-id: CEPH-83574434

  - test:
      name: bucket granular sync policy with sync from differnt bucket
      desc: Test bucket granular sync with sync from differnt bucket
      polarion-id: CEPH-83575140
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_sync_from_diff_bucket.yaml
            timeout: 5500

  - test:
      name: bucket granular sync policy with sync to differnt bucket
      desc: Test bucket granular sync with sync to differnt bucket
      polarion-id: CEPH-83575141
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_sync_to_diff_bucket.yaml
            timeout: 5500

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_zone_deletion.yaml
      desc: Test zone deletion in master
      module: sanity_rgw_multisite.py
      name: Perform zone deletion in master
      polarion-id: CEPH-10753

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_realm_rename.yaml
      desc: Test realm rename in master
      module: sanity_rgw_multisite.py
      name: Perform realm rename in master
      polarion-id: CEPH-10739
