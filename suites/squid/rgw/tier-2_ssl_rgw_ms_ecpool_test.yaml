# Test suite for evaluating RGW multi-site having SSL endpoints.
#
# This suite deploys a single realm (India) spanning across two RHCS clusters. It has a
# zonegroup (shared) which also spans across the clusters. There exists a master (pri)
# and secondary (sec) zones within this group. The master zone is part of the pri
# cluster whereas the sec zone is part of the sec datacenter (cluster). It also uses
# EC pool 4+2 configuration

# The deployment is evaluated by running IOs across the environments.
# global-conf: conf/squid/rgw/ms-ec-profile-4+2-cluster.yaml
---
tests:

  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  args:
                    - "ceph osd erasure-code-profile set rgwec01 k=4 m=2"
                    - "crush-failure-domain=host crush-device-class=hdd"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool create primary.rgw.buckets.data 32 32"
                    - "erasure rgwec01"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool application enable"
                    - "primary.rgw.buckets.data rgw"
                  command: shell
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.pri
                      spec:
                        ssl: true
                        rgw_frontend_ssl_certificate: create-cert
                      placement:
                        nodes:
                          - node7
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  args:
                    - "ceph osd erasure-code-profile set rgwec01 k=4 m=2"
                    - "crush-failure-domain=host crush-device-class=hdd"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool create secondary.rgw.buckets.data 32 32"
                    - "erasure rgwec01"
                  command: shell
              - config:
                  args:
                    - "ceph osd pool application enable"
                    - "secondary.rgw.buckets.data rgw"
                  command: shell
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.sec
                      placement:
                        nodes:
                          - node7
                      spec:
                        ssl: true
                        rgw_frontend_ssl_certificate: create-cert
      desc: RHCS cluster deployment using cephadm.
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
  - test:
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node8
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node8
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints https://{node_ip:node7} --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints https://{node_ip:node7} --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary"
              - "ceph config set client.rgw rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_verify_ssl False"
              - "ceph orch restart {service_name:shared.pri}"
        ceph-sec:
          config:
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url https://{node_ip:ceph-pri#node7} --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url https://{node_ip:ceph-pri#node7} --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints https://{node_ip:node7} --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary"
              - "ceph orch restart {service_name:shared.sec}"
              - "sleep 120"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10362
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83575227
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on secondary
      polarion-id: CEPH-83575227

  # Test work flow

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

  - test:
      name: enable compression on secondary
      desc: test_Mbuckets_with_Nobjects_compression on secondary
      polarion-id: CEPH-11350
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-pri"]
            config-file-name: test_Mbuckets_with_Nobjects_compression.yaml

  - test:
      name: download objects on secondary
      desc: test_Mbuckets_with_Nobjects_download on secondary
      polarion-id: CEPH-14237
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-pri"]
            config-file-name: test_Mbuckets_with_Nobjects_download.yaml

  - test:
      name: test encryption on secondary
      desc: test_Mbuckets_with_Nobjects_enc on secondary
      polarion-id: CEPH-11358 # also applies to CEPH-11361
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-pri"]
            config-file-name: test_Mbuckets_with_Nobjects_enc.yaml

  - test:
      name: Test large omap objects warnings via ceph status in a multisite
      desc: Test large omap objects warnings via ceph status in a multisite on secondary
      polarion-id: CEPH-83574987
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-pri"]
            config-file-name: test_Mbuckets.yaml

  - test:
      name: LargeObjGet_GC test
      desc: test_LargeObjGet_GC on secondary
      polarion-id: CEPH-83574416
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_LargeObjGet_GC.py
            config-file-name: test_LargeObjGet_GC.yaml

  - test:
      name: enabling bucket versioning and uploading objects on secondary
      polarion-id: CEPH-14261 # also applies to CEPH-9222 and CEPH-10652
      desc: test_versioning_objects_enable on secondary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_objects_enable.yaml
            verify-io-on-site: ["ceph-pri"]

  - test:
      name: setting acls to versioned objects on secondary
      polarion-id: CEPH-9190
      desc: test_versioning_objects_acls on secondary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_objects_acls.yaml
            verify-io-on-site: ["ceph-pri"]

  - test:
      name: listing pseudo ordered dir only buckets on secondary
      polarion-id: CEPH-83573651
      desc: test_bucket_listing_pseudo_ordered_dir_only on secondary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_pseudo_ordered_dir_only.yaml

  - test:
      name: create tenanted user
      desc: create tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: tenanted_user.yaml
            copy-user-info-to-site: ceph-sec

  - test:
      name: modify bucket policy on secondary
      desc: test_bucket_policy_modify.yaml on secondary
      polarion-id: CEPH-11633
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_bucket_policy_ops.py
            config-file-name: test_bucket_policy_modify.yaml
            verify-io-on-site: ["ceph-pri"]
