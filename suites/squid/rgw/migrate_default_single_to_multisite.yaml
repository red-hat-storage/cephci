tests:
  # Conf file is conf/squid/rgw/migrate_default_single_to_multisite.yaml
  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - rgw.default
                  args:
                    placement:
                      nodes:
                        - node5
                        - node4
                        - node3
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - rgw.default
                  args:
                    placement:
                      nodes:
                        - node5
                        - node4
                        - node3
      desc: RHCS cluster deployment using cephadm.
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758

#Configure HAProxy on client nodes and port 5000
# configuring HAproxy on the client node 'node6' and port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node4:80
              - node5:80
              - node3:80
        ceph-pri:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node4:80
              - node5:80
              - node3:80

      desc: "Configure HAproxy"
      module: haproxy.py
      name: "Configure HAproxy"
#Perform some IOs on separate sites before converting to multisite

  - test:
      clusters:
        ceph-sec:
          config:
            set-env: true
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_compression_haproxy.yaml
            run-on-haproxy: true
      desc: test M buckets compression on haproxy node
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      name: test M buckets compression on haproxy node

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_multipart_haproxy.yaml
            run-on-haproxy: true
            timeout: 5000
      desc: test M buckets multipart uploads on haproxy node
      module: sanity_rgw_multisite.py
      name: test M buckets multipart uploads on haproxy node
      polarion-id: CEPH-83575433

#Convert single site to multisite with default configuration
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup rename --rgw-zonegroup default --zonegroup-new-name shared"
              - "radosgw-admin zone rename --rgw-zone default --zone-new-name primary --rgw-zonegroup shared"
              - "radosgw-admin zonegroup modify --rgw-realm india --commit --rgw-zonegroup shared --endpoints  http://{node_ip:node6}:5000 --master --default"
              - "radosgw-admin zone modify --rgw-realm india --commit --rgw-zonegroup shared --rgw-zone primary --endpoints  http://{node_ip:node6}:5000 --master --default"
              - "radosgw-admin user create --uid sync-user --display-name sync-user --access-key a123 --secret s123 --system"
              - "radosgw-admin zone modify --rgw-realm india --commit --rgw-zonegroup shared --rgw-zone primary  --access-key a123 --secret s123"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.rgw.default rgw_realm india"
              - "ceph config set client.rgw.rgw.default rgw_zonegroup shared"
              - "ceph config set client.rgw.rgw.default rgw_zone primary"
              - "ceph orch restart rgw.rgw.default"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node6}:5000 --access-key a123 --secret s123 --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node6}:5000 --access-key a123 --secret s123"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node6}:5000 --access-key a123 --secret s123 "
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.rgw.default rgw_realm india"
              - "ceph config set client.rgw.rgw.default rgw_zonegroup shared"
              - "ceph config set client.rgw.rgw.default rgw_zone secondary"
              - "ceph orch restart rgw.rgw.default"
      desc: Single site to multisite conversion on default configuration
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-83587141

  - test:
      clusters:
        ceph-pri:
          config:
            extra-pkgs:
              - jq
            script-name: test_rgw_restore_index_tool.py
            config-file-name: test_rgw_restore_index_versioned_buckets.yaml
            run-on-haproxy: true
            monitor-consistency-bucket-stats: true
            timeout: 7200
      desc: test rgw restore index tool on versioned bucket
      polarion-id: CEPH-83575473
      module: sanity_rgw_multisite.py
      name: test rgw restore index tool on versioned bucket

  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_Mbuckets_with_Nobjects_no_reshard_haproxy.yaml
            script-name: test_Mbuckets_with_Nobjects.py
            run-on-haproxy: true
            timeout: 5000
      desc: Execute M buckets with N objects on primary and verify on secondary cluster
      polarion-id: CEPH-83575575
      module: sanity_rgw_multisite.py
      name: m buckets with n objects

  - test:
      name: RGW multipart object expiration through lc
      desc: RGW multipart object expiration through lc
      polarion-id: CEPH-83574834
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_lc_object_exp_multipart.py
            config-file-name: test_bucket_lc_object_exp_multipart.yaml
            verify-io-on-site: ["ceph-sec"]
