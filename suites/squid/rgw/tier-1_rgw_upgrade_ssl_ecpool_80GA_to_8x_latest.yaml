#Objective: Testing single site ssl setup upgrade from RHCS 8.0 G.A latest to 8.x latest build
#platform : RHEL-9
#conf: conf/squid/rgw/tier-0_rgw.yaml

tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: Setup pre-requisites

  - test:
      abort-on-fail: true
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                rhcs-version: 8.0
                release: rc
                registry-url: registry.redhat.io
                mon-ip: node1
                orphan-initial-daemons: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              args:
                - "ceph osd erasure-code-profile set rgwecprofile01 k=4 m=2"
                - "crush-failure-domain=host crush-device-class=hdd"
              command: shell
          - config:
              args:
                - "ceph osd pool create default.rgw.buckets.data 32 32"
                - "erasure rgwecprofile01"
              command: shell
          - config:
              args:
                - "ceph osd pool create default.rgw.buckets.index 32 32"
              command: shell
          - config:
              args:
                - "ceph osd pool application enable"
                - "default.rgw.buckets.data rgw"
              command: shell
          - config:
              args:
                - "ceph osd pool application enable"
                - "default.rgw.buckets.index rgw"
              command: shell
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
          # - config:
          #     command: apply_spec
          #     service: orch
          #     specs:
          #       - service_type: rgw
          #         service_id: rgw.ssl
          #         placement:
          #           nodes:
          #             - node7
          #         spec:
          #           ssl: true
          #           rgw_frontend_ssl_certificate: create-cert
      desc: bootstrap and deployment services with label placements.
      polarion-id: CEPH-83573777
      destroy-cluster: false
      module: test_cephadm.py
      name: Deploy RHCS-ssl cluster using cephadm

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node6
        install_packages:
          - ceph-common
        copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: Configure client
      polarion-id: CEPH-83573758

  - test:
      abort-on-fail: true
      config:
        haproxy_clients:
          - node6
        rgw_endpoints:
          - node5:80
      desc: "Configure HAproxy"
      module: haproxy.py
      name: "Configure HAproxy"

  - test:
      abort-on-fail: true
      config:
        install:
          - agent
        run-on-rgw: true
      desc: Setup and configure vault agent
      destroy-cluster: false
      module: install_vault.py
      name: configure vault agent
      polarion-id: CEPH-83575226

  - test:
      name: Manual Resharding tests pre upgrade
      desc: Resharding test - manual
      polarion-id: CEPH-83571740
      module: sanity_rgw.py
      config:
        script-name: test_dynamic_bucket_resharding.py
        config-file-name: test_manual_resharding_without_bucket_delete.yaml
        run-on-haproxy: true

  - test:
      name: Dynamic Resharding tests with version pre upgrade
      desc: Resharding test - dynamic
      polarion-id: CEPH-83571740
      module: sanity_rgw.py
      config:
        script-name: test_dynamic_bucket_resharding.py
        config-file-name: test_dynamic_resharding_with_version_without_bucket_delete.yaml
        run-on-haproxy: true

  - test:
      name: S3CMD small and multipart object download pre upgrade
      desc: S3CMD small and multipart object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml
        run-on-haproxy: true

  - test:
      name: compression_with_zstd_type pre upgrade
      desc: test compression with zstd type pre upgrade
      polarion-id: CEPH-11350
      module: sanity_rgw.py
      config:
        run-on-haproxy: true
        script-name: test_Mbuckets_with_Nobjects.py
        config-file-name: test_Mbuckets_with_Nobjects_compression_zstd.yaml

  - test:
      config:
        script-name: test_sse_s3_kms_with_vault.py
        config-file-name: test_sse_s3_per_bucket_encryption_normal_object_upload.yaml
      desc: test sse-s3 per bucket encryption pre upgrade
      module: sanity_rgw.py
      name: sse-s3 per bucket encryption test pre upgrade
      polarion-id: CEPH-83574615 # CEPH-83574617, CEPH-83575151

  - test:
      name: Test NFS cluster and export create
      desc: Test NFS cluster and export create
      polarion-id: CEPH-83574597
      module: sanity_rgw.py
      config:
        run-on-rgw: true
        script-name: ../nfs_ganesha/nfs_cluster.py
        config-file-name: ../../nfs_ganesha/config/nfs_cluster.yaml

  # upgrade cluster
  - test:
      name: Parallel run
      desc: RGW upgarde and IO parallelly.
      module: test_parallel.py
      parallel:
        - test:
            desc: test to create "M" no of buckets and "N" no of objects with download
            module: sanity_rgw.py
            name: Test download with M buckets with N objects
            polarion-id: CEPH-14237
            config:
              script-name: test_Mbuckets_with_Nobjects.py
              config-file-name: test_Mbuckets_with_Nobjects_download.yaml
              run-on-haproxy: true
        - test:
            name: Upgrade cluster to latest 8.x ceph version
            desc: Upgrade cluster to latest version
            module: test_cephadm_upgrade.py
            polarion-id: CEPH-83573791
            abort-on-fail: true
            verify_cluster_health: true
            config:
              command: start
              service: upgrade
              base_cmd_args:
                verbose: true

# Post Upgrade tests

  - test:
      desc: Retrieve the versions of the cluster
      module: exec.py
      name: post upgrade gather version
      polarion-id: CEPH-83575200
      config:
        cephadm: true
        commands:
          - "ceph versions"

  - test:
      name: S3CMD small and multipart object download post upgrade
      desc: S3CMD small and multipart object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml
        run-on-haproxy: true

  - test:
      config:
        script-name: test_sse_s3_kms_with_vault.py
        config-file-name: test_sse_s3_per_bucket_encryption_multipart_object_upload.yaml
      module: sanity_rgw.py
      desc: test sse-s3 per bucket encryption with multipart post upgrade
      name: test sse-s3 per bucket encryption with multipart post upgrade
      polarion-id: CEPH-83575155

  - test:
      name: compresstion_with_zstd_type post upgarde
      desc: test compresstion with zstd type post upgarde
      polarion-id: CEPH-11350
      module: sanity_rgw.py
      config:
        run-on-haproxy: true
        script-name: test_Mbuckets_with_Nobjects.py
        config-file-name: test_Mbuckets_with_Nobjects_compression_zstd.yaml

  - test:
      name: NFS export delete
      desc: NFS cluster and exports delete
      polarion-id: CEPH-83574600 # also covers CEPH-83574601
      module: sanity_rgw.py
      config:
        script-name: ../nfs_ganesha/nfs_cluster.py
        config-file-name: ../../nfs_ganesha/config/nfs_cluster_delete.yaml
