#Objective: Testing single site ssl setup upgrade from RHCS 8.0 G.A latest to 8.x latest build
#platform : RHEL-9
#conf: conf/squid/rgw/ec-profile-4+2-cluster.yaml

tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: Setup pre-requisites

  - test:
      abort-on-fail: true
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                rhcs-version: 8.1
                release: released
                registry-url: registry.redhat.io
                mon-ip: node1
                orphan-initial-daemons: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              args:
                - "ceph osd erasure-code-profile set rgwecprofile01 k=4 m=2"
                - "crush-failure-domain=host crush-device-class=hdd"
              command: shell
          - config:
              args:
                - "ceph osd pool create default.rgw.buckets.data 32 32"
                - "erasure rgwecprofile01"
              command: shell
          - config:
              args:
                - "ceph osd pool create default.rgw.buckets.index 32 32"
              command: shell
          - config:
              args:
                - "ceph osd pool application enable"
                - "default.rgw.buckets.data rgw"
              command: shell
          - config:
              args:
                - "ceph osd pool application enable"
                - "default.rgw.buckets.index rgw"
              command: shell
          - config:
              command: apply_spec
              service: orch
              specs:
                - service_type: rgw
                  service_id: rgw.ssl
                  placement:
                    nodes:
                      - node7
                  spec:
                    ssl: true
                    generate_cert: true
      desc: bootstrap and deployment services with label placements.
      polarion-id: CEPH-83573777
      destroy-cluster: false
      module: test_cephadm.py
      name: Deploy RHCS-ssl cluster using cephadm

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node8
        install_packages:
          - ceph-common
        copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: Configure client
      polarion-id: CEPH-83573758

  - test:
      abort-on-fail: true
      config:
        commands:
          - "ceph config set client.rgw.rgw.ssl rgw_verify_ssl False"
          - "ceph_version=$(ceph version | cut -d ' ' -f 3 | cut -d '-' -f 1); if [ \"$ceph_version\" = \"19.2.0\" ]; then ceph orch cert-store get cert cephadm_root_ca_cert > /home/cephuser/cephadm-root-ca.crt; else ceph orch certmgr cert get cephadm_root_ca_cert > /home/cephuser/cephadm-root-ca.crt; fi"
          - "sudo yum install -y sshpass"
          - "sleep 20"
          - "sshpass -p 'passwd' scp /home/cephuser/cephadm-root-ca.crt root@{node_ip:node7}:/etc/pki/ca-trust/source/anchors/"
          - "sshpass -p 'passwd' scp /home/cephuser/cephadm-root-ca.crt root@{node_ip:node8}:/etc/pki/ca-trust/source/anchors/"
      name: copy-crt to rgw and client node
      desc: Copying crt to rgw and client node
      polarion-id: CEPH-83575227
      module: exec.py

  - test:
      abort-on-fail: true
      config:
        role: rgw
        sudo: True
        commands:
          - "update-ca-trust"
      desc: update-ca-trust on rgw node
      polarion-id: CEPH-83575227
      module: exec.py
      name: update-ca-trust on rgw node
  - test:
      abort-on-fail: true
      config:
        role: client
        sudo: True
        commands:
          - "update-ca-trust"
      desc: update-ca-trust on client node
      polarion-id: CEPH-83575227
      module: exec.py
      name: update-ca-trust on client node

  - test:
      name: Manual Resharding tests pre upgrade
      desc: Resharding test - manual
      polarion-id: CEPH-83571740
      module: sanity_rgw.py
      config:
        script-name: test_dynamic_bucket_resharding.py
        config-file-name: test_manual_resharding_without_bucket_delete.yaml

  - test:
      name: Dynamic Resharding tests with version pre upgrade
      desc: Resharding test - dynamic
      polarion-id: CEPH-83571740
      module: sanity_rgw.py
      config:
        script-name: test_dynamic_bucket_resharding.py
        config-file-name: test_dynamic_resharding_with_version_without_bucket_delete.yaml

  - test:
      name: S3CMD small and multipart object download pre upgrade
      desc: S3CMD small and multipart object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml

  - test:
      name: compression_with_zstd_type pre upgrade
      desc: test compression with zstd type pre upgrade
      polarion-id: CEPH-11350
      module: sanity_rgw.py
      config:
        script-name: test_Mbuckets_with_Nobjects.py
        config-file-name: test_Mbuckets_with_Nobjects_compression_zstd.yaml

  - test:
      name: Test NFS cluster and export create
      desc: Test NFS cluster and export create
      polarion-id: CEPH-83574597
      module: sanity_rgw.py
      config:
        run-on-rgw: true
        script-name: ../nfs_ganesha/nfs_cluster.py
        config-file-name: ../../nfs_ganesha/config/nfs_cluster.yaml

  # upgrade cluster
  - test:
      abort-on-fail: true
      name: Parallel run
      desc: RGW upgarde and IO parallelly.
      module: test_parallel.py
      parallel:
        - test:
            desc: test to create "M" no of buckets and "N" no of objects with download
            module: sanity_rgw.py
            name: Test download with M buckets with N objects
            polarion-id: CEPH-14237
            config:
              script-name: test_Mbuckets_with_Nobjects.py
              config-file-name: test_Mbuckets_with_Nobjects_download.yaml
        - test:
            name: Upgrade cluster to latest 8.x ceph version
            desc: Upgrade cluster to latest version
            module: test_cephadm_upgrade.py
            polarion-id: CEPH-83573791
            verify_cluster_health: true
            config:
              command: start
              service: upgrade
              base_cmd_args:
                verbose: true

# Post Upgrade tests

  - test:
      desc: Retrieve the versions of the cluster
      module: exec.py
      name: post upgrade gather version
      polarion-id: CEPH-83575200
      config:
        cephadm: true
        commands:
          - "ceph versions"

  - test:
      name: S3CMD small and multipart object download post upgrade
      desc: S3CMD small and multipart object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml

  - test:
      name: compresstion_with_zstd_type post upgarde
      desc: test compresstion with zstd type post upgarde
      polarion-id: CEPH-11350
      module: sanity_rgw.py
      config:
        script-name: test_Mbuckets_with_Nobjects.py
        config-file-name: test_Mbuckets_with_Nobjects_compression_zstd.yaml

  - test:
      name: NFS export delete
      desc: NFS cluster and exports delete
      polarion-id: CEPH-83574600 # also covers CEPH-83574601
      module: sanity_rgw.py
      config:
        script-name: ../nfs_ganesha/nfs_cluster.py
        config-file-name: ../../nfs_ganesha/config/nfs_cluster_delete.yaml

  - test:
      abort-on-fail: true
      config:
        install:
          - agent
        run-on-rgw: true
      desc: Setup and configure vault agent
      destroy-cluster: false
      module: install_vault.py
      name: configure vault agent
      polarion-id: CEPH-83575226

  - test:
      config:
        script-name: test_sse_s3_kms_with_vault.py
        config-file-name: test_sse_s3_per_bucket_encryption_normal_object_upload.yaml
      desc: test sse-s3 per bucket encryption post upgrade
      module: sanity_rgw.py
      name: sse-s3 per bucket encryption test post upgrade
      polarion-id: CEPH-83574615 # CEPH-83574617, CEPH-83575151

  - test:
      config:
        script-name: test_sse_s3_kms_with_vault.py
        config-file-name: test_sse_s3_per_bucket_encryption_multipart_object_upload.yaml
      module: sanity_rgw.py
      desc: test sse-s3 per bucket encryption with multipart post upgrade
      name: test sse-s3 per bucket encryption with multipart post upgrade
      polarion-id: CEPH-83575155
