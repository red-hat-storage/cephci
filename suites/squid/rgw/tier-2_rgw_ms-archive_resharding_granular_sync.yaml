tests:

  # Cluster deployment stage
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri
                  args:
                    placement:
                      nodes:
                        - node5
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec
                  args:
                    placement:
                      nodes:
                        - node5
        ceph-arc:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.arc
                  args:
                    placement:
                      nodes:
                        - node5
      desc: RHCS cluster deployment using cephadm.
      polarion-id: CEPH-83573386
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-arc:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      polarion-id: CEPH-83573758
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary"
              - "ceph orch restart {service_name:shared.pri}"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary"
              - "ceph orch restart {service_name:shared.sec}"
        ceph-arc:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone archive --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --tier-type=archive"
              - "radosgw-admin zone modify  --rgw-realm india --rgw-zonegroup shared --rgw-zone archive  --sync-from-all false --sync-from-rm secondary --sync-from primary"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_zone archive"
              - "radosgw-admin zone modify --rgw-zone archive --sync_from primary --sync_from_all false"
              - "radosgw-admin period update --commit"
              - "radosgw-admin period get"
              - "ceph orch restart {service_name:shared.arc}"
      desc: Setting up RGW multisite replication environment with archive zone
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10362

  - test:
      clusters:
        ceph-pri:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-sec:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-arc:
          config:
            install:
              - agent
            run-on-rgw: true
      desc: Setup and configure vault agent
      destroy-cluster: false
      module: install_vault.py
      name: configure vault agent
      polarion-id: CEPH-83575226

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_crypt_sse_s3_vault_secret_engine transit"
              - "radosgw-admin zone placement modify --rgw-zone secondary --placement-id default-placement  --compression zlib"
              - "radosgw-admin period update --commit"
              - "ceph orch restart {service_name:shared.sec}"
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_crypt_sse_s3_vault_secret_engine transit"
              - "radosgw-admin zone placement modify --rgw-zone primary --placement-id default-placement  --compression zlib"
              - "radosgw-admin period update --commit"
              - "ceph orch restart {service_name:shared.pri}"
        ceph-arc:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_crypt_sse_s3_vault_secret_engine transit"
              - "radosgw-admin zone placement modify --rgw-zone archive --placement-id default-placement  --compression zlib"
              - "radosgw-admin period update --commit"
              - "ceph orch restart {service_name:shared.arc}"
      desc: Setting vault configs for sse-s3 on multisite
      module: exec.py
      name: sse-s3 vault configs, and zlib compression
      polarian-id: CEPH-83575916

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199
  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-arc
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_Mbuckets_with_Nobjects.yaml
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 5000
      desc: Execute M buckets with N objects on primary and verify on secondary & archive cluster
      polarion-id: CEPH-9789
      module: sanity_rgw_multisite.py
      name: m buckets with n objects
# Dynamic resharding test
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_dynamic_resharding_without_bucket_delete.yaml
            script-name: test_dynamic_bucket_resharding.py
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
      desc: Resharding test - dynamic resharding on primary and verify on secondary & archive cluster
      name: Dynamic Resharding - dynamic
      polarion-id: CEPH-83571740
      module: sanity_rgw_multisite.py

  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_dynamic_resharding_with_version_without_bucket_delete.yaml
            script-name: test_dynamic_bucket_resharding.py
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
      desc: Resharding test - dynamic resharding on versioned bucket on primary and verify on secondary & archive cluster
      name: Dynamic Resharding with versioning on primary and verify on archive
      polarion-id: CEPH-83575393
      module: sanity_rgw_multisite.py

  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_manual_resharding_without_bucket_delete.yaml
            script-name: test_dynamic_bucket_resharding.py
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
      desc: Resharding test - manual resharding
      name: Manual Resharding on primary and verify on secondary & archive cluster
      polarion-id: CEPH-83571740
      module: sanity_rgw_multisite.py
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bilog_trimming.py
            config-file-name: test_bilog_trim_archive.yaml
      desc: test no bilogs are generated at archive zone
      polarion-id: CEPH-83575468
      module: sanity_rgw_multisite.py
      name: test no bilogs are generated at archive zone
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_sse_s3_kms_with_vault.py
            config-file-name: test_archive_versions.yaml
      desc: test multipart download at remote site
      polarion-id: CEPH-83575862
      module: sanity_rgw_multisite.py
      name: test how many object versions archived at archive zone
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_sse_s3_kms_with_vault.py
            config-file-name: test_sse_s3_bucket_enc_multipart_download_remote_site.yaml
      desc: test multipart download at remote site
      polarion-id: CEPH-11357
      module: sanity_rgw_multisite.py
      name: test multipart download at remote site
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_sse_s3_kms_with_vault.py
            config-file-name: test_sse_s3_bucket_enc_multipart_download_archive_site.yaml
      desc: test multipart+encrypt object access at the archive site
      polarion-id: CEPH-83573390
      module: sanity_rgw_multisite.py
      name: test multipart+encrypt object access at the archive site

  - test:
      name: bucket granular sync policy with directional flow having archive zone
      desc: Test bucket granular sync policy with directional flow having archive zone
      polarion-id: CEPH-83575879
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_archive_directional.yaml
            timeout: 5500
