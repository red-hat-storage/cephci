tests:
   - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

   - test:
      name: containerized ceph ansible
      polarion-id: CEPH-83571503
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with collocated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: docker to podman with osd with collocated journal
      polarion-id: CEPH-83573538
      module: docker_to_podman.py
      desc: docker to podman with osd with collocated journal
 
   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571493
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
      desc: Purge ceph cluster

   - test:
      name: containerized ceph ansible
      polarion-id: CEPH-83571502
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            dmcrypt: True
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: encrypted osd with collocated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: docker to podman encrypted osd with collocated journal
      polarion-id: CEPH-83573538
      module: docker_to_podman.py
      desc: docker to podman encrypted osd with collocated journal

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571493
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
      desc: Purge ceph cluster

   - test:
      name: containerized ceph ansible
      polarion-id: CEPH-83571497
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: non-collocated
            dedicated_devices:
              - /dev/vde
              - /dev/vde
              - /dev/vde
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with dedicated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: docker to podman osd with dedicated journal
      polarion-id: CEPH-83573538
      module: docker_to_podman.py
      desc: docker to podman osd with dedicated journal

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571493
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
      desc: Purge ceph cluster

   - test:
      name: containerized ceph ansible
      polarion-id: CEPH-83571495
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: non-collocated
            dmcrypt: True
            dedicated_devices:
              - /dev/vde
              - /dev/vde
              - /dev/vde
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: encrypted osd with dedicated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: docker to podman encrypted osd with dedicated journal
      polarion-id: CEPH-83573538
      module: docker_to_podman.py
      desc: docker to podman encrypted osd with dedicated journal

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571493
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
      desc: Purge ceph cluster

   - test:
      name: containerized ceph ansible
      polarion-id: CEPH-83571499
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            dmcrypt: True
            osd_auto_discovery: True
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with collocated journal and autodiscovery
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: docker to podman osd with collocated journal and autodiscovery
      polarion-id: CEPH-83573538
      module: docker_to_podman.py
      desc: docker to podman osd with collocated journal and autodiscovery

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571493
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
      desc: Purge ceph cluster

   - test:
      name: containerized ceph ansible
      polarion-id: CEPH-83571503
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with collocated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: docker to podman osd with collocated journal
      polarion-id: CEPH-83573538
      module: docker_to_podman.py
      desc: docker to podman osd with collocated journal

   - test:
      name: config roll over
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
          add:
              - node:
                  node-name: .*node11.*
                  demon:
                      - mon
      desc: add containerized monitor

   - test:
      name: config roll over
      polarion-id: CEPH-9583
      module: test_ansible_roll_over.py
      config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
          add:
              - node:
                  node-name: .*node12.*
                  demon:
                      - osd
      desc: add new containerized osd node


   - test:
      name: config roll over
      polarion-id: CEPH-9582
      module: test_ansible_roll_over.py
      config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            ceph_docker_image: rhceph/rhceph-3-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
          add:
              - node:
                  node-name: .*node12.*
                  demon:
                      - osd
      desc: add containerized osd to existing node


   - test:
      name: shrink mon
      polarion-id: CEPH-9584
      module: shrink_mon.py
      config:
           mon-to-kill:
            - .*node6.*
      desc: remove containerized monitor


   - test:
      name: shrink osd
      polarion-id: CEPH-9585
      module: shrink_osd.py
      config:
          osd-to-kill:
            - 2
      desc: remove containerized osd


   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571493
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
      desc: Purge ceph cluster
      destroy-cluster: True
  
