tests:
   - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571467
      module: test_ansible.py
      config:
        build: '3.x'
        use_cdn: True
        is_mixed_lvm_configs: True
        ansi_config:
            ceph_test: False
            ceph_origin: repository
            ceph_stable_release: luminous
            ceph_repository_type: cdn
            ceph_rhcs_version: 3
            ceph_repository: rhcs
            osd_scenario: lvm
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with 6 osd scenarios with lvm
      destroy-cluster: False
      abort-on-fail: true

# basic rbd tests

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info

# basic ansible tests

   - test:
      name: config roll over
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          build: '3.x'
          use_cdn: True
          is_mixed_lvm_configs: True
          ansi_config:
              ceph_test: False
              ceph_origin: repository
              ceph_stable_release: luminous
              ceph_repository_type: cdn
              ceph_rhcs_version: 3
              ceph_repository: rhcs
              osd_scenario: lvm
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node10.*
                  demon:
                      - mon
      desc: add mon

   - test:
      name: config roll over
      polarion-id: CEPH-9583
      module: test_ansible_roll_over.py
      config:
          build: '3.x'
          use_cdn: True
          is_mixed_lvm_configs: True
          ansi_config:
              ceph_test: False
              ceph_origin: repository
              ceph_stable_release: luminous
              ceph_repository_type: cdn
              ceph_rhcs_version: 3
              ceph_repository: rhcs
              osd_scenario: lvm
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node11.*
                  demon:
                      - osd
      desc: add new osd node with lvm scenario

   - test:
      name: config roll over
      polarion-id: CEPH-9582
      module: test_ansible_roll_over.py
      config:
          build: '3.x'
          upstream: True
          is_mixed_lvm_configs: True
          device: /dev/vdc
          ansi_config:
              ceph_test: False
              ceph_origin: repository
              ceph_stable_release: luminous
              ceph_repository_type: cdn
              ceph_rhcs_version: 3
              ceph_repository: rhcs
              osd_scenario: lvm
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node11.*
                  demon:
                      - osd
      desc: add osd to existing node with lvm scenario


   # - test:
   #    name: shrink mon
   #    polarion-id: CEPH-9584
   #    module: shrink_mon.py
   #    config:
   #         mon-to-kill:
   #          - .*node6.*
   #    desc: remove monitor


   # - test:
   #    name: shrink osd
   #    polarion-id: CEPH-9585
   #    module: shrink_osd.py
   #    config:
   #        osd-to-kill:
   #          - 2
   #    desc: shrink osd

# basic rgw tests

   # - test:
   #    name: s3 tests
   #    module: test_s3.py
   #    config:
   #       branch: ceph-luminous
   #    desc: execution of s3 tests against radosgw
   #    destroy-cluster: False

#above upstream tests script has known failure, will add once its fixed later


   # - test:
   #     name: Mbuckets
   #     desc: test to create "M" no of buckets
   #     polarion-id: CEPH-9789
   #     module: sanity_rgw.py
   #     config:
   #         script-name: test_Mbuckets_with_Nobjects.py
   #         config-file-name: test_Mbuckets.yaml
   #         timeout: 300

   - test:
       name: Mbuckets_with_Nobjects
       desc: test to create "M" no of buckets and "N" no of objects
       polarion-id: CEPH-9789
       module: sanity_rgw.py
       config:
           script-name: test_Mbuckets_with_Nobjects.py
           config-file-name: test_Mbuckets_with_Nobjects.yaml
           timeout: 300

   - test:
       name: Mbuckets_with_Nobjects_delete
       desc: test to create "M" no of buckets and "N" no of objects with delete
       module: sanity_rgw.py
       polarion-id: CEPH-14237
       config:
           script-name: test_Mbuckets_with_Nobjects.py
           config-file-name: test_Mbuckets_with_Nobjects_delete.yaml
           timeout: 300

# basic cephfs tests

   - test:
       name: cephfs-lifecycle ops
       module: CEPH-11333.py
       polarion-id: CEPH-11333
       desc: Perfrom cephfs lifecycle ops like delete cephfs,recreate cephfs
       abort-on-fail: false

   - test:
       name: multi-client-rw-io
       module: CEPH-10528_10529.py
       polarion-id: CEPH-10528,CEPH-10529
       desc: Single CephFS on multiple clients,performing IOs and checking file locking mechanism
       abort-on-fail: false

   - test:
       name: cephfs-basics
       module: cephfs_basic_tests.py
       polarion-id: CEPH-11293,CEPH-11296,CEPH-11297,CEPH-11295
       desc: cephfs basic operations
       abort-on-fail: false

# basic rados tests

   - test:
      name: LRCec pool io test_9281
      module: test_9281.py
      polarion-id: CEPH-9281
      desc: LRCec pool with IO and osd failures

   - test:
      name: Different k,m,l:test_9322
      module: test_9322.py
      polarion-id: CEPH-9322
      desc: LRCec pool with different k,m,l values

   - test:
      name: corrupt snaps test_9928
      module: test_9928.py
      polarion-id: CEPH-9928
      desc: corrupt snapset of an object and list-inconsistent-snapset

   - test:
      name: corrupt object in ec pool test_9929
      module: test_9929.py
      polarion-id: CEPH-9929
      desc: corrupt an object in ec pool and list-inconsistent-obj

   # - test:
   #    name: ceph ansible purge
   #    polarion-id: CEPH-83571498
   #    module: purge_cluster.py
   #    config:
   #          ansible-dir: /usr/share/ceph-ansible
   #    desc: Purge ceph cluster
#above test fails to due to some unknown issue , will add once its fixed later
