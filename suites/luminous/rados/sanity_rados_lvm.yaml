tests:
   - test:
       name: install ceph pre-requisites
       module: install_prereq.py
       abort-on-fail: true

   - test:
       name: ceph ansible
       polarion-id: CEPH-83571467
       module: test_ansible.py
       config:
         is_mixed_lvm_configs: True
         ansi_config:
           ceph_test: True
           ceph_origin: distro
           ceph_stable_release: luminous
           ceph_repository: rhcs
           osd_scenario: lvm
           journal_size: 1024
           ceph_stable: True
           ceph_stable_rh_storage: True
           fetch_directory: ~/fetch
           copy_admin_key: true
           ceph_conf_overrides:
             global:
               osd_pool_default_pg_num: 64
               osd_default_pool_size: 2
               osd_pool_default_pgp_num: 64
               mon_max_pg_per_osd: 1024
             mon:
               mon_allow_pool_delete: true
             client:
               rgw crypt require ssl: false
               rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                 testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
           cephfs_pools:
             - name: "cephfs_data"
               pgs: "8"
             - name: "cephfs_metadata"
               pgs: "8"
       desc: osd with lvm scenario
       destroy-cluster: False
       abort-on-fail: true

   - test:
      name: LRCec pool io test_9281
      module: test_9281.py
      polarion-id: CEPH-9281
      desc: LRCec pool with IO and osd failures

   - test:
      name: corrupt snaps test_9928
      module: test_9928.py
      polarion-id: CEPH-9928
      desc: corrupt snapset of an object and list-inconsistent-snapset
   - test:
      name: corrupt object in ec pool test_9929
      module: test_9929.py
      polarion-id: CEPH-9929
      desc: corrupt an object in ec pool and list-inconsistent-obj
   - test:
      name: Delete snapset of an object test_9939
      module: test_9939.py
      polarion-id: CEPH-9939
      desc: Delete snapset of an object followed by list-inconsistent-obj
   - test:
      name: Corrupt snapset of an object on non-primary test_83571452
      module: test_83571452.py
      polarion-id: CEPH-83571452
      desc: Corrupt snapset of an object on non-primary followed \
             by list-inconsistent-*
   - test:
      name: Corrupt snapset on primary osd in ec pool test_83571453
      module: test_83571453.py
      polarion_id: CEPH-83571453
      desc: Corrupt snapset on a primary osd in ec pool
   - test:
      name: LRCec pool local recovery test_9311
      module: test_9311.py
      polarion_id: CEPH-9311
      desc: Trigger recovery from local codes in LRCpool
   - test:
      name: switch between simple and async messenger test_11538
      module: test_11538.py
      polarion_id: CEPH-11538
      desc: switch between simple and async messenger
   - test:
      name: remove omap key on a replica
      module: test_9925.py
      polarion_id: CEPH-9925
      desc: remove a known omap item of a replica and list-inconsistent-obj
   - test:
      name: corrupt omap on a replica
      module: test_9924.py
      polarion_id: CEPH-9924
      desc: rewrite a known omap item of a replica and check list-inconsistent-obj

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
         ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
      destroy-cluster: False

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571467
      module: test_ansible.py
      config:
         is_mixed_lvm_configs: True
         ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: lvm
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            dashboard_enabled: False
            ceph_conf_overrides:
               global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
               mon:
                  mon_allow_pool_delete: true
               client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                     testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with lvm scenario
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: deep scrub and osd memory utilization
      module: test_bz_1754078.py
      config:
            pg_count: 500
            timeout: 300
      desc: run deep scrub and check each osd daemon memory usage

