# RHCS 6.x sanity test suite for RGW daemon with S3CMD.

# Runs the Object Gateway tests from https://github.com/red-hat-storage/ceph-qe-scripts/tree/master/rgw
# each script under the above repo has a yaml ( config defined ) which is actually a test
# We are calling the script and associated yaml here.
#
# examples:
# config:
#    script-name: test_s3cmd.py
#    config-file-name: test_s3cmd.yaml
#
# some of the other config option for this yamls are
#
# 1. To run verification of io generated.
#  config:
#    < script-name >
#    < config-file-name >
#    run_io_verify: true or false - to run
#
#  ---------------
# 2. To specify any external packages to be installed for a particular test
#  a. distro specific packages
#  config:
#    <script-name>
#    <config-file-name>
#    extra-pkgs:
#      7:
#        - pkg1
#        - pkg2
#      8:
#        - pkg1
#        - pkg2
#
#
#  b. just list of packages which are not distro dependent
#  config:
#    <script-name>
#    <config-file-name>
#    extra-pkgs:
#      - pkg1
#      - pkg2

tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                mon-ip: node1
                orphan-initial-daemons: true
                skip-monitoring-stack: true
                initial-dashboard-password: admin@123
                dashboard-password-noupdate: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.all
              args:
                placement:
                  label: rgw
      desc: RHCS cluster deployment using cephadm.
      destroy-cluster: false
      module: test_cephadm.py
      polarion-id: CEPH-83573713
      name: deploy cluster

  - test:
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727
      config:
        steps:
          - config:
              command: apply_spec
              service: orch
              validate-spec-services: true
              specs:
                - service_type: prometheus
                  placement:
                    count: 1
                    nodes:
                      - node1
                - service_type: grafana
                  placement:
                    nodes:
                      - node1
                - service_type: alertmanager
                  placement:
                    count: 1
                - service_type: node-exporter
                  placement:
                    host_pattern: "*"
                - service_type: crash
                  placement:
                    host_pattern: "*"

  - test:
      abort-on-fail: true
      config:
        haproxy_clients:
          - node5
        rgw_endpoints:
          - node5:80
      desc: "Configure HAproxy"
      module: haproxy.py
      name: "Configure HAproxy"

#   S3CMD tests
  - test:
      name: S3CMD basic  operations
      desc: S3CMD basic  operations
      polarion-id: CEPH-83573244
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_s3cmd.yaml

  - test:
      name: S3CMD large object download with GC
      desc: S3CMD large object download with GC
      polarion-id: CEPH-83574416
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_large_object_gc.py
        config-file-name: ../../s3cmd/configs/test_large_object_gc.yaml

  - test:
      name: S3CMD bucket stats consistency
      desc: S3CMD bucket stats consistency
      polarion-id: CEPH-83574668
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_bucket_stats.py
        config-file-name: ../../s3cmd/configs/test_bucket_stats.yaml

  - test:
      name: S3CMD object header size check
      desc: S3CMD object header size check
      polarion-id: CEPH-83574691
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_header_size.py
        config-file-name: ../../s3cmd/configs/test_header_size.yaml

  - test:
      name: Test rgw opslog io though s3cmd
      desc: S3CMD tests, Testing rgw opslog
      polarion-id: CEPH-83575194 # also CEPH-83575470
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_rgw_ops_log.py
        config-file-name: ../../s3cmd/configs/test_rgw_opslog.yaml
        run-on-rgw: true # as logs generated on rgw nodes

  - test:
      name: Test single delete marker for versioned object using s3cmd
      desc: Test single delete marker for versioned object using s3cmd
      polarion-id: CEPH-83574806
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_multiple_delete_marker_check.yaml

  - test:
      name: S3CMD object download
      desc: S3CMD object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml

  - test:
      name: Test Ratelimit at global scope
      desc: Test Ratelimit at global scope
      polarion-id: CEPH-83574915 # also CEPH-83574916 and CEPH-83574919
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_ratelimit_global.py
        config-file-name: ../../s3cmd/configs/test_ratelimit_global.yaml

  - test:
      name: Test Ratelimit for regular buckets
      desc: Test Ratelimit for regular buckets
      polarion-id: CEPH-83574910
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_rate_limit.py
        config-file-name: ../../s3cmd/configs/test_rate_limit.yaml

  - test:
      name: Test Ratelimit for versioned buckets
      desc: Test Ratelimit for versioned buckets
      polarion-id: CEPH-83574912
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_rate_limit.py
        config-file-name: ../../s3cmd/configs/test_ratelimit_versioned_bucket.yaml

  - test:
      name: Test Ratelimit for tenanted users
      desc: Test Ratelimit for tenanted users
      polarion-id: CEPH-83574914
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_rate_limit.py
        config-file-name: ../../s3cmd/configs/test_ratelimit_tenanted_user.yaml

  # - test:
  #     name: Test ratelimit on bucket owner change
  #     desc: Test ratelimit on bucket owner change
  #     polarion-id: CEPH-83574920
  #     module: sanity_rgw.py
  #     comments: known issue BZ2278599 pointed to 7.0
  #     config:
  #       script-name: ../s3cmd/test_ratelimit_bucketowner.py
  #       config-file-name: ../../s3cmd/configs/test_ratelimit_bucketowner.yaml

  - test:
      name: Test deletlifecycle rule via s3cmd
      desc: Test deletlifecycle rule via s3cmd
      polarion-id: CEPH-83580364
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_deletelifecycle.yaml

  - test:
      name: Test by adding almost 1K buckets to the resharding queue
      desc: disable and enable dynamic resharding for 1K buckets
      polarion-id: CEPH-11478
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_disable_and_enable_dynamic_resharding_with_1k_bucket.yaml
        timeout: 5000

  - test:
      name: Test multipart upload with failed upload parts using s3cmd and boto3
      desc: Test multipart upload with failed upload parts using s3cmd and boto3
      polarion-id: CEPH-83589550
      module: sanity_rgw.py
      config:
        script-name: ../s3cmd/test_s3cmd.py
        config-file-name: ../../s3cmd/configs/test_multipart_upload_with_failed_parts_using_s3cmd_and_boto3.yaml
