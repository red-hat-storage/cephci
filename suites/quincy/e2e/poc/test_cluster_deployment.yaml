# This workflow installs and configures Red Hat Ceph Storage cluster with
# following
#
#   - OSD daemons are classified into two tiers (archive and hot_tier) based
#     on HDD + NVMe & SSD
#   - RGW data pool uses erasure profile 4+2
#   - HA Proxy is used for balancing RGW daemons
#   - cephfs pool has max 2 MDS
#   - rbd pool created and initialized
---
tests:
  -
    test:
      abort-on-fail: true
      description: "Prepare the environment for testing."
      module: install_prereq.py
      name: "test setup"
  -
    test:
      abort-on-fail: true
      config:
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                mon-ip: node1
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr

      desc: "Evaluate cluster deployment using CephADM orchestrator."
      destroy-cluster: false
      module: test_cephadm.py
      name: "Test ceph cluster deployment"
      polarion-id: CEPH-83573713

  -
    test:
      abort-on-fail: true
      config:
        command: apply_spec
        service: orch
        specs:
          -
            service_type: osd
            service_id: archive_tier
            placement:
              label: osd
            spec:
              data_devices:
                rotational: 1
              db_devices:
                paths:
                  -
                    /dev/nvme0n1
          -
            service_type: osd
            service_id: hot_tier
            placement:
              label: osd
            spec:
              data_devices:
                rotational: 0
                size: "3T:4T"
      desc: "Evaluate deployment of OSD daemons using spec file."
      destroy-cluster: false
      module: test_osd.py
      name: "Test OSD daemon deployment"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - ceph fs volume create cephfs --placement='4 label:mds'
          - ceph fs set cephfs max_mds 2
      desc: "Create and configure a volume with 2 MDS limit"
      destroy-cluster: false
      module: exec.py
      name: "Test CephFS volume with 2 MDS configured"

  -
    test:
      abort-on-fail: true
      config:
        args:
          placement:
            label: mds
        command: apply
        pos_args:
          - cephfs
        service: mds
      desc: "Test deploying MDS daemons on hosts with label mds."
      destroy-cluster: false
      module: test_mds.py
      name: "Test deployment of MDS daemons"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "ceph osd pool create rbd"
      desc: "Create a volume for rbd pool"
      destroy-cluster: false
      module: exec.py
      name: "Evaluate OSD pool create"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "rbd pool init rbd"
      desc: "Initialize the RBD pool"
      destroy-cluster: false
      module: exec.py
      name: "Evaluate RBD pool init"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "ceph osd erasure-code-profile set rgwec01 k=4 m=2 crush-failure-domain=host crush-device-class=hdd"
      desc: "Create erasure profile"
      destroy-cluster: false
      module: exec.py
      name: "Evaluate creating of erasure profile"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "ceph osd pool create default.rgw.buckets.data 32 32 erasure rgwec01"
      desc: "Create OSD pool with erasure profile."
      destroy-cluster: false
      module: exec.py
      name: "Evaluate creating of OSD pool with erasure profile"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "ceph osd pool create default.rgw.buckets.index 32 32"
      desc: "Enable OSD pool for RGW index"
      destroy-cluster: false
      module: exec.py
      name: "Evaluate OSD pool for RGW index"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "ceph osd pool application enable default.rgw.buckets.data rgw"
      desc: "Enable OSD pool for RGW data"
      destroy-cluster: false
      module: exec.py
      name: "Evaluate OSD pool configuration for RGW data"

  -
    test:
      abort-on-fail: true
      config:
        cephadm: true
        commands:
          - "ceph osd pool application enable default.rgw.buckets.index rgw"
      desc: "Enable OSD pool for RGW index"
      destroy-cluster: false
      module: exec.py
      name: "Evaluate OSD pool configuration for RGW index"

  -
    test:
      abort-on-fail: true
      config:
        args:
          placement:
            label: rgw
        command: apply
        pos_args:
          - rgw.all
        service: rgw
      desc: "Test deploying of RGW daemon using CephAdm with label"
      destroy-cluster: false
      module: test_rgw.py
      name: "Test RGW daemon deployment with labels"

  -
    test:
      abort-on-fail: true
      config:
        command: add
        id: client.node10
        install_packages:
          - ceph-common
          - ceph-base
          - rbd-nbd
        copy_admin_keyring: true
        node: node10
      desc: "Configure a ceph client"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"

  -
    test:
      abort-on-fail: true
      config:
        command: add
        id: client.node11
        install_packages:
          - ceph-common
          - ceph-base
          - rbd-nbd
        copy_admin_keyring: true
        node: node11
      desc: "Configure a ceph client"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"

  -
    test:
      abort-on-fail: false
      config:
        haproxy_clients:
          - argo024
        rgw_endpoints:
          - argo020:80
          - argo021:80
          - argo022:80
          - argo023:80
      desc: "Install and configure HA-Proxy for RGW"
      destroy-cluster: false
      module: haproxy.py
      name: "Configure RGW HA Proxy"
