#===============================================================================================
# Tier-level: 2
# Test-Suite: tier-2_rbd_mirror_regression.yaml
# Suite contains rbd-mirror tier-2 testcases
#
# Cluster Configuration:
#    Conf file - conf/quincy/rbd/5-node-2-clusters.yaml
#    No of Clusters : 2
#    Node 2 must to be a client node
#===============================================================================================
tests:
  - test:
      name: setup install pre-requisistes
      desc: Setup phase to deploy the required pre-requisites for running the tests.
      module: install_prereq.py
      abort-on-fail: true
  - test:
      abort-on-fail: true
      clusters:
        ceph-rbd1:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rbd-mirror
                  args:
                    placement:
                      nodes:
                        - node5
        ceph-rbd2:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rbd-mirror
                  args:
                    placement:
                      nodes:
                        - node5
      desc: RBD Mirror cluster deployment using cephadm
      destroy-clster: false
      module: test_cephadm.py
      name: deploy cluster
  - test:
        abort-on-fail: true
        clusters:
          ceph-rbd1:
            config:
              command: add
              id: client.1
              node: node2
              install_packages:
                - ceph-common
              copy_admin_keyring: true
          ceph-rbd2:
            config:
                command: add
                id: client.1
                node: node2
                install_packages:
                    - ceph-common
                copy_admin_keyring: true
        desc: Configure the client system 1
        destroy-cluster: false
        module: test_client.py
        name: configure client
  - test:
      abort-on-fail: true
      clusters:
        ceph-rbd1:
          config:
            cephadm: true
            commands:
              - "ceph config set mon mon_allow_pool_delete true"
        ceph-rbd2:
          config:
            cephadm: true
            commands:
              - "ceph config set mon mon_allow_pool_delete true"
      desc: Enable mon_allow_pool_delete to True for deleting the pools
      module: exec.py
      name: configure mon_allow_pool_delete to True

  - test:
      name: test-image-delete-from-primary-site
      module: test-image-delete-primary-site.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
      polarion-id: CEPH-9501
      desc: Verify that image deleted at primary site updated at secondary

  - test:
      name: test image delete from secondary after multiple promote and demote
      module: test-image-delete-from-secondary.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
            repeat_count: 1
      polarion-id: CEPH-83574741
      desc: Verify that deleting primary image also delete the secondary image

  - test:
      name: Attempt shrinking secondary image
      module: test_9500_shrink_img_at_secondary.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
      polarion-id: CEPH-9500
      desc: Verify that deleting secondary image fails

  - test:
      name: test image meta operations sync to secondary
      module: test_rbd_image_meta_mirroring.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
            key: ping
            value: pong
      polarion-id: CEPH-9524
      desc: Verify removal of image meta gets mirrored

  - test:
      name: Recovery of abrupt failure of primary cluster
      module: test_9470.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
      polarion-id: CEPH-9470
      desc: Test unplanned failover and failback scenario

  - test:
      name: Recovery of shutdown primary cluster
      module: test_9471.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
      polarion-id: CEPH-9471
      desc: Test planned failover and failback scenario

  - test:
      name: Recovery of shutdown secondary cluster
      module: test_9475.py
      clusters:
        ceph-rbd1:
          config:
            imagesize: 2G
            io-total: 200M
      polarion-id: CEPH-9475
      desc: Testing secondary cluster unplanned failover test
