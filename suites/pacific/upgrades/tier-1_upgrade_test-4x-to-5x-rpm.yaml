#############################################################################
# - Automation support for RPM upgrade from RHCS4 to RHCS5 cluster
#
# Cluster configuration: (conf/pacific/upgrades/upgrades.yaml)
# --------------------------------------------------------------------------
# Nodes:
#   - 3 MONS, 3 MGRs, 3 OSDs, 2 RGW, 2 ISCSIGW's, 1 MDS's, 1 CLIENT, 1 GRAFANA, 1 CLIENT
#
# Test steps:
# ---------------------------------------------------------------------
# - Deploy bare-metal Nautilus Ceph cluster using CDN RPMS with lvm osd scenario
# - Run some I/O's
# - Convert the cluster to containerized using ceph-ansible
# - Upgrade to Pacific using ceph-ansible and parallel run I/O's
# - adopt to cephadm using ceph-adopt playbook
# - Run some I/O's
#
#############################################################################
tests:
- test:
    name: install ceph pre-requisites
    module: install_prereq.py
    abort-on-fail: True

- test:
    name: ceph ansible install rhcs 4.x from cdn
    polarion-id: CEPH-83573588
    module: test_ansible.py
    config:
      use_cdn: True
      build: '4.x'
      ansi_config:
        ceph_origin: repository
        ceph_repository: rhcs
        ceph_repository_type: cdn
        ceph_rhcs_version: 4
        ceph_stable_release: nautilus
        osd_scenario: lvm
        osd_auto_discovery: False
        ceph_stable_rh_storage: True
        ceph_docker_image: "rhceph/rhceph-4-rhel8"
        ceph_docker_image_tag: "latest"
        ceph_docker_registry: "registry.redhat.io"
        copy_admin_key: true
        radosgw_num_instances: 2
        dashboard_admin_user: admin
        dashboard_admin_password: p@ssw0rd
        grafana_admin_user: admin
        grafana_admin_password: p@ssw0rd
        node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
        grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
        prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
        alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
        cephfs_pools:
          - name: "cephfs_data"
            pgs: "16"
          - name: "cephfs_metadata"
            pgs: "16"
    desc: deploy ceph containerized 4.x cdn setup using ceph-ansible
    destroy-cluster: False
    abort-on-fail: true

- test:
    name: rados_bench_test
    module: radosbench.py
    config:
      pg_num: '128'
      pool_type: 'normal'
    desc: run rados bench for 360 - normal profile

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      sudo: True
    desc: Check for ceph health debug info

- test:
      name: switch-from-non-containerized-to-containerized-ceph-daemons
      polarion-id: CEPH-83573510
      module: switch_rpm_to_container.py
      abort-on-fail: true

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      sudo: True
    desc: Check for ceph health debug info

- test:
    name: rados_bench_test
    module: radosbench.py
    config:
      pg_num: '128'
      pool_type: 'normal'
    desc: run rados bench for 360 - normal profile

- test:
    name: Parallel run
    module: test_parallel.py
    parallel:
      - test:
          name: rados_bench_test
          module: radosbench.py
          config:
            pg_num: '128'
            pool_type: 'normal'
          desc: run rados bench for 360 - normal profile
      - test:
          name: rbd-io
          module: rbd_faster_exports.py
          config:
            io-total: 100M
          desc: Perform export during read/write,resizing,flattening,lock operations
      - test:
          name: Upgrade containerized ceph to 5.x latest
          polarion-id: CEPH-83573679
          module: test_ansible_upgrade.py
          config:
            build: '5.x'
            ansi_config:
              ceph_origin: distro
              ceph_stable_release: pacific
              ceph_repository: rhcs
              ceph_rhcs_version: 5
              osd_scenario: lvm
              osd_auto_discovery: False
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              radosgw_num_instances: 2
              copy_admin_key: true
              containerized_deployment: true
              upgrade_ceph_packages: True
              dashboard_admin_user: admin
              dashboard_admin_password: p@ssw0rd
              grafana_admin_user: admin
              grafana_admin_password: p@ssw0rd
              node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
              grafana_container_image: registry.redhat.io/rhceph/rhceph-5-dashboard-rhel8:5
              prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
              alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
          desc: Test Ceph-Ansible rolling update 4.x cdn -> 5.x latest -> cephadm adopt
          abort-on-fail: True
    desc: Running upgrade and i/o's parallelly

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      sudo: True
    desc: Check for ceph health debug info

- test:
    name: rados_bench_test
    module: radosbench.py
    config:
      pg_num: '128'
      pool_type: 'normal'
    desc: run rados bench for 360 - normal profile

- test:
    name: rbd-io
    module: rbd_faster_exports.py
    config:
        io-total: 100M
    desc: Perform export during read/write,resizing,flattening,lock operations

- test:
    name: rgw sanity tests
    module: sanity_rgw.py
    config:
        script-name: test_multitenant_user_access.py
        config-file-name: test_multitenant_access.yaml
        timeout: 300
    desc: Perform rgw tests
