# Below are the multi-site test scenarios run on the master and verified the sync/io on the slave
# The test  will create a primary site 'ceph-pri', write IOs on the first site, indeuce delay of 10ms on first site and second site, and then convert it to a multisite and test sync.
# ceph-pri is master/primary site
# ceph-sec is slave/secondary site
# CEPH-83575355 - [single-site to multi-site conversion] Test no recovering shards are seen ( Bug 2026101)

tests:

  # Cluster deployment stage

  - test:
      name: setup pre-requisites
      module: install_prereq.py
      abort-on-fail: true
      desc: install ceph pre requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri
                  args:
                    placement:
                      nodes:
                        - node5
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec
                  args:
                    placement:
                      nodes:
                        - node5
      desc: RHCS cluster deployment using cephadm.
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
      polarion-id: CEPH-83575222

  - test:
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758

  # Test work flow before migration

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary"
              - "ceph orch restart {service_name:shared.pri}"
      desc: Setting up only primary site first
      module: exec.py
      name: setup only primary site
      polarion-id: CEPH-10362

  - test:
      name: create user
      desc: create non-tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml

  #  Dynamic resharding tests
  - test:
      name: Verify DBR feature enabled on greenfield cluster
      desc: Check DBR feature enabled on greenfield cluster
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83573596
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_check_sharding_enabled_greenfield.yaml
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_greenfield.yaml
      desc: Test dynamic resharding on greenfield deployment
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster
      polarion-id: CEPH-83574737

  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_dynamic_resharding_with_version_without_bucket_delete.yaml
            script-name: test_dynamic_bucket_resharding.py
      desc: Resharding test - dynamic resharding on versioned bucket
      name: Dynamic Resharding on versioned buckets
      polarion-id: CEPH-83571740
      module: sanity_rgw_multisite.py

  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects on primary(single site)
      polarion-id: CEPH-9789
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects.yaml
  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects_compression on primary(single site)
      polarion-id: CEPH-11350
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_compression.yaml
  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects_aws4 on primary(single site)
      polarion-id: CEPH-9637
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_aws4.yaml
  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects_delete on primary(single site)
      polarion-id: CEPH-14237
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_delete.yaml
  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects_download on primary(single site)
      polarion-id: CEPH-14237
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_download.yaml
  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects_enc on primary(single site)
      polarion-id: CEPH-11358 # also applies to CEPH-11361
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_enc.yaml
  - test:
      name: Buckets and Objects test
      desc: test_Mbuckets_with_Nobjects_multipart on primary(single site)
      polarion-id: CEPH-9801
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml

  - test:
      name: Bucket listing test
      desc: test_bucket_listing_flat_ordered_versionsing on primary(single site)
      polarion-id: CEPH-83573545
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_ordered_versionsing.yaml

  - test:
      name: Bucket listing test
      desc: test_bucket_listing_flat_unordered.yaml on primary(single site)
      polarion-id: CEPH-83573545
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_unordered.yaml

  - test:
      name: Buckets Versioning test
      desc: test_versioning_objects_acls on on primary(single site)
      polarion-id: CEPH-9190
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_objects_acls.yaml

  # migrating from singlesite to multisite

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary"
              - "ceph orch restart {service_name:shared.sec}"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-83575223

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83575227
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on secondary
      polarion-id: CEPH-83575355

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            roles:
              - rgw
            rule: root netem delay 10ms 5ms distribution normal
        ceph-sec:
          config:
            roles:
              - rgw
            rule: root netem delay 10ms 5ms distribution normal
      desc: Configuring network traffic delay
      module: configure-tc.py
      name: apply-net-qos
      polarion-id: CEPH-83575222

  # Test work flow after migration

  - test:
      name: create user
      desc: create tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: tenanted_user.yaml
            copy-user-info-to-site: ceph-sec

  - test:
      name: Verify DBR feature enabled on single to multisite cluster
      desc: Check DBR feature enabled on single to multisite cluster
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83573596
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_check_sharding_enabled_brownfield.yaml

  - test:
      name: Bucket policy tests
      desc: test_bucket_policy_modify.yaml on secondary
      polarion-id: CEPH-11214
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_policy_ops.py
            config-file-name: test_bucket_policy_modify.yaml
            verify-io-on-site: ["ceph-sec"]
  - test:
      name: Bucket policy tests
      desc: test_bucket_policy_delete.yaml on secondary
      polarion-id: CEPH-11213
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_policy_ops.py
            config-file-name: test_bucket_policy_delete.yaml
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: Bucket policy tests
      desc: test_bucket_policy_replace on secondary
      polarion-id: CEPH-11215
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_policy_ops.py
            config-file-name: test_bucket_policy_replace.yaml
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: put and get bucket notification using tenant user
      desc: put and get bucket notification using tenant user
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83575500
      clusters:
        ceph-pri:
          config:
            run-on-rgw: true
            extra-pkgs:
              - wget https://download.oracle.com/java/23/latest/jdk-23_linux-x64_bin.rpm
            install_start_kafka: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_with_tenant_user.yaml

# Log trimming tests
  - test:
      name: Mdlog trimming test on primary
      desc: test mdlog trimming on primary
      polarion-id: CEPH-10544 #CEPH-10722, CEPH-10547
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bilog_trimming.py
            config-file-name: test_mdlog_trimming.yaml
            timeout: 5000
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: Datalog trimming test on primary
      desc: test datalog trimming on primary
      polarion-id: CEPH-10540 #CEPH-10722, CEPH-10547
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bilog_trimming.py
            config-file-name: test_datalog_trimming.yaml
            verify-io-on-site: ["ceph-sec"]


  - test:
      name: Bilog trimming test on primary
      desc: test bilog trimming on primary
      polarion-id: CEPH-83572658 #CEPH-10722, CEPH-10547
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bilog_trimming.py
            config-file-name: test_bilog_trimming.yaml

  - test:
      name: bucket request payer
      desc: Basic test for bucket request payer
      polarion-id: CEPH-10352
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_request_payer.py
            config-file-name: test_bucket_request_payer.yaml
