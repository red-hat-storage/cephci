# Polarion ID: CEPH-83574647
# Objective: Testing Multisite upgrade from RHCS 5 GA to RHCS 5 latest development build.
# conf: rgw_multisite.yaml
# platform: rhel-8
---
tests:

  - test:
      abort-on-fail: true
      desc: install ceph pre requisites
      module: install_prereq.py
      name: install vm pre-requsites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    custom_repo: cdn
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
                  cephadm-ansible:
                    playbook: cephadm-preflight.yml
                    extra-vars:
                      ceph_origin: rhcs
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.pri
                  args:
                    placement:
                      nodes:
                        - node5
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    custom_repo: cdn
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
                  cephadm-ansible:
                    playbook: cephadm-preflight.yml
                    extra-vars:
                      ceph_origin: rhcs
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply
                  service: rgw
                  pos_args:
                    - shared.sec
                  args:
                    placement:
                      nodes:
                        - node5
      desc: RHCS cluster deployment using cephadm.
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
      polarion-id: CEPH-83575222
  - test:
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node5:80
        ceph-sec:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node5:80
      desc: "Configure HAproxy"
      module: haproxy.py
      name: "Configure HAproxy"

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary"
              - "ceph orch restart {service_name:shared.pri}"
        ceph-sec:
          config:
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary"
              - "ceph orch restart {service_name:shared.sec}"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-10362

  - test:
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph versions"
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph versions"
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info
      polarion-id: CEPH-83575227

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
            timeout: 300
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

  # Baseline tests before upgrade

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_download.yaml
            verify-io-on-site: ["ceph-sec"]
            run-on-haproxy: true
            timeout: 300
      desc: test to create "M" no of buckets and "N" no of objects with download
      module: sanity_rgw_multisite.py
      name: download objects pre upgrade
      polarion-id: CEPH-14237

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_ordered.yaml
            timeout: 300
      desc: test duration for ordered listing of bucket with top level objects
      module: sanity_rgw_multisite.py
      name: ordered listing of buckets pre upgrade
      polarion-id: CEPH-83573545

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_unordered.yaml
            timeout: 300
      desc: test duration for unordered listing of buckets with top level objects
      module: sanity_rgw_multisite.py
      name: unordered listing of buckets pre upgrade
      polarion-id: CEPH-83573545

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_swift_basic_ops.py
            config-file-name: test_swift_basic_ops.yaml
            verify-io-on-site: ["ceph-sec"]
            timeout: 300
      desc: Test object operations with swift
      module: sanity_rgw_multisite.py
      name: swift basic operations pre upgrade
      polarion-id: CEPH-11019

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300
      desc: Create bucket for Testing dynamic resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Create bucket for Testing dynamic resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574736

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_manual_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300
      desc: Create bucket for Testing manual resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Create bucket for Testing manual resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574735

  # Performing cluster upgrade

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: start
            service: upgrade
            verify_cluster_health: true
        ceph-sec:
          config:
            command: start
            service: upgrade
            verify_cluster_health: true
      desc: Multisite upgrade
      module: test_cephadm_upgrade.py
      name: multisite ceph upgrade
      polarion-id: CEPH-83574647

  - test:
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph versions"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph versions"
      desc: Retrieve the versions of the cluster
      module: exec.py
      name: post upgrade gather version
      polarion-id: CEPH-83575200

  # Post upgrade testing

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_download.yaml
            run-on-haproxy: true
            verify-io-on-site: ["ceph-sec"]
            timeout: 300
      desc: test to create "M" no of buckets and "N" no of objects with download
      module: sanity_rgw_multisite.py
      name: download objects post upgrade
      polarion-id: CEPH-14237

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_ordered_versionsing.yaml
            timeout: 300
      desc: test the duration for ordered listing of versioned buckets
      module: sanity_rgw_multisite.py
      name: ordered listing of versioned buckets post upgrade
      polarion-id: CEPH-83573545

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_unordered.yaml
            timeout: 300
      desc: test duration for unordered listing of buckets
      module: sanity_rgw_multisite.py
      name: unordered listing of buckets post upgrade
      polarion-id: CEPH-83573545

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_swift_basic_ops.py
            config-file-name: test_swift_object_expire_op.yaml
            verify-io-on-site: ["ceph-sec"]
            timeout: 500
      desc: test object expiration with swift
      module: sanity_rgw_multisite.py
      name: swift object expiration post upgrade
      polarion-id: CEPH-9718

  - test:
      name: Verify DBR feature enabled on upgraded cluster
      desc: Check DBR feature enabled on upgraded cluster
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83573596
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_check_sharding_enabled_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300
      desc: Test dynamic resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Test dynamic resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574736

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_manual_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300
      desc: Test manual resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Test dynamic resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574735

  - test:
      name: test sharding on primary
      desc: test_Mbuckets_with_Nobjects_sharding on primary
      polarion-id: CEPH-83573600
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_sharding.yaml
            timeout: 300
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300
      desc: Test dynamic resharding brownfield scenario after upgrade on new bucket
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster
      polarion-id: CEPH-83574737

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_manual_resharding_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
            timeout: 300
      desc: Test manual resharding brownfield scenario after upgrade on new bucket
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      name: Manual Resharding tests on Primary cluster
      polarion-id: CEPH-83574734

  # Testing stage
# configuring vault agent on all the sites

  - test:
      clusters:
        ceph-pri:
          config:
            install:
              - agent
            run-on-rgw: true
        ceph-sec:
          config:
            install:
              - agent
            run-on-rgw: true
      desc: Setup and configure vault agent
      destroy-cluster: false
      module: install_vault.py
      name: configure vault agent
      polarion-id: CEPH-83575226

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.sec rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.sec rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.sec"
            timeout: 120
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph config set client.rgw.shared.pri rgw_crypt_require_ssl false"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_backend vault"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_addr http://127.0.0.1:8100"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_auth agent"
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_prefix /v1/transit "
              - "ceph config set client.rgw.shared.pri rgw_crypt_sse_s3_vault_secret_engine transit"
              - "ceph orch restart rgw.shared.pri"
            timeout: 120
      desc: Setting vault configs for sse-s3 on multisite archive
      module: exec.py
      name: set sse-s3 vault configs on multisite


  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            script-name: test_sse_s3_kms_with_vault.py
            config-file-name: test_sse_s3_per_bucket_encryption_version_enabled.yaml
            timeout: 300
      desc: test_sse_s3_per_bucket_encryption_version_enabled in brownfield
      module: sanity_rgw.py
      name: sse-s3 per bucket encryption test on a versiond bucket
      polarion-id: CEPH-83575152
