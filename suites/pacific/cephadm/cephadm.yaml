tests:
  - test:
      name: Install ceph pre-requisites
      desc: installation of ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true
  - test:
      name: Cephadm Bootstrap
      desc: cephadm cluster bootstrap
      module: test_bootstrap.py
      polarion-id: CEPH-83573719,CEPH-83573720
      config:
        command: bootstrap
        base_cmd_args:
          verbose: true
        args:
          registry-json: registry.redhat.io
          custom_image: true
          mon-ip: "node1"
          orphan-initial-daemons: true
          skip-monitoring-stack: true
          initial-dashboard-user: admin123
          initial-dashboard-password: admin123
          fsid: f64f341c-655d-11eb-8778-fa163e914bcc
        get_cluster_details:           # to view ceph status only
          - "ceph status"
          - "ceph orch ls -f yaml"
          - "ceph orch ps -f yaml"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add Host
      desc: Add host to Bootstrapped Cluster
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: add
        args:                     # arguments to ceph orch
          node: node2
        get_cluster_details:                    # to view ceph status only
          - "ceph status"
          - "ceph orch host ls -f yaml"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add host
      desc: Add new host node with IP address
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: add
        base_cmd_args:
          verbose: true
        args:
          node: node3
          attach_ip_address: true
          add_label: false
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add host
      desc: Add new host node with IP address and labels
      module: test_host.py
      polarion-id: CEPH-83573729
      config:
        service: host
        command: add
        args:
          node: node4
          attach_ip_address: true
          labels:
            - osd
            - node-exporter
            - mon
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Host label
      desc: Remove host label osd from node3
      module: test_host.py
      polarion-id: CEPH-83573882
      config:
        service: host
        command: label_remove
        args:
          node: node4
          labels:
            - osd
            - mon
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Host label
      desc: Remove host label node-exporter from node4
      module: test_host.py
      polarion-id: CEPH-83573882
      config:
        service: host
        command: label_remove
        base_cmd_args:
          verbose: true
        args:
          node: node4
          labels:
            - node-exporter
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add Host label
      desc: Add host label mon,osd to existing node
      module: test_host.py
      polarion-id: CEPH-83573881
      config:
        service: host
        command: label_add
        base_cmd_args:
          verbose: true
        args:
          node: node4
          labels:
            - mon
            - osd
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Attach Host label to installer
      desc: Add host label(s) to existing installer node
      module: test_host.py
      polarion-id: CEPH-83573881
      config:
        service: host
        command: label_add
        base_cmd_args:
          verbose: true
        args:
          node: node1
          labels: apply-all-labels
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Set Host address
      desc: Set IP address to a existing node
      module: test_host.py
      polarion-id: CEPH-83573880
      config:
        service: host
        command: set_address
        base_cmd_args:
          verbose: true
        args:
          node: node1
        get_cluster_details:                     # to view ceph status only
          - "ceph status"
          - "ceph orch host ls -f yaml"
          - "ceph health detail"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove host
      desc: Remove node from cluster
      module: test_host.py
      polarion-id: CEPH-83573747
      config:
        service: host
        command: remove
        base_cmd_args:
          verbose: true
        args:
          node: node4
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove host
      desc: Remove node from cluster
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: remove
        args:
          node: node3
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add hosts to ceph cluster
      desc: Add host node(s) with IP address and labels
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: add_hosts
        args:
          nodes:
            - node5
            - node6
            - node7
            - node8
          attach_ip_address: false
          labels: apply-all-labels
        get_cluster_details:                        # to view ceph status only
          - "ceph status"
          - "ceph orch host ls -f yaml"
          - "ceph health detail"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Hosts from cluster
      desc: Remove nodes from cluster execpt installer
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: remove_hosts
        args:
          nodes:
            - node5
            - node6
        get_cluster_details:                    # to view ceph status only
          - "ceph status"
          - "ceph orch host ls -f yaml"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove all Hosts
      desc: Remove all nodes from cluster execpt installer
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: remove_hosts
        args:
          nodes: []
        get_cluster_details:                    # to view ceph status only
          - "ceph status"
          - "ceph orch host ls -f yaml"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add all hosts to ceph cluster
      desc: Add all host node with IP address and labels
      module: test_host.py
      polarion-id:
      config:
        service: host
        command: add_hosts
        args:
          nodes: []
          attach_ip_address: true
          labels: apply-all-labels
        get_cluster_details:                      # to view ceph status only
          - "ceph status"
          - "ceph orch host ls -f yaml"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply Monitor with placement and limit
      desc: Apply monitor service with placement and limit
      module: test_mon.py
      polarion-id: CEPH-83573732
      config:
        command: apply
        service: mon
        base_cmd_args:
          verbose: true
        args:
          placement:
            nodes:
                - node1
                - node2
                - node6
            limit: 3    # no of daemons
            sep: " "    # separator to be used for placements
        get_cluster_details:                        # to view ceph status only
          - "ceph status"
          - "ceph orch ls mon -f json-pretty"
          - "ceph orch ps '' mon -f json-pretty"
          - "ceph health detail -f yaml"
  - test:
      name: Apply Monitor with placement
      desc: Apply monitor with placement only
      module: test_mon.py
      polarion-id:
      config:
        command: apply
        service: mon
        base_cmd_args:
          verbose: true
        args:
          placement:
            nodes:
                - node1
                - node2
                - node6
            sep: ";"
        get_cluster_details:                          # to view ceph status only
          - "ceph status"
          - "ceph orch ls mon -f json-pretty"
          - "ceph orch ps '' mon -f json-pretty"
          - "ceph health detail -f yaml"
  - test:
      name: Apply Monitor using label
      desc: Apply monitor using label mon
      module: test_mon.py
      polarion-id: CEPH-83573883,CEPH-83573772
      config:
        command: apply
        service: mon
        base_cmd_args:
          verbose: true
        args:
          placement:
            label: mon
        get_cluster_details:                          # to view ceph status only
          - "ceph status"
          - "ceph orch ls mon -f json-pretty"
          - "ceph orch ps '' mon -f json-pretty"
          - "ceph health detail -f yaml"
  - test:
      name: Apply Manager service
      desc: Apply manager service with placement option
      module: test_mgr.py
      polarion-id: CEPH-83573734
      config:
        command: apply
        service: mgr
        base_cmd_args:
          verbose: true
        args:
          placement:
            nodes:
              - node1
            sep: " "
        get_cluster_details:                        # to view ceph status only
          - "ceph status"
          - "ceph orch ls mgr -f json-pretty"
          - "ceph orch ps '' mgr -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply OSD Service
      desc: Apply OSD service with all available devices
      module: test_osd.py
      polarion-id: CEPH-83573735,CEPH-83573771
      config:
        command: apply
        service: osd
        base_cmd_args:
          verbose: true
        args:
          all-available-devices: true
        get_cluster_details:                        # to view ceph status only
          - "ceph status"
          - "ceph orch ls osd -f json-pretty"
          - "ceph orch ps '' osd.all-available-devices -f json-pretty"
          - "ceph osd df"
          - "ceph osd tree"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Add cephfs file system volume
      desc: Add file system for MDS service
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:          # arguments to ceph orch
          - ceph
          - fs
          - volume
          - create
          - cephfs
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply MDS Service
      desc: Apply MDS service on all mds nodes
      module: test_mds.py
      polarion-id: CEPH-83573737
      config:
        command: apply
        service: mds
        base_cmd_args:          # arguments to ceph orch
          verbose: true
        pos_args:
          - cephfs              # name of the filesystem
        args:
          placement:
            nodes:
              - node2
              - node8
            limit: 2            # no of daemons
            sep: " "            # separator to be used for placements
        get_cluster_details:                        # to view ceph status only
          - "ceph status"
          - "ceph orch ls mds -f json-pretty"
          - "ceph orch ps '' mds.cephfs -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Create replicated pool
      desc: Add pool for ISCSI service
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:             # command arguments
          - ceph
          - osd
          - pool
          - create
          - iscsi
          - "3"
          - "3"
          - replicated
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Enable rbd application on pool
      desc: enable rbd on iscsi pool
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:             # command arguments
          - ceph
          - osd
          - pool
          - application
          - enable
          - iscsi
          - rbd
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply ISCSI Service
      desc: Apply ISCSI target service on iscsi role nodes
      module: test_iscsi.py
      polarion-id: CEPH-83573756
      config:
        command: apply
        service: iscsi
        base_cmd_args:            # arguments to ceph orch
            verbose: true
        pos_args:
          - iscsi                 # name of the pool
          - api_user              # name of the API user
          - api_pass              # password of the api_user.
          - ["1.1.1.1", "2.2.2.2"]    # space separate list of IPs
        args:
          placement:
            label: iscsi          # either label or node.
        get_cluster_details:            # to view ceph status only
          - "ceph status"
          - "ceph orch ls iscsi -f json-pretty"
          - "ceph orch ps '' iscsi.iscsi -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply ISCSI Service
      desc: Apply ISCSI target service on iscsi role nodes
      module: test_iscsi.py
      polarion-id: CEPH-83573728
      config:
        command: apply
        service: iscsi
        base_cmd_args:            # arguments to ceph orch
          verbose: true
        pos_args:
          - iscsi                 # name of the pool
          - api_user              # name of the API user
          - api_pass              # password of the api_user.
        args:
          placement:
            nodes:
              - node8          # either label or node.
        get_cluster_details:              # to view ceph status only
          - "ceph status"
          - "ceph orch ls iscsi -f json-pretty"
          - "ceph orch ps '' iscsi.iscsi -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply Prometheus Service
      desc: Apply Prometheus service on role nodes
      module: test_monitoring.py
      polarion-id: CEPH-83573738
      config:
        command: apply
        service: prometheus
        base_cmd_args:          # arguments to ceph orch
          verbose: true
        args:
          placement:
            nodes:
              - node1
        get_cluster_details:              # to view ceph status only
          - "ceph status"
          - "ceph orch ls prometheus -f json-pretty"
          - "ceph orch ps '' prometheus -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply Node-exporter Service
      desc: Apply Node-exporter service on role nodes
      module: test_monitoring.py
      polarion-id: CEPH-83573884
      config:
        command: apply
        service: node-exporter
        base_cmd_args:          # arguments to ceph orch
          verbose: true
        args:
          placement:
            nodes: "*"
        get_cluster_details:            # to view ceph status only
          - "ceph status"
          - "ceph orch ls node-exporter -f json-pretty"
          - "ceph orch ps '' node-exporter -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply Alert-manager Service
      desc: Apply Alert-manager service on role nodes
      module: test_monitoring.py
      polarion-id: CEPH-83573885
      config:
        command: apply
        service: alertmanager
        base_cmd_args:          # arguments to ceph orch
          verbose: true
        args:
          placement:
            nodes:
              - node1
              - node2
        get_cluster_details:            # to view ceph status only
          - "ceph status"
          - "ceph orch ls alertmanager -f json-pretty"
          - "ceph orch ps '' alertmanager -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply Grafana Service
      desc: Apply Grafana service on role nodes
      module: test_monitoring.py
      polarion-id: CEPH-83573886
      config:
        command: apply
        service: grafana
        base_cmd_args:          # arguments to ceph orch
          verbose: true
        args:
          placement:
            nodes:
              - node1
        get_cluster_details:            # to view ceph status only
          - "ceph status"
          - "ceph orch ls grafana -f json-pretty"
          - "ceph orch ps '' grafana -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply RGW Service
      desc: Apply RGW service on all rgw nodes
      module: test_rgw.py
      polarion-id: CEPH-83573739
      config:
        command: apply
        service: rgw
        base_cmd_args:          # arguments to ceph orch
          verbose: true
        pos_args:               # positional arguments
          - india             # realm
          - south             # zone
        args:
          placement:
            nodes:              # A list of strings that would looked up
                - node6
                - node7
            limit: 2            # no of daemons
            sep: ";"            # separator to be used for placements
        get_cluster_details:          # to view ceph status only
          - "ceph status"
          - "ceph orch ls rgw -f json-pretty"
          - "ceph orch ps '' rgw.india.south -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Create replicated pool for NFS
      desc: Add pool for NFS Ganesha service
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:                     # command arguments
          - ceph
          - osd
          - pool
          - create
          - nfs-ganesha-pool
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Enable rgw application on nfs-ganesha pool
      desc: enable rgw on nfs-ganesha pool
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:             # command arguments
          - ceph
          - osd
          - pool
          - application
          - enable
          - nfs-ganesha-pool
          - rgw
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply NFS Service
      desc: Apply NFS-Ganesha service on role nodes
      module: test_nfs.py
      polarion-id: CEPH-83573749,CEPH-83573750
      config:
        command: apply
        service: nfs
        base_cmd_args:
            verbose: true
        pos_args:
          - mynfs                         # nfs service ID
          - nfs-ganesha-pool              # name of the pool
        args:
          namespace: mynfs-ns             # specify namespace
          placement:
            nodes:
              - node8
              - node6
            limit: 2
            sep: ";"
        get_cluster_details:              # to view ceph status only
          - "ceph status"
          - "ceph orch ls nfs -f json-pretty"
          - "ceph orch ps '' nfs.mynfs -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Apply Crash Service
      desc: Apply Crash service on all nodes
      module: test_crash.py
      polarion-id: CEPH-83573887
      config:
        command: apply
        service: crash
        base_cmd_args:
            verbose: true
        args:
          placement:
            nodes: "*"
        get_cluster_details:              # to view ceph status only
          - "ceph status"
          - "ceph orch ls crash -f json-pretty"
          - "ceph orch ps '' crash -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove NFS-Ganesha Service
      desc: Remove NFS-Ganesha service using orch remove option
      module: test_nfs.py
      polarion-id: CEPH-83573784
      config:
        command: remove
        service: nfs
        base_cmd_args:
          verbose: true
        args:
          service_name: nfs.mynfs
          verify: true
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove RGW Service
      desc: Remove RGW service using orch remove option
      module: test_rgw.py
      polarion-id:
      config:
        command: remove
        service: rgw
        base_cmd_args:
          verbose: true
        args:
            service_name: rgw.india.south
            verify: true
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove ISCSI Service
      desc: Remove ISCSI service using orch remove option
      module: test_iscsi.py
      polarion-id: CEPH-83573757
      config:
        command: remove
        service: iscsi
        args:
            service_name: iscsi.iscsi
            verify: true
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Prometheus Service
      desc: Remove Prometheus service using orch remove option
      module: test_monitoring.py
      polarion-id: CEPH-83573738
      config:
        command: removess
        service: prometheus
        base_cmd_args:
          verbose: true
        args:
          service_name: prometheus
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Node-exporter Service
      desc: Remove Node-exporter service using orch remove option
      module: test_monitoring.py
      polarion-id: CEPH-83573884
      config:
        command: remove
        service: node-exporter
        args:
          service_name: node-exporter
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Alert-manager Service
      desc: Remove Alert-manager service using orch remove option
      module: test_monitoring.py
      polarion-id: CEPH-83573885
      config:
        command: remove
        service: alertmanager
        args:
          service_name: alertmanager
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Grafana Service
      desc: Remove Grafana service using orch remove option
      module: test_monitoring.py
      polarion-id: CEPH-83573886
      config:
        command: remove
        service: grafana
        args:
          service_name: grafana
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove MDS Service
      desc: Remove MDS service using orch remove option
      module: test_mds.py
      polarion-id: CEPH-83573748
      config:
        command: remove
        service: mds
        base_cmd_args:
          verbose: true
        args:
          service_name: 'mds.cephfs'
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: set mon_allow_pool_delete
      desc: set mon_allow_pool_delete to true
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:
          - ceph
          - config
          - set
          - mon
          - mon_allow_pool_delete
          - "true"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: remove cephfs file system volume
      desc: remove file system
      module: test_bootstrap.py
      polarion-id:
      config:
        command: shell
        args:
          - ceph
          - fs
          - volume
          - rm
          - cephfs
          - "--yes-i-really-mean-it"
      destroy-cluster: false
      abort-on-fail: true
  - test:
      name: Remove Crash Service
      desc: Remove Crash service using orch remove option
      module: test_crash.py
      polarion-id: CEPH-83573887
      config:
        command: remove
        service: crash
        base_cmd_args:
          verbose: true
        args:
          service_name: crash
        get_cluster_details:              # to view ceph status only
          - "ceph status"
          - "ceph orch ls -f json-pretty"
          - "ceph orch ps -f json-pretty"
          - "ceph health detail -f yaml"
      destroy-cluster: false
      abort-on-fail: true
