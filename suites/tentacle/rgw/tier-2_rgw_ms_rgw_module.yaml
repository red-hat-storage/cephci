# global conf : /conf/tentacle/rgw_multisite.yaml
# Rgw multisite deployed with rgw module method
tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
      desc: RHCS cluster deployment using cephadm.
      polarion-id: CEPH-83575222
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster

  - test:
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: apply_spec
                  service: orch
                  validate-spec-services: true
                  specs:
                    - service_type: prometheus
                      placement:
                        count: 1
                        nodes:
                          - node1
                    - service_type: grafana
                      placement:
                        nodes:
                          - node1
                    - service_type: alertmanager
                      placement:
                        count: 1
                    - service_type: node-exporter
                      placement:
                        host_pattern: "*"
                    - service_type: crash
                      placement:
                        host_pattern: "*"
      name: Monitoring Services deployment
      desc: Add monitoring services using spec file.
      module: test_cephadm.py
      polarion-id: CEPH-83574727

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      polarion-id: CEPH-83573758
      destroy-cluster: false
      module: test_client.py
      name: configure client
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node5:80
        ceph-sec:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node5:80
      desc: "Configure HAproxy"
      module: haproxy.py
      name: "Configure HAproxy"
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: false
            role: client
            commands:
              - "ceph mgr module enable rgw"
              - "ceph rgw realm bootstrap --realm-name india --zonegroup-name shared --zone-name primary --port 80 --placement=\"{node:node5}\" --start-radosgw"
              - "radosgw-admin realm default --rgw-realm=india"
              - "radosgw-admin zone modify --rgw-realm=india --rgw-zonegroup=shared --rgw-zone=primary --endpoints=http://{node_ip:node5}:80"
              - "radosgw-admin period update --rgw-realm=india --commit"
              - "ceph config set client.rgw.{daemon_id:india.primary} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:india.primary} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:india.primary} rgw_zone primary"
              - "ceph config set client.rgw rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:india.primary} rgw_verify_ssl False"
              - "ceph orch restart {service_name:india.primary}"
              - "sudo yum install -y sshpass"
              - "sleep 120"
              - "ceph rgw realm tokens | jq -r '.[0].token' > /home/cephuser/realm-token.txt"
              - "sshpass -p 'passwd' scp /home/cephuser/realm-token.txt root@{node_ip:ceph-sec#node6}:/tmp/realm-token.txt"
        ceph-sec:
          config:
            cephadm: false
            role: client
            commands:
              - |
                TOKEN=$(cat /tmp/realm-token.txt)
                cat <<EOF > /tmp/zone-spec.yaml
                rgw_zone: secondary
                rgw_realm_token: $TOKEN
                placement:
                  hosts:
                    - {node:node5}
                spec:
                  rgw_frontend_port: 80
                EOF
              - "ceph mgr module enable rgw"
              - "ceph rgw zone create -i /tmp/zone-spec.yaml"
              - "radosgw-admin zone modify --rgw-realm=india --rgw-zonegroup=shared --rgw-zone=secondary --endpoints=http://{node_ip:node5}:80"
              - "radosgw-admin realm default --rgw-realm=india"
              - "radosgw-admin period update --rgw-realm=india --commit"
              - "ceph config set client.rgw rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:india.secondary} rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:india.secondary} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:india.secondary} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:india.secondary} rgw_zone secondary"
              - "ceph orch restart {service_name:india.secondary}"
              - "sleep 120"
      desc: Setting up RGW multisite replication environment using rgw module with token
      module: exec.py
      name: setup multisite with rgw module
      polarion-id: CEPH-10362
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      polarion-id: CEPH-83575227
      module: exec.py
      name: get shared realm info on primary

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      polarion-id: CEPH-83575227
      module: exec.py
      name: get shared realm info on secondary

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
  - test:
      name: multipart upload on primary
      desc: test_Mbuckets_with_Nobjects_multipart on primary
      polarion-id: CEPH-14265
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-sec"]
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml
