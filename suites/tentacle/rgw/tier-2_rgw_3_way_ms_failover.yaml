# Test suite for RGW active active multi-site deployment scenario.

# master zone - pri
# secondary zone - sec
# tertiary zone - ter

# The deployment is evaluated by running IOs across the environments
# Sync rgws & Client IO rgws are sepearated
# global-conf: conf/tentacle/rgw/3-way_ms_4nodes_1client_cluster.yaml
---

tests:

  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.pri.io
                      spec:
                        disable_multisite_sync_traffic: true
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: primary
                        rgw_frontend_port: 80
                      placement:
                        nodes:
                          - node3
                          - node4
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.pri.sync
                      spec:
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: primary
                        rgw_frontend_port: 81
                      placement:
                        nodes:
                          - node3
                          - node4
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.sec.io
                      spec:
                        disable_multisite_sync_traffic: true
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: secondary
                        rgw_frontend_port: 80
                      placement:
                        nodes:
                          - node3
                          - node4
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.sec.sync
                      spec:
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: secondary
                        rgw_frontend_port: 81
                      placement:
                        nodes:
                          - node3
                          - node4
        ceph-ter:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    initial-dashboard-password: admin@123
                    dashboard-password-noupdate: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.ter.io
                      spec:
                        disable_multisite_sync_traffic: true
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: tertiary
                        rgw_frontend_port: 80
                      placement:
                        nodes:
                          - node3
                          - node4
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.ter.sync
                      spec:
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: tertiary
                        rgw_frontend_port: 81
                      placement:
                        nodes:
                          - node3
                          - node4
      desc: RHCS cluster deployment using cephadm.
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster
      polarion-id: CEPH-10117

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node5
            install_packages:
              - ceph-common
              - git
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node5
            install_packages:
              - ceph-common
              - git
            copy_admin_keyring: true
        ceph-ter:
          config:
            command: add
            id: client.1
            node: node5
            install_packages:
              - ceph-common
              - git
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758

# # configuring HAproxy for IO RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node5
            rgw_endpoints:
              - node3:80
              - node4:80
      desc: "Configure HAproxy for IO rgws"
      module: haproxy.py
      name: "Configure HAproxy for IO rgws"

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node3}:81,http://{node_ip:node4}:81 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node3}:81,http://{node_ip:node4}:81 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.pri.io}"
              - "ceph orch restart {service_name:shared.pri.sync}"
      desc: Setting up only primary site first
      module: exec.py
      name: setup only primary site
      polarion-id: CEPH-83608639

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph -s"
              - "radosgw-admin sync status"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83575227

  - test:
      name: create user
      desc: create tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: tenanted_user.yaml

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node3}:81 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node3}:81 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node3}:81,http://{node_ip:node4}:81 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
              - "sleep 120"
      desc: Setting up 2-way RGW multisite replication betwwen primary and secondary environment
      module: exec.py
      name: Setting up 2-way RGW multisite replication betwwen primary and secondary environment
      polarion-id: CEPH-83608639

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83608639
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on secondary
      polarion-id: CEPH-83608639

  # Test work flow
  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

# configuring HAproxy for IO RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            haproxy_clients:
              - node5
            rgw_endpoints:
              - node3:80
              - node4:80
      desc: "Configure HAproxy for IO rgws"
      module: haproxy.py
      name: "Configure HAproxy for IO rgws"

  - test:
      name: S3CMD object download on primary
      desc: S3CMD object download or GET
      polarion-id: CEPH-83575477
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../s3cmd/test_s3cmd.py
            config-file-name: ../../s3cmd/configs/test_get_s3cmd.yaml
            run-on-haproxy: true
            monitor-consistency-bucket-stats: true

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --master
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "ceph orch restart {service_name:shared.pri.io}"
              - "ceph orch restart {service_name:shared.pri.sync}"
              - "sleep 120"
      desc: promote secondary as master
      name: promote secondary as master
      module: exec.py
      polarion-id: CEPH-83608639

  - test:
      name: suspend bucket versioning on primary
      desc: test_versioning_objects_suspend on secondary
      polarion-id: CEPH-14263
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_versioning_with_objects.py
            config-file-name: test_versioning_objects_suspend.yaml
            verify-io-on-site: ["ceph-pri"]

  - test:
      name: multipart upload on primary
      desc: test_Mbuckets_with_Nobjects_multipart on primary
      polarion-id: CEPH-14265
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-sec"]
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml
            monitor-consistency-bucket-stats: true

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin zonegroup remove --rgw-zone primary --rgw-zonegroup shared"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "sleep 120"
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: bring primary down by removing it from the multisite configuration & update period
      name: remove primary from multisite configuration
      module: exec.py
      polarion-id: CEPH-83608639

  - test:
      name: replace bucket policy on secondary
      desc: test_bucket_policy_replace on secondary
      polarion-id: CEPH-11215
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_bucket_policy_ops.py
            config-file-name: test_bucket_policy_replace.yaml

  - test:
      abort-on-fail: true
      clusters:
        ceph-ter:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-sec#node3}:81 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-sec#node3}:81 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone tertiary --endpoints http://{node_ip:node3}:81,http://{node_ip:node4}:81 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.ter.io}"
              - "ceph orch restart {service_name:shared.ter.sync}"
              - "sleep 120"
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
              - "ceph osd dump"
              - "ceph orch ls"
      desc: Setting up 2-way RGW multisite replication betwwen secondary and tertiary environment
      module: exec.py
      name: Setting up 2-way RGW multisite replication betwwen secondary and tertiary environment
      polarion-id: CEPH-83608639

# configuring HAproxy for IO RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-ter:
          config:
            haproxy_clients:
              - node5
            rgw_endpoints:
              - node3:80
              - node4:80
      desc: "Configure HAproxy for IO rgws"
      module: haproxy.py
      name: "Configure HAproxy for IO rgws"

  - test:
      clusters:
        ceph-sec:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-ter
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

  - test:
      name: S3CMD object download on tertiary
      desc: S3CMD object download or GET on the tertiary cluster
      polarion-id: CEPH-83575477
      module: sanity_rgw_multisite.py
      clusters:
        ceph-ter:
          config:
            script-name: ../s3cmd/test_s3cmd.py
            config-file-name: ../../s3cmd/configs/test_get_s3cmd_tertiary.yaml
