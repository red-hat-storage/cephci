# This suites perform io on a cluster where rgw deplyed using a adavance rgw spec file updation to stop the RGW io sending multisite sync data
# New feature of 9.x
# test with conf file: conf/tentacle/rgw/rgw_multisite_spec.yaml
tests:

  # Cluster deployment stage

  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.pri.io
                      spec:
                        disable_multisite_sync_traffic: true
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: primary
                      placement:
                        nodes:
                          - node4
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.pri.sync
                      spec:
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: primary
                      placement:
                        nodes:
                          - node5
        ceph-sec:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-dashboard: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.sec.io
                      spec:
                        disable_multisite_sync_traffic: true
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: secondary
                      placement:
                        nodes:
                          - node4
              - config:
                  command: apply_spec
                  service: orch
                  specs:
                    - service_type: rgw
                      service_id: shared.sec.sync
                      spec:
                        rgw_realm: india
                        rgw_zonegroup: shared
                        rgw_zone: secondary
                      placement:
                        nodes:
                          - node5
      desc: Cluster deployment to stop the RGW io sending multisite sync data
      polarion-id: CEPH-83591291
      destroy-cluster: false
      module: test_cephadm.py
      name: deploy cluster with advance fetaure in rgw spec file
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.1
            node: node6
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.2
            node: node5
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.2
            node: node5
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system in sync node
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            command: add
            id: client.3
            node: node4
            install_packages:
              - ceph-common
            copy_admin_keyring: true
        ceph-sec:
          config:
            command: add
            id: client.3
            node: node4
            install_packages:
              - ceph-common
            copy_admin_keyring: true
      desc: Configure the RGW client system in io node
      destroy-cluster: false
      module: test_client.py
      name: configure client
      polarion-id: CEPH-83573758
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints http://{node_ip:node5}:80 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.pri.io}"
              - "ceph orch restart {service_name:shared.pri.sync}"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
              - "sleep 120"
      desc: Setting up RGW multisite replication environment
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-83591291
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "ceph config dump | grep rgw_run_sync_thread"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on primary
      polarion-id: CEPH-83591291
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin sync status"
              - "ceph -s"
              - "ceph config dump | grep rgw_run_sync_thread"
              - "radosgw-admin realm list"
              - "radosgw-admin zonegroup list"
              - "radosgw-admin zone list"
      desc: Retrieve the configured environment details
      module: exec.py
      name: get shared realm info on secondary
      polarion-id: CEPH-83591291
  # create user from primary
  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      name: create non-tenanted user

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: ../s3cmd/test_s3cmd.py
            config-file-name: ../../s3cmd/multisite_configs/test_s3cmd_put_obj_primary_cp_obj_secondary.yaml
      desc: test put object from primary and copy objevts from secondary
      module: sanity_rgw_multisite.py
      name: Put object from primary and copy objevts from secondary
      polarion-id: CEPH-83628287

  - test:
      name: bucket granular sync policy via s3 replication
      desc: Test bucket granular sync policy via s3 replication
      polarion-id: CEPH-83575876
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../aws/test_aws_s3_replication.py
            config-file-name: ../../aws/multisite_configs/test_aws_s3_bucket_replication.yaml

  - test:
      name: modify bucket granular sync policy which has user mode set
      desc: Test modify bucket granular sync policy which has user mode set
      polarion-id: CEPH-83629972
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../aws/test_aws_s3_replication.py
            config-file-name: ../../aws/multisite_configs/test_aws_granular_sync_policy_pipe_modify.yaml

  - test:
      name: bucket granular sync policy with sync to different bucket
      desc: Test bucket granular sync with sync to different bucket
      polarion-id: CEPH-83575141
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_sync_to_diff_bucket.yaml
            timeout: 5500

  - test:
      name: bucket granular sync policy with sync from different bucket
      desc: Test bucket granular sync with sync from different bucket
      polarion-id: CEPH-83575140
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_sync_from_diff_bucket.yaml
            timeout: 5500
