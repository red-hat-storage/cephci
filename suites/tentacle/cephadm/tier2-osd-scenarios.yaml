#===============================================================================================
# Tier-level: 2
# Test-Suite: tier2-osd-scenarios.yaml
#
# Cluster Configuration:
#   conf/tentacle/cephadm/1admin-4node-1client-cluster.yaml
#
# Test steps:
#   (1) Bootstrap cluster with options,
#       - skip-monitoring-stack
#       - orphan-initial-daemons
#       - fsid : f64f341c-655d-11eb-8778-fa163e914bcc
#       - initial-dashboard-user: admin123
#       - initial-dashboard-password: admin@123,
#       - registry-json: registry.json
#       - apply-spec: <list of service specification containing multiple admin nodes, mon, mgr, raw osd and rgw deployment>
#       - ssh-user: <ssh user name>
#       - ssh-public-key: <path to the custom ssh public key file>
#       - ssh-private-key: <path to the custom ssh private key file>
#       - mon-ip: <monitor IP address: Required>
#    (2) Add some OSD
#    (3) Perform Replace OSD
#    (4) Test OSD Memory Target
#===============================================================================================
tests:
  - test:
      name: Install ceph pre-requisites
      desc: installation of ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

  - test:
      name: Cephadm Bootstrap with apply-spec option.
      desc: bootstrap with apply-spec option.
      module: test_bootstrap.py
      config:
        command: bootstrap
        base_cmd_args:
          verbose: true
        args:
          registry-json: registry.redhat.io
          custom_image: true
          mon-ip: node1
          fsid: f64f341c-655d-11eb-8778-fa163e914bcc
          orphan-initial-daemons: true
          ssh-user: cephuser
          ssh-public-key: /home/cephuser/.ssh/id_rsa.pub # if ssh-public-key is provided then provide
          ssh-private-key: /home/cephuser/.ssh/id_rsa # ssh-private-key also else validation fails
          apply-spec:
            - service_type: host
              address: true
              labels:
                - admin
                - mgr
                - alertmanager
              nodes:
                - node1
                - node2
                - node3
            - service_type: mon
              placement:
                nodes:
                  - node1
                  - node2
                  - node3
            - service_type: mgr
              placement:
                label: mgr
            - service_type: prometheus
              placement:
                count: 1
                nodes:
                  - node1
            - service_type: grafana
              placement:
                nodes:
                  - node1
            - service_type: alertmanager
              placement:
                count: 2
                label: alertmanager
            - service_type: node-exporter
              placement:
                host_pattern: "*"
            - service_type: crash
              placement:
                host_pattern: "*"
            - service_type: osd
              service_id: osd_first_node
              placement:
                nodes:
                  - node1
              spec:
                data_devices:
                  all: "true"
                encrypted: "true"                     # boolean as string
      destroy-cluster: false
      abort-on-fail: true

  - test:
      name: Add osd
      desc: Add osd on second node
      module: test_daemon.py
      config:
        command: add
        service: osd
        pos_args:
          - "node3"
          - "/dev/vdb"
          - "/dev/vdc"
          - "/dev/vdd"
      destroy-cluster: false
      abort-on-fail: true

  - test:
      name: Replace OSD
      desc: Perform replace OSD operation
      module: test_replace_osd.py
      polarion-id: CEPH-83573766
      config:
        replace:
          command: rm
          base_cmd_args:
            verbose: true
          pos_args:
            - 6
          nodes:
            - "node3"
        add:
          command: add
          service: osd
          pos_args:
            - "node3"
            - "/dev/vde"
        destroy-cluster: false
        abort-on-fail: true

  - test:
      name: Test OSD Memory Target
      desc: Set OSD Memory Target on OSD and Host
      module: test_osd_memory_target_cephadm.py
      config:
        hosts:
            - node3
        value: 8888888888
      polarion-id: CEPH-83575349
      abort-on-fail: true

  - test:  # The test is expected fail till BZ:2259884 is fixed
      name: Test OSD memory usage
      desc: Verify ceph orch ps output does show an incorrect amount of used memory for OSDs
      module: test_verify_incorrect_mem_usage.py
      polarion-id: CEPH-83584007
      abort-on-fail: false

  - test:
      name: Test OSD blocklisted clients
      desc: Verify ceph OSD blocklist client list
      module: test_osd_blocklist_operations.py
      polarion-id: CEPH-83593817
      abort-on-fail: true
