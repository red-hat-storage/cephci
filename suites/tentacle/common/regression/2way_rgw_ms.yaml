# Conf file : conf/tentacle/common/ms_2way_9node_1client_rh.yaml
# deployment: suites/tentacle/common/regression/ms_2way_deploy-and-configure.yaml

tests:

  - test:
      name: get cluster and multisite information from both cluster parallely
      desc: Parallely get cluster and multisite information from both cluster
      module: test_parallel.py
      parallel:
        - test:
            abort-on-fail: true
            clusters:
              ceph-pri:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
            desc: Retrieve primary environment details
            module: exec.py
            name: get shared realm info on primary before starting the IO
            polarion-id: CEPH-83591291
        - test:
            abort-on-fail: true
            clusters:
              ceph-sec:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
                    - "sleep 120"
            desc: Retrieve secondary environment details
            module: exec.py
            name: get shared realm info on secondary before starting the IO
            polarion-id: CEPH-83591291

# configuring HAproxy for IO RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node2:80
              - node3:80
        ceph-sec:
          config:
            haproxy_clients:
              - node6
            rgw_endpoints:
              - node2:80
              - node3:80
      desc: "Configure HAproxy for IO rgws"
      module: haproxy.py
      name: "Configure HAproxy for IO rgws"

  # create user from primary
  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-sec
      desc: create non-tenanted user
      polarion-id: CEPH-83575199
      module: sanity_rgw_multisite.py
      name: create non-tenanted user

  - test:
      name: Delete object version with id null
      desc: test_delete_object_version_id_null
      polarion-id: CEPH-83576431
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: ../aws/test_delete_version_id_null.py
            config-file-name: ../../aws/configs/test_delete_version_id_null.yaml

# Bucket generation verify
  - test:
      name: Bucket generation verification
      desc: Bucket generation verification
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83574423
      clusters:
        ceph-pri:
          config:
            script-name: test_dynamic_bucket_resharding.py
            config-file-name: test_bucket_generation.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]

#  Dynamic resharding tests
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_bucket_chown_reshard.yaml
            script-name: test_dynamic_bucket_resharding.py
      desc: Resharding test - Test manual reshard after bucket owner change
      name: Test manual reshard after bucket owner change
      polarion-id: CEPH-83574627
      module: sanity_rgw_multisite.py
  - test:
      name: Disable DBR feature on zonegroup
      desc: Disable DBR feature on zonegroup
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83574626
      clusters:
        ceph-pri:
          config:
            script-name: test_dynamic_bucket_resharding.py
            config-file-name: test_resharding_disable_in_zonegroup.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]

  - test:
      name: Re-enable DBR feature on cluster and Verify
      desc: Re-enable DBR feature on cluster an verify
      module: sanity_rgw_multisite.py
      polarion-id: CEPH-83573596
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_check_sharding_enabled_brownfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dbr_tenanted_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
      desc: Test dynamic resharding on greenfield deployment tenanted user
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster tenanted user
      polarion-id: CEPH-83573596
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dbr_versioning_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
      desc: Test dynamic resharding on versioned bucket
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster
      polarion-id: CEPH-83573596
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dbr_versioning_tenanted_greenfield.yaml
            verify-io-on-site: ["ceph-pri", "ceph-sec"]
      desc: Test dynamic resharding on versioned bucket and tenanted user
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on versioned bucket and tenanted user Primary cluster
      polarion-id: CEPH-83573596
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_dynamic_resharding_with_version_without_bucket_delete.yaml
            script-name: test_dynamic_bucket_resharding.py
            verify-io-on-site: ["ceph-pri", "ceph-sec" ]
      desc: Resharding test - dynamic resharding on versioned bucket
      name: Dynamic Resharding on versioned buckets
      polarion-id: CEPH-83571740
      module: sanity_rgw_multisite.py
  - test:
      clusters:
        ceph-pri:
          config:
            config-file-name: test_dynamic_resharding_quota_exceed.yaml
            script-name: test_dynamic_bucket_resharding.py
      desc: test exceeding quota limit on dynamically resharded bucket able to access bucket on secondary
      name: test exceeding quota limit on dynamically resharded bucket able to access bucket on secondary
      polarion-id: CEPH-83574669
      module: sanity_rgw_multisite.py

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_sync_disable_enable.yaml
      desc: Test sync enable and disable with manual resharding
      module: sanity_rgw_multisite.py
      name: Manual Resharding tests on Primary cluster
      polarion-id: CEPH-83574434

  - test:
      name: bucket granular sync policy with sync from differnt bucket
      desc: Test bucket granular sync with sync from differnt bucket
      polarion-id: CEPH-83575140
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_sync_from_diff_bucket.yaml
            timeout: 5500

  - test:
      name: bucket granular sync policy with sync to differnt bucket
      desc: Test bucket granular sync with sync to differnt bucket
      polarion-id: CEPH-83575141
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_sync_to_diff_bucket.yaml
            timeout: 5500

  - test:
      name: bucket granular sync policy on enabled forbidden semantic with symmetrical and directional flow
      desc: Test bucket granular sync policy on enabled forbidden semantic with symmetrical and directional flow
      polarion-id: CEPH-83575139
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_enabled_forbidden.yaml
            timeout: 5500

  - test:
      name: bucket granular sync policy on forbidden allowed semantic with symmetrical and directional flow
      desc: Test bucket granular sync policy on forbidden allowed semantic symmetrical and directional flow
      polarion-id: CEPH-83575139
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_forbidden_allowed.yaml
            timeout: 5500

  - test:
      name: bucket granular sync policy on forbidden enabled semantic with directional and symmetrical flow
      desc: Test bucket granular sync policy on forbidden enabled semantic with directional flow
      polarion-id: CEPH-83575139
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_multisite_bucket_granular_sync_policy.py
            config-file-name: test_multisite_granular_bucketsync_forbidden_enabled.yaml
            timeout: 5500

  - test:
      name: notify copy events with kafka_broker_persistent
      desc: notify copy events with kafka_broker_persistent
      polarion-id: CEPH-83574066
      module: sanity_rgw_multisite.py
      config:
        run-on-rgw: true
        extra-pkgs:
          - jdk
        install_start_kafka: true
        script-name: test_bucket_notifications.py
        config-file-name: test_bucket_notification_kafka_broker_persistent_copy.yaml

  - test:
      name: notify put,delete events with kafka_broker_persistent
      desc: notify put,delete events with kafka_broker_persistent
      polarion-id: CEPH-83574086
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_broker_persistent_delete.yaml

  - test:
      name: notify on multipart upload events with kafka_broker_persistent
      desc: notify on multipart upload events with kafka_broker_persistent
      polarion-id: CEPH-83574086
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_broker_persistent_multipart.yaml

  - test:
      name: notify copy events with kafka_none
      desc: notify copy events with kafka_none
      polarion-id: CEPH-83574066
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_none_copy.yaml

  - test:
      name: notify on put,copy,delete events with kafka_broker_persistent when kafka is down
      desc: notify on put,copy,delete events with kafka_broker_persistent when kafka is down
      polarion-id: CEPH-83574078
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_down_broker_persistent.yaml

  - test:
      name: notify on multipart events with kafka_broker_persistent when kafka is down
      desc: notify on multipart events with kafka_broker_persistent when kafka is down
      polarion-id: CEPH-83574417
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_down_broker_persistent_multipart.yaml

  - test:
      name: notify on multisite replication create events with kafka_broker on pri site
      desc: notify on multisite replication create events with kafka_broker on pri site
      polarion-id: CEPH-83575571
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_broker_ms_replication_from_pri.yaml

  - test:
      name: notify on multisite replication create events with kafka_broker on sec site
      desc: notify on multisite replication create events with kafka_broker on sec site
      polarion-id: CEPH-83575571
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            run-on-rgw: true
            script-name: test_bucket_notifications.py
            config-file-name: test_bucket_notification_kafka_broker_ms_replication_from_sec.yaml

  - test:
      name: resharding in versioned bucket with and without suspend versioning
      desc: test resharding in versioned bucket with and without suspend versioning
      polarion-id: CEPH-83574431
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_dynamic_bucket_resharding.py
            verify-io-on-site: ["ceph-sec"]
            config-file-name: test_versioning_objects_suspend_enable.yaml

  # IO performed should be readable on secondary
  - test:
      name: multipart upload on primary
      desc: test_Mbuckets_with_Nobjects_multipart on primary
      polarion-id: CEPH-14265
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-sec"]
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml

  # IO should fail on read only secondary
  - test:
      name: object upload on secondary
      desc: test_Mbuckets_with_Nobjects_failed on secondary
      polarion-id: CEPH-14265
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects_failed.yaml


  # Disable multisite sync between primary and secondary zones and test replication doesn't happen

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            role: rgw
            sudo: True
            commands:
              - "yum install -y jq"
              - "radosgw-admin zonegroup get --rgw-zonegroup=shared > /tmp/zonegroup_shared_backup.json"
              - "jq -r '.zones[].log_data=false | .zones[].sync_from_all=false' /tmp/zonegroup_shared_backup.json > /tmp/zonegroup_shared.json"
              - "radosgw-admin zonegroup set --rgw-zonegroup=shared --infile=/tmp/zonegroup_shared.json"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "sleep 20"
      desc: Disabling multisite sync between primary and secondary zones
      module: exec.py
      name: disable multisite sync between zones
      polarion-id: CEPH-83581229

  - test:
      name: Test the byte ranges with get object on primary zone and check multisite replication doesn't happen
      desc: Test the byte ranges with get_object on primary zone and check multisite replication doesn't happen
      polarion-id: CEPH-83572691
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            test-version: v2
            script-name: test_byte_range.py
            config-file-name: ../configs/test_byte_range.yaml
            multisite-replication-disabled: True
            verify-io-on-site: ["ceph-sec"]

  - test:
      name: Test the byte ranges with get object on secondary zone and check multisite replication doesn't happen
      desc: Test the byte ranges with get_object on secondary zone and check multisite replication doesn't happen
      polarion-id: CEPH-83572691
      module: sanity_rgw_multisite.py
      clusters:
        ceph-sec:
          config:
            test-version: v2
            script-name: test_byte_range.py
            config-file-name: ../configs/test_byte_range.yaml
            multisite-replication-disabled: True
            verify-io-on-site: ["ceph-pri"]

  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            role: rgw
            sudo: True
            commands:
              - "radosgw-admin zonegroup get --rgw-zonegroup=shared > /tmp/zonegroup_shared_backup.json"
              - "jq -r '.zones[].log_data=true | .zones[].sync_from_all=true' /tmp/zonegroup_shared_backup.json > /tmp/zonegroup_shared.json"
              - "radosgw-admin zonegroup set --rgw-zonegroup=shared --infile=/tmp/zonegroup_shared.json"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "sleep 120"
      desc: enable multisite sync between primary and secondary zones
      module: exec.py
      name: enable multisite sync between zones
      polarion-id: CEPH-83581229

  # Failover Primary, make secondary writable
  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --master --default --read-only=false"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
              - "sleep 120"
      desc: RGW multisite failover
      module: exec.py
      name: Failover to secondary
      polarion-id: CEPH-10362

  # IO on secondary which is current primary
  - test:
      clusters:
        ceph-sec:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_Mbuckets_with_Nobjects.yaml
      desc: test M buckets uploads on current primary zone
      module: sanity_rgw_multisite.py
      name: test M buckets uploads on current primary zone
      polarion-id: CEPH-83575433

# configuring HAproxy for Sync RGW daemons on port '5000'
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            haproxy_clients:
              - node1
            rgw_endpoints:
              - node2:80
              - node3:80
        ceph-sec:
          config:
            haproxy_clients:
              - node1
            rgw_endpoints:
              - node2:80
              - node3:80
      desc: "Configure HAproxy for sync rgws"
      module: haproxy.py
      name: "Configure HAproxy for sync rgws"

  # Failback to make primary master again and secondary read-only
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-sec#node1}:5000 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zonegroup modify --rgw-realm india --rgw-zonegroup shared --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --master --default --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.pri.io}"
              - "ceph orch restart {service_name:shared.pri.sync}"
              - "sleep 120"
      desc: RGW multisite failback
      module: exec.py
      name: Failback to primary
      polarion-id: CEPH-10362

  - test:
      abort-on-fail: true
      clusters:
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --read-only=true --endpoints http://{node_ip:node1}:5000"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph orch restart {service_name:shared.sec.io}"
              - "ceph orch restart {service_name:shared.sec.sync}"
              - "sleep 120"
      desc: RGW secondary read-only
      module: exec.py
      name: Failover to secondary
      polarion-id: CEPH-10362

  - test:
      name: get cluster and multisite information from both cluster parallely
      desc: Parallely get cluster and multisite information from both cluster
      module: test_parallel.py
      parallel:
        - test:
            abort-on-fail: true
            clusters:
              ceph-pri:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
            desc: Retrieve primary environment details
            module: exec.py
            name: get shared realm info on primary before starting the IO
            polarion-id: CEPH-83591291
        - test:
            abort-on-fail: true
            clusters:
              ceph-sec:
                config:
                  cephadm: true
                  commands:
                    - "ceph -s"
                    - "radosgw-admin sync status"
                    - "ceph config dump | grep rgw_run_sync_thread"
                    - "radosgw-admin realm list"
                    - "radosgw-admin zonegroup list"
                    - "radosgw-admin zone list"
                    - "sleep 120"
            desc: Retrieve secondary environment details
            module: exec.py
            name: get shared realm info on secondary before starting the IO
            polarion-id: CEPH-83591291

  # IO from primary again
  - test:
      name: Object upload on primary
      desc: test_Mbuckets_with_Nobjects on primary
      polarion-id: CEPH-14265
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            verify-io-on-site: ["ceph-sec"]
            config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_zone_deletion.yaml
      desc: Test zone deletion in master
      module: sanity_rgw_multisite.py
      name: Perform zone deletion in master
      polarion-id: CEPH-10753

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_zone_rename.yaml
      desc: Test zone rename in master
      module: sanity_rgw_multisite.py
      name: Perform zone rename in master
      polarion-id: CEPH-10740

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_check_sharding_enabled.py
            config-file-name: test_realm_rename.yaml
      desc: Test realm rename in master
      module: sanity_rgw_multisite.py
      name: Perform realm rename in master
      polarion-id: CEPH-10739
