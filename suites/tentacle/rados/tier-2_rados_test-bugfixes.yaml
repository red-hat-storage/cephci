# Suite contains  tier-2 rados bug verification automation
#===============================================================================================
#------------------------------------------------------------------------------------------
#----- Tier-2 - Bug verification  automation   ------
#------------------------------------------------------------------------------------------
# Conf: conf/tentacle/rados/7-node-cluster.yaml
# Bugs:
#     1. https://bugzilla.redhat.com/show_bug.cgi?id=2233800
#     2. https://bugzilla.redhat.com/show_bug.cgi?id=1900127
#     3. https://bugzilla.redhat.com/show_bug.cgi?id=2229651
#     4. https://bugzilla.redhat.com/show_bug.cgi?id=2011756
#     5. https://bugzilla.redhat.com/show_bug.cgi?id=2264053
#     6. https://bugzilla.redhat.com/show_bug.cgi?id=2264054
#     7. https://bugzilla.redhat.com/show_bug.cgi?id=2264052
#     8. https://bugzilla.redhat.com/show_bug.cgi?id=2260306
#     9. https://bugzilla.redhat.com/show_bug.cgi?id=2237038
#     10.https://bugzilla.redhat.com/show_bug.cgi?id=1892173
#     11.https://bugzilla.redhat.com/show_bug.cgi?id=2174954
#     12.https://bugzilla.redhat.com/show_bug.cgi?id=2151501
#     13.https://bugzilla.redhat.com/show_bug.cgi?id=2315595
#     14.https://bugzilla.redhat.com/show_bug.cgi?id=2326179
#     15.https://bugzilla.redhat.com/show_bug.cgi?id=2213766
#     16.https://bugzilla.redhat.com/show_bug.cgi?id=2336885
#     17.https://bugzilla.redhat.com/show_bug.cgi?id=2336886
#     18.https://bugzilla.redhat.com/show_bug.cgi?id=2336352
#     19.https://bugzilla.redhat.com/show_bug.cgi?id=2421457
#===============================================================================================
# RHOS-d run duration: 330 mins
tests:
  - test:
      name: setup install pre-requisistes
      desc: Setup phase to deploy the required pre-requisites for running the tests.
      module: install_prereq.py
      abort-on-fail: true

  - test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                registry-url: registry.redhat.io
                mon-ip: node1
                allow-fqdn-hostname: true
                orphan-initial-daemons: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: shell
              args:          # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
          - config:
              command: apply
              service: mds
              base_cmd_args:          # arguments to ceph orch
                verbose: true
              pos_args:
                - cephfs              # name of the filesystem
              args:
                placement:
                  nodes:
                    - node2
                    - node6
                  limit: 2            # no of daemons
                  sep: " "            # separator to be used for placements
      destroy-cluster: false
      abort-on-fail: true

  - test:
      name: Configure client admin
      desc: Configures client admin node on cluster
      module: test_client.py
      polarion-id:
      config:
        command: add
        id: client.1                      # client Id (<type>.<Id>)
        node: node7                       # client node
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true          # Copy admin keyring to node
        caps:                             # authorize client capabilities
          mon: "allow *"
          osd: "allow *"
          mds: "allow *"
          mgr: "allow *"

  - test:
      name: Enable logging to file
      module: rados_prep.py
      config:
        log_to_file: true
      desc: Change config options to enable logging to file

  - test:
      name: Verification of the Mon ballooning
      module: test_rados_ballooning_client.py
      polarion-id: CEPH-83584004
      desc: Verify MON ballooning client

# moved prior to Inconsistent object test due to chances of an OSD being down
  - test:
      name: OSD restart when bluefs_shared_alloc_size is lower than bluestore_min_alloc_size
      module: test_bug_fixes.py
      config:
        lower_bluefs_shared_alloc_size: true
      polarion-id: CEPH-83591092
      desc: verify OSD resiliency when 'bluefs_shared_alloc_size' is below 'bluestore_min_alloc_size'

  - test:
      name: Verification the recovery flag functionality
      desc: Verify the pg_num and recovery status
      module: test_norecovery_flag_functionality.py
      polarion-id: CEPH-83591783
      config:
        replicated_pool:
          create: true
          pool_name: recovery_pool
      delete_pool:
        - recovery_pool

  - test:
      name: Verification of ceph config show & get
      module: test_bug_fixes.py
      config:
        test-config-show-get: true
      polarion-id: CEPH-83590689
      desc: Verify ceph config show & get outputs

  - test:
      name: Verification of Slow OSD heartbeat
      module: test_bug_fixes.py
      config:
        slow-osd-heartbeat: true
      polarion-id: CEPH-83590688
      desc: Generate Slow OSD heartbeats by inducing network delay

  - test:
      name: Client connection over v1 port
      module: test_v1client.py
      polarion-id: CEPH-83594645
      desc: Ensure client connection over v1 port does not crash

  - test:
      name: Mon connection scores testing
      desc: Verification of mon connection scores in different scenarios
      module: test_mon_connection_scores.py
      polarion-id: CEPH-83602911
      comments: Intermittent bug 2357894

  - test:
      name: Verify config values via cephadm-ansible
      module: test_cephadm_ansible.py
      polarion-id: CEPH-83602686
      desc: Ensure configuration values are accessible via cephadm-ansible module

  - test:
      name: Verify read crc error message
      module: test_crc_check_logs.py
      desc: Verify read crc error message in the osd logs
      polarion-id: CEPH-83612766
      config:
        create_pools:
          - create_pool:
              pool_name: replicated_pool
              pool_type: replicated
              pg_num: 1
              pg_num_max: 1

          - create_pool:
              pool_name: ec_pool
              pool_type: erasure
              pg_num: 1
              pg_num_max: 1
              k: 3
              m: 2
              plugin: jerasure
              crush-failure-domain: osd

  - test:
      name: Verify that there are no 'defunct' ssh connections after the manager is redeployed
      module: test_defunct_ssh_connections.py
      polarion-id: CEPH-83602699
      desc: ceph-mgr cephadm ssh connections 'defunct' after the manager is redeployed

  - test:
      name: Verify that there are no CEPHADM_STRAY_DAEMON warning while replacing the osd
      module: test_bug_fixes.py
      config:
        stray_daemon_warning: true
      polarion-id: CEPH-83615076
      desc: No CEPHADM_STRAY_DAEMON warning while replacing the osd

  - test:
      name: Verify that OSDs can be added-moved to new spec
      module: test_bug_fixes.py
      config:
        test_osd_spec_update: true
      polarion-id: CEPH-83615839
      desc: Verify that OSDs can be added-moved to new spec

  - test:
      name: Verify watchers are properly cleaned up
      module: test_rados_watcher_cleanup.py
      config:
          Single_Client_watcher: true
      polarion-id: CEPH-83616882
      desc: Ensures watchers do not persist after client disconnects or interruptions

  # max run time for this test is 4 mins
  - test:
      name: CephFS flush kernel mnt with RE pools
      module: test_bug_fixes.py
      config:
        cephfs_file_flush_issue: true
        pool_type: replicated
        mount_type: kernel
      polarion-id: CEPH-83632223
      desc: Verify file contents are preserved after unmount-remount with kernel client using replicated pools

  # max run time for this test is 4 mins
  - test:
      name: CephFS flush kernel mnt with EC pools
      module: test_bug_fixes.py
      config:
        cephfs_file_flush_issue: true
        pool_type: ec
        mount_type: kernel
      polarion-id: CEPH-83632223
      desc: Verify file contents are preserved after unmount-remount with kernel client using EC pools

  # max run time for this test is 4 mins
  - test:
      name: CephFS flush fuse mnt with RE pools
      module: test_bug_fixes.py
      config:
        cephfs_file_flush_issue: true
        pool_type: replicated
        mount_type: fuse
      polarion-id: CEPH-83632223
      desc: Verify file contents are preserved after unmount-remount with fuse client using replicated pools

  # max run time for this test is 4 mins
  - test:
      name: CephFS flush fuse mnt with EC pools
      module: test_bug_fixes.py
      config:
        cephfs_file_flush_issue: true
        pool_type: ec
        mount_type: fuse
      polarion-id: CEPH-83632223
      desc: Verify file contents are preserved after unmount-remount with fuse client using EC pools
