#===============================================================================================
#-------------------------------------
#---    Test Suite for Nfs Ganesha ---
#-------------------------------------
# Conf: conf/tentacle/nfs/1admin-7node-3client.yaml
# Smoke test cases for
#    - Bootstrap
#    - Host management
#    - Configure nfs-ganesha on nfs server,do mount on any client and do IOs
#    - Test NFS server with SPECstorage benchmark
#
#===============================================================================================
tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      config:
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                mon-ip: node1
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
          - config:
              args:
                - "ceph fs volume create cephfs"
              command: shell
          - config:
              args:
                placement:
                  label: mds
              base_cmd_args:
                verbose: true
              command: apply
              pos_args:
                - cephfs
              service: mds
          - config:
              args:
                - "ceph osd pool create rbd"
              command: shell
          - config:
              args:
                - "rbd pool init rbd"
              command: shell
      desc: bootstrap and deploy services.
      destroy-cluster: false
      polarion-id: CEPH-83573713
      module: test_cephadm.py
      name: Deploy cluster using cephadm

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node4
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.2
        node: node5
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.3
        node: node6
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.4
        node: node7
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      name: Test nfs server with SPECstorage workload SWBUILD
      module: nfs_spec_storage.py
      desc: Test nfs server performance with SPECstorage workload SWBUILD
      polarion-id:
      abort-on-fail: false
      config:
        mount_point: /mnt/nfs
        nfs_version: 4.1
        benchmark: SWBUILD
        load: 1
        incr_load: 1
        num_runs: 1
        clients:
          - client1
          - client2
          - client3
          - client4
        benchmark_defination:
          Warmup_time: 30
          Dir_count: 15
          Files_per_dir: 30
          File_size: 3k
          Instances: 4

  - test:
      name: Test nfs scale with spec storage
      module: nfs_verify_scale.py
      desc: Test nfs scale setup with spec storage io tool
      polarion-id:
      abort-on-fail: false
      config:
        execution_type: parallel
        num_of_servers: 1
        num_of_exports: 4
        num_of_clients: 4
        exports_per_client: 1
        nfs_version: 4.1
        benchmark: SWBUILD
        load: 1
        incr_load: 1
        num_runs: 1
        clients:
          - client1
          - client2
          - client3
          - client4
        benchmark_defination:
          Warmup_time: 30
          Dir_count: 15
          Files_per_dir: 30
          File_size: 3k
          Instances: 4
