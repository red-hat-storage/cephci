#===============================================================================================
#-------------------------------------
#---    Test Suite for Nfs Ganesha Functional Sanity Tests---
#-------------------------------------
# Conf: conf/tentacle/nfs/1admin-7node-3client.yaml
# Functional sanity test cases for
#    - Bootstrap
#    - Host management
#    - Configure nfs-ganesha on nfs server,do mount on any client and do IOs
#    - Test NFS cluster and export create
#    - QoS, gRPC, and BYOK functional tests
#
#===============================================================================================
tests:
  - test:
      abort-on-fail: true
      desc: Install software pre-requisites for cluster deployment.
      module: install_prereq.py
      name: setup pre-requisites

  - test:
      abort-on-fail: true
      config:
        steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                mon-ip: node1
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: apply
              service: rgw
              pos_args:
                - rgw.1
              args:
                placement:
                  label: rgw
          - config:
              args:
                - "ceph fs volume create cephfs"
              command: shell
          - config:
              args:
                placement:
                  label: mds
              base_cmd_args:
                verbose: true
              command: apply
              pos_args:
                - cephfs
              service: mds
          - config:
              args:
                - "ceph osd pool create rbd"
              command: shell
          - config:
              args:
                - "rbd pool init rbd"
              command: shell
      desc: bootstrap and deploy services.
      destroy-cluster: false
      polarion-id: CEPH-83573713
      module: test_cephadm.py
      name: Deploy cluster using cephadm

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.1
        node: node4
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.2
        node: node5
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.3
        node: node6
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      abort-on-fail: true
      config:
        command: add
        id: client.4
        node: node7
        install_packages:
          - ceph-common
          - ceph-fuse
        copy_admin_keyring: true
      desc: Configure the RGW,RBD client system
      destroy-cluster: false
      module: test_client.py
      name: configure client

  - test:
      name: Qos enablement PerShare-PerClient on export level with nfs service with restart
      module: qos.test_nfs_qos_on_export_level_enablement.py
      desc: Verify qos PerShare-PerClient enablement on export level
      polarion-id: CEPH-83613691
      abort-on-fail: false
      config:
        control: bandwidth_control
        cephfs_volume : cephfs
        cluster_name : cephfs-nfs
        cluster_bw :
          - max_export_write_bw : 100MB
            max_export_read_bw : 100MB
            max_client_write_bw : 100MB
            max_client_read_bw: 100MB
        export_bw:
          - max_export_write_bw: 8MB
            max_export_read_bw: 8MB
            max_client_write_bw: 8MB
            max_client_read_bw: 8MB
        qos_type : PerShare_PerClient
        operation : restart

  - test:
      name: Qos PerShare enablement on Cluster level with nfs with service restart
      module: qos.test_nfs_qos_on_cluster_level_enablement.py
      desc: Verify qos Pershare enablement on cluster level
      polarion-id: CEPH-83611390
      abort-on-fail: false
      config:
        control: bandwidth_control
        cephfs_volume : cephfs
        cluster_name : cephfs-nfs
        max_export_write_bw : 8MB
        max_export_read_bw : 8MB
        max_client_write_bw : 8MB
        max_client_read_bw: 8MB
        qos_type : PerShare
        operation : restart

  - test:
      name: NFS gRPC - Verify ID Updates After Client Unmount
      module: test_nfs_grpc.py
      desc: Unmount from 1 client and verify client/session IDs are updated
      polarion-id: CEPH-83630616
      abort-on-fail: false
      config:
        operation: verify_id_after_unmount
        nfs_name: cephfs-nfs
        nfs_export: /export
        nfs_mount: /mnt/nfs
        fs_name: cephfs
        clients: 3
        nfs_version: 4.1
        port: 2049

  - test:
      name: Test Encryption with the correct keys (single cluster)
      module: byok.test_byok_single_multi_restart.py
      desc: BYOK single-cluster with FUSE encryption validation
      polarion-id: CEPH-83625072
      config:
         nfs_version: 4.2
         total_export_num: 4
         clients: 1
         nfs_replication_number: 1
         nfs_mount: /mnt/nfs_byok
         nfs_export: /export_byok
         nfs_instance_name: nfs_byok

