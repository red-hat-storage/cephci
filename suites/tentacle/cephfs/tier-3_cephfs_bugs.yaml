#=======================================================================================================================
# Tier-level: 3
# Test-Suite: cephfs_bugs
# Conf file : conf/pacific/cephfs/tier-3_cephfs_4clients_4osd.yaml
# Description - This test suite contains tests which were automated during bug verification
# Test-Case Covered:
  # CEPH-11260 - Change replication size of Cephfs pools increase and decrease, with client IO
  # CEPH-83574833 - Performace test for file write_sync() on cephfs kernel mount
  # CEPH-83574632 - Allow recreating file system with specific fscid
  # CEPH-83572726 - Active-Active MDS with multiclients to verify the race condition
  # CEPH-83575622 - create a file(fileA) , create a hard link (fileB),
  #          remove the fileA first and then fileB and run this in a loop for some time and check for deadlock
  # CEPH-83575623 - Run fsstress.sh repeatedly on fuse and kernel clients for a while and validate no crash is seen.
  # CEPH-83575781: Remove contents of lost+found directory.
  # CEPH-83575762: Run create and unlink operations concurrently.
  # CEPH-83575630: Check for large omaps with files and snapshots.
  # CEPH-83575624: Check for Standby-reply nodes changes if there is a network glitch for more than 60 seconds.
  # CEPH-83583762: Check to prevent usage of Pools which has Pool level snaps for Filesystem creation.
  # CEPH-83584084: Validate if no repeated mclientscaps(revoke) entries found in MDS.
  # CEPH-83586730: validate standby replay node not removed after setting mds_inject_health_dummy
  # CEPH-83583721 - Validate the details of all MDS servers are displayed with command `ceph mds metadata`.
  # CEPH-83583723 - Ensure deletion of clones are allowed when the clones are stuck and in pending state
  # CEPH-83589266 - Validate Ceph commands when ceph file system is in failed state
  # CEPH-83592512 - Prevent scrub running using standby MDS
  # CEPH-83592513 - Verify Client evict -h,--help works without running the function
  # CEPH-83609202 - Verify data scan works on missing non-immediate dir frags and FS recovery suceeds
  # CEPH-83593402 - Clone create with parallel volume operations
#=======================================================================================================================
---
tests:
  -
    test:
      abort-on-fail: true
      desc: "Setup phase to deploy the required pre-requisites for running the tests."
      module: install_prereq.py
      name: "setup install pre-requisistes"
  -
    test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                mon-ip: node1
                orphan-initial-daemons: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: shell
              args:          # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - create
                - cephfs-data-ec
                - "64"
                - erasure
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - create
                - cephfs-metadata
                - "64"
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - set
                - cephfs-data-ec
                - allow_ec_overwrites
                - "true"
          - config:
              command: shell
              args: # arguments to ceph orch
                - ceph
                - fs
                - new
                - cephfs-ec
                - cephfs-metadata
                - cephfs-data-ec
                - --force
          - config:
              command: apply
              service: mds
              base_cmd_args:          # arguments to ceph orch
                verbose: true
              pos_args:
                - cephfs              # name of the filesystem
              args:
                placement:
                  label: mds
          - config:
              args:
                - ceph
                - fs
                - set
                - cephfs
                - max_mds
                - "2"
              command: shell
      destroy-cluster: false
      abort-on-fail: true
  - test:
        abort-on-fail: true
        config:
            command: add
            id: client.1
            nodes:
              - node9
              - node10
              - node11
              - node12
            install_packages:
                - ceph-common
            copy_admin_keyring: true
        desc: Configure the Cephfs client system 1
        destroy-cluster: false
        module: test_client.py
        name: configure client
  - test:
      name: Change pg size of cephfs pools
      module: cephfs_bugs.test_cephfs_pool_size_change.py
      polarion-id: CEPH-11260
      desc: test decrease & increase in pg size of cephfs pools
      abort-on-fail: false
  - test:
      name: Performace test for file write_fsync() on cephfs kernel mount
      module: cephfs_bugs.test_write_fsync_on_file_kernel_mount.py
      polarion-id: CEPH-83574833
      desc: Performace test for file write_fsync() on cephfs kernel mount
      abort-on-fail: false
  - test:
      name: Allow recreating file system with specific fscid
      module: cephfs_bugs.test_custom_fscid_cephfs.py
      polarion-id: CEPH-83574632
      desc: Allow recreating file system with specific fscid
      abort-on-fail: false
  - test:
      name: Active-Active MDS with multiclients to verify the race condition
      module: cephfs_bugs.verify_race_condition_between_MDS.py
      polarion-id: CEPH-83572726
      desc: Active-Active MDS with multiclients to verify the race condition
      abort-on-fail: false
  - test:
      name: Validate deadlock between unlink and rename
      module: cephfs_bugs.test_validate_deadlock_bw_unlink_and_rename.py
      polarion-id: CEPH-83575622
      desc: Validate deadlock between unlink and rename
      abort-on-fail: false
  - test:
      name: Run fsstress on kernel and fuse mounts
      module: cephfs_bugs.test_fsstress_on_kernel_and_fuse.py
      polarion-id: CEPH-83575623
      desc: Run fsstress on kernel and fuse mounts
      abort-on-fail: false
  - test:
      name: Remove Contents of lost+found dir
      module: cephfs_bugs.test_rm_on_lost_found_dir.py
      polarion-id: CEPH-83575781
      desc: Remove Contents of lost+found dir
      abort-on-fail: false
  - test:
      name: Run create and unlink operations concurrently.
      module: cephfs_bugs.test_create_unlink_ops_concorrently.py
      polarion-id: CEPH-83575762
      desc: Run create and unlink operations concurrently
      abort-on-fail: false
  - test:
      name: Check for large omaps with files and snapshots
      module: cephfs_bugs.test_large_omap.py
      polarion-id: CEPH-83575630
      desc: Check for large omaps with files and snapshots
      abort-on-fail: false
  - test:
      name: Check for Standby-reply nodes changes if there is network glitch for more than 60 sec
      module: cephfs_bugs.test_mds_laggy_thrown_out.py
      polarion-id: CEPH-83575624
      desc: Check for Standby-reply nodes changes if there is network glitch for more than 60 sec
      abort-on-fail: false
  - test:
      name: Check to prevent usage of Pools which has Pool level snaps for Filesystem creation.
      module: cephfs_bugs.test_disallow_fs_creation_with_pool_level_snaps.py
      polarion-id: CEPH-83583762
      desc: Creation of pool-level snaps for pools actively associated with a filesystem is disallowed
      abort-on-fail: false
  - test:
      name: validate standby reply node not removed after setting mds_inject_health_dummy
      module: cephfs_bugs.test_mds_inject_health_dummy.py
      polarion-id: CEPH-83586730
      desc: validate standby reply node not removed after setting mds_inject_health_dummy
      abort-on-fail: false
  - test:
      name:  Validate the details of all MDS servers are displayed with command ceph mds metadata
      module: cephfs_bugs.validate_mds_metadata.py
      polarion-id: CEPH-83583721
      desc: Validate the details of all MDS servers are displayed with command ceph mds metadata
      abort-on-fail: false
  - test:
      name: Ensure deletion of clones are allowed when the clones are stuck and in pending state
      module: cephfs_bugs.test_clone_delete_with_FS_fail.py
      polarion-id: CEPH-83583723
      desc: Ensure deletion of clones are allowed when the clones are stuck and in pending state
      abort-on-fail: false
  - test:
      name: Validate Ceph commands when ceph file system is in failed state
      module: cephfs_bugs.validate_ceph_ops_wtih_fs_fail.py
      polarion-id: CEPH-83589266
      desc: Validate Ceph commands when ceph file system is in failed state
      abort-on-fail: false
  - test:
      name: Verify Ceph Health Warning for Slow MDS Requests After Cluster Installation
      module: cephfs_bugs.validate_mds_health_with_client_evict.py
      polarion-id: CEPH-83591136
      desc: Verify Ceph Health Warning for Slow MDS Requests After Cluster Installation
      abort-on-fail: false
  - test:
      name: Prevent scrub running using standby MDS
      module: cephfs_bugs.prevent_scrub_with_standby_mds.py
      comments: BZ 2294478
      polarion-id: CEPH-83592512
      desc: Prevent scrub running using standby MDS
      abort-on-fail: false
  - test:
      name: Verify Client evict -h,--help works without running the function
      module: cephfs_bugs.evict_all_clients_add_to_blocklist.py
      comments: BZ 2160855
      polarion-id: CEPH-83592513
      desc: Verify Client evict -h,--help works without running the function
      abort-on-fail: false
  - test:
      name: clone_no_wait_if_max_concurrent
      module: cephfs_bugs.clone_no_wait_if_max_concurrent.py
      polarion-id: CEPH-83593402
      desc: Additional clone create not started if max concurrent clone creating and snapshot_clone_no_wait is True
      abort-on-fail: false
  - test:
      name: mds_cache_oversized_warning_counter
      module: cephfs_bugs.test_mds_cache_oversized_warning_counter.py
      polarion-id: CEPH-83594160
      desc: mds_cache_oversized_warning_counter
      abort-on-fail: false
  - test:
      name: mds_dir_max_commit_size
      module: cephfs_bugs.test_mds_dir_max_commit_size.py
      polarion-id: CEPH-83600107
      desc: mds_dir_max_commit_size
      abort-on-fail: false
  - test:
      name: fs_recovery_dir_rm
      module: cephfs_bugs.test_fs_recovery_dir_rm.py
      polarion-id: CEPH-83609202
      desc: fs_recovery_dir_rm
      abort-on-fail: false
  - test:
      name: volume_ops_parallel_to_clone
      module: cephfs_bugs.test_volume_ops_parallel_to_clone.py
      polarion-id: CEPH-83593402
      desc: volume_ops_parallel_to_clone
      abort-on-fail: false
