#===============================================================================================
# Tier-level: Upstream - Core Functionality
# Test-Suite:   suites/tentacle/cephfs/upstream_cephfs_core_functionality.yaml
# Test-Case:    conf/tentacle/cephfs/tier-2_cephfs_7-node-cluster.yaml
#
# Cluster Configuration:
#    No of Clusters : 1
#     Node1 - Admin, Mon, Mgr, Installer, MDS
#     Node2 - Mon, Mgr,
#     Node3 - Mon, MDS
#     Node4 - OSD, MDS, SMB
#     Node5 - OSD, MDS, SMB
#     Node6 - OSD, MDS, NFS
#     Node7 - OSD, MDS, NFS
#     Node8 - Client
#     Node9 - Client
#     Node10 - Client
#     Node11 - Client
#===============================================================================================
---
tests:
  -
    test:
      abort-on-fail: true
      desc: "Setup phase to deploy the required pre-requisites for running the tests."
      module: install_prereq.py
      name: "setup install pre-requisistes"
  -
    test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                mon-ip: node1
                orphan-initial-daemons: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: shell
              args:          # arguments to ceph orch
                - ceph
                - fs
                - volume
                - create
                - cephfs
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - create
                - cephfs-data-ec
                - "64"
                - erasure
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - create
                - cephfs-metadata
                - "64"
          - config:
              command: shell
              args:
                - ceph
                - osd
                - pool
                - set
                - cephfs-data-ec
                - allow_ec_overwrites
                - "true"
          - config:
              command: shell
              args: # arguments to ceph orch
                - ceph
                - fs
                - new
                - cephfs-ec
                - cephfs-metadata
                - cephfs-data-ec
                - --force
          - config:
              command: apply
              service: mds
              base_cmd_args:          # arguments to ceph orch
                verbose: true
              pos_args:
                - cephfs              # name of the filesystem
              args:
                placement:
                  label: mds
          - config:
              args:
                - ceph
                - fs
                - set
                - cephfs
                - max_mds
                - "2"
              command: shell
      destroy-cluster: false
      abort-on-fail: true
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.1
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node8
      desc: "Configure the Cephfs client system 1"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.2
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node9
      desc: "Configure the Cephfs client system 2"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.3
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node10
      desc: "Configure the Cephfs client system 3"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.4
        install_packages:
          - ceph-common
          - ceph-fuse
        node: node11
      desc: "Configure the Cephfs client system 4"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"

  - test:
        name: cephfs_tier1_ops
        module: cephfs_tier1_ops.py
        polarion-id: CEPH-83573447
        desc: cephfs tier1 operations
        abort-on-fail: false
  - test:
      name: multiple clients run IO's on same directory from each clients and exersize POSIX locks
      module: clients.multiple_clients_posix_calls.py
      polarion-id: CEPH-10529
      desc: multiple clients exersizing POSIX locks
      abort-on-fail: false
  - test:
      name: Client File locking on mounts
      module: clients.file_lock_on_mounts.py
      polarion-id: CEPH-11304
      desc: Test File locking on mounts
      abort-on-fail: false
  - test:
      name: Client eviction
      module: clients.client_evict.py
      polarion-id: CEPH-11335
      desc: Test Filesystem client eviction
      abort-on-fail: false
  - test:
      abort-on-fail: false
      desc: "MDS failover while IO is going on each client"
      module: clients.MDS_failover_while_client_IO.py
      polarion-id: CEPH-11242
      config:
        num_of_file_dir: 1000
      name: "MDS failover whi client IO"
  - test:
      abort-on-fail: false
      desc: "Validate Root Sqaush operations on Cephfs"
      module: clients.validate_root_squash.py
      name: Validate Root Sqaush operations on Cephfs
      comments: "BZ-2293943"
      polarion-id: "CEPH-83591419"
  -
    test:
      abort-on-fail: true
      desc: "test cephfs nfs export path"
      module: cephfs_nfs.nfs_export_path.py
      name: "cephfs nfs export path"
      polarion-id: "CEPH-83574028"
  -
    test:
      abort-on-fail: false
      desc: "test cephfs nfs snapshot clone operations"
      module: cephfs_nfs.nfs_snaphshot_clone.py
      name: "cephfs nfs snapshot clone operations"
      polarion-id: "CEPH-83574024"
  -
    test:
      abort-on-fail: false
      desc: "test cephfs nfs subvolume & subvolumegroup operations"
      module: cephfs_nfs.nfs_subvolume_subvolumegroup.py
      name: "cephfs nfs subvolume & subvolumegroup operations"
      polarion-id: "CEPH-83574027"
  -
    test:
      abort-on-fail: false
      desc: "Run IOs by mounting same subvolume with all the three mounts"
      module: cephfs_nfs.nfs_fuse_kernel_same_subvolume.py
      name: "Mount same volume on fuse,kernel and nfs and runIOs"
      polarion-id: "CEPH-11310"
  - test:
      abort-on-fail: false
      desc: "test cephfs nfs with io and network failures"
      module: cephfs_nfs.nfs_io_network_failures.py
      name: "cephfs nfs with io and network failures"
      polarion-id: "CEPH-83574020"
  - test:
      name: snap_schedule_with_mds_restart
      module: snapshot_clone.snap_schedule_with_mds_restart.py
      polarion-id: CEPH-83600860
      desc: Validate Verify Kernel and FUSE Mount Behavior with Snapshot Scheduling and MDS Restarts
      abort-on-fail: false
  - test:
      name: snap_schedule_retention_vol_subvol
      module: snapshot_clone.snap_schedule_retention_vol_subvol.py
      polarion-id: CEPH-83579271
      desc: snap schedule and retention functional test on vol and subvol
      abort-on-fail: false
      config:
        test_name : functional
  - test:
      name: snapshot_metadata
      module: snapshot_clone.snapshot_metadata.py
      polarion-id: CEPH-83575038
      desc: verify CRUD operation on metadata of subvolume's snapshot
      abort-on-fail: false
  - test:
      name: cancel the subvolume snapshot clonning
      module: snapshot_clone.clone_cancel_in_progress.py
      polarion-id: CEPH-83574681
      desc: Try to cancel the snapshot while clonning is operating
      abort-on-fail: false
  - test:
      name: cross_platform_snaps
      module: snapshot_clone.cross_platform_snaps.py
      polarion-id: CEPH-11319
      desc: Clone a subvolume and remove the orginal volume and verify the contents in subvolume
      abort-on-fail: false
  - test:
      name: Clone_cancel_status
      module: snapshot_clone.clone_cancel_status.py
      polarion-id: CEPH-83573502
      desc: Checks the clone status and states of the clone process
      abort-on-fail: false
  - test:
      name: Subvolume Resize
      module: cephfs_vol_management.cephfs_vol_mgmt_subvolume_resize.py
      polarion-id: CEPH-83574193
      desc: subvolume resize
      abort-on-fail: false
  - test:
      name: volume_permission_test
      module: cephfs_vol_management.cephfs_vol_mgmt_volume_permissions.py
      polarion-id: CEPH-83574190
      desc: volume_permission_test
      abort-on-fail: false
  - test:
      name: subvolume_isolated_namespace
      module: cephfs_vol_management.cephfs_vol_mgmt_subvolume_isolated_namespace.py
      polarion-id: CEPH-83574187
      desc: subvolume_isolated_namespace
      abort-on-fail: false
  - test:
      name: cephfs_vol_mgmt_fs_life_cycle
      module: cephfs_vol_management.cephfs_vol_mgmt_fs_life_cycle.py
      polarion-id: CEPH-11333
      desc: File system life cycle
      abort-on-fail: false
  - test:
      abort-on-fail: false
      desc: "MON node power failure, with client IO"
      module: cephfs_system.mon_failure_with_client_IO.py
      polarion-id: CEPH-11261
      name: "MON node power failure, with client IO"
      config:
          num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "MDS node power failure, with client IO"
      module: cephfs_system.mds_failure_with_client_IO.py
      polarion-id: CEPH-11263
      name: "MDS node power failure, with client IO"
      config:
        num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "OSD node power failure, with client IO"
      module: cephfs_system.osd_failure_with_client_IO.py
      polarion-id: CEPH-11262
      name: "OSD node power failure, with client IO"
      config:
          num_of_osds: 12
  - test:
      abort-on-fail: false
      desc: "mds_nfs_node_failure_ops"
      module: cephfs_system.mds_nfs_node_failure_ops.py
      polarion-id: CEPH-11311
      name: "mds_nfs_node_failure_ops.py"
  - test:
      name: CephFS-IO Read and Write from multiple different clients
      module: cephfs_perf.cephfs_io_read_write_from_multiple_clients.py
      polarion-id: CEPH-11221
      desc: Mount CephFS on multiple clients, perform IO ,fill cluster upto 30%, read and write from multiple clients
      abort-on-fail: false
  - test:
      name: cephfs-stressIO
      module: cephfs_perf.cephfs_stress_io_from_multiple_clients.py
      polarion-id: CEPH-11222
      config:
        num_of_osds: 12
      desc: Mount CephFS on multiple clients,
      abort-on-fail: false
  - test:
      name: Directory pinning on two MDss with max:min number of directories
      module: cephfs_mds_pinning.mds_pinning_max_min_dir_on_two_mdss.py
      config:
        num_of_dirs: 100
      polarion-id: CEPH-11228
      desc: MDSfailover on active-active mdss,performing client IOs with max:min directory pinning with 2 active mdss
      abort-on-fail: false
  - test:
      name: Subtree Split and Subtree merging by pinning Subtrees directories to MDS.
      module: cephfs_mds_pinning.mds_pinning_split_merge.py
      polarion-id: CEPH-11233
      desc: Subtree Split and Subtree merging by pinning Subtrees directories to MDS.
      abort-on-fail: false
  - test:
      name: map and unmap directory trees to a mds rank
      module: cephfs_mds_pinning.map_and_unmap_directory_trees_to_a_mds_rank.py
      polarion-id: CEPH-83574329
      desc: map and unmap directory trees to a mds rank
      abort-on-fail: false

  -
    test:
      abort-on-fail: false
      desc: "Verify quiesce suceeds with IO from nfs,fuse and kernel mounts"
      destroy-cluster: false
      module: snapshot_clone.cg_snap_test.py
      name: "cg_snap_interop_workflow_2"
      polarion-id: CEPH-83591508
      config:
       test_name: cg_snap_interop_workflow_2
  -
    test:
      abort-on-fail: false
      desc: "Verify quiesce lifecycle with and without --await"
      destroy-cluster: false
      module: snapshot_clone.cg_snap_test.py
      name: "cg_snap_func_workflow_1"
      polarion-id: CEPH-83581467
      config:
       test_name: cg_snap_func_workflow_1
  - test:
      name: MDS client metrics functional
      module: cephfs_metrics.cephfs_metrics_functional.py
      polarion-id: CEPH-83588303
      desc: Verify MDS client metrics
      abort-on-fail: false
  - test:
      name: MDS client metrics negative case
      module: cephfs_metrics.cephfs_client_metrics_negative_case.py
      polarion-id: CEPH-83588357
      desc: Reboot MDS node and verify the metrics
      abort-on-fail: false

  - test:
      name: test cephfs attributes - functional
      module: cephfs_case_sensitivity.case_sensitivity_functional.py
      polarion-id: CEPH-83606639
      desc: Test fs attributes for functional use case
      abort-on-fail: false
  - test:
      name: test cephfs attributes - negative
      module: cephfs_case_sensitivity.case_sensitivity_negative.py
      polarion-id: CEPH-83611927
      desc: Test fs attributes for negative use case
      abort-on-fail: false
  - test:
      abort-on-fail: false
      desc: "Verify fscrypt functional tests - fscrypt_enctag,fscrypt_datapath,fscrypt_xttrs,fscrypt_file_types"
      destroy-cluster: false
      module: cephfs_fscrypt.test_fscrypt_functional.py
      name: "fscrypt_functional"
      polarion-id: CEPH-83619943
  - test:
      abort-on-fail: false
      desc: "Verify fscrypt negative tests - filename_len,key_remove,key_remove_diff_clients,wrong_key_msg_validate"
      destroy-cluster: false
      module: cephfs_fscrypt.test_fscrypt_negative.py
      name: "fscrypt_negative"
      polarion-id: CEPH-83607406
