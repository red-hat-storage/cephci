#===============================================================================================
# Tier-level: Upstream - CephFS Mirroring + Core Functionality
# Test-Suite:   suites/tentacle/cephfs/upstream_cephfs_mirror_core_functionality.yaml
# Test-Case:    conf/tentacle/cephfs/tier-1_cephfs_mirror.yaml
#
# Cluster Configuration:
#    No of Clusters : 2
#    Cluster 1 :
#     Node1 - Admin, Mon, Mgr, Installer, MDS
#     Node2 - Mon, Mgr, MDS
#     Node3 - Mon, OSD, MDS
#     Node4 - OSD, MDS, NFS
#     Node5 - OSD, MDS, NFS
#     Node6 - MDS, CephFS Mirror
#     Node7 - Client
#    Cluster 2 :
#     Node1 - Admin, Mon, Mgr, Installer, MDS
#     Node2 - Mon, Mgr, MDS
#     Node3 - Mon, OSD, MDS
#     Node4 - OSD, MDS, NFS
#     Node5 - OSD, MDS, NFS
#     Node6 - Client

#===============================================================================================
tests:
  - test:
      name: setup install pre-requisistes
      desc: Setup phase to deploy the required pre-requisites for running the tests.
      module: install_prereq.py
      abort-on-fail: true
  - test:
      abort-on-fail: true
      clusters:
        ceph1:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: shell
                  args: # arguments to ceph orch
                    - "ceph fs volume create cephfs"
              - config:
                  command: apply
                  service: mds
                  base_cmd_args: # arguments to ceph orch
                    verbose: true
                  pos_args:
                    - cephfs                        # name of the filesystem
                  args:
                    placement:
                      nodes:
                        - node4
                        - node5
              - config:
                  command: apply
                  service: cephfs-mirror
                  args:
                    placement:
                      nodes:
                        - node6
        ceph2:
          config:
            verify_cluster_health: true
            steps:
              - config:
                  command: bootstrap
                  service: cephadm
                  args:
                    registry-url: registry.redhat.io
                    mon-ip: node1
                    orphan-initial-daemons: true
                    skip-monitoring-stack: true
              - config:
                  command: add_hosts
                  service: host
                  args:
                    attach_ip_address: true
                    labels: apply-all-labels
              - config:
                  command: apply
                  service: mgr
                  args:
                    placement:
                      label: mgr
              - config:
                  command: apply
                  service: mon
                  args:
                    placement:
                      label: mon
              - config:
                  command: apply
                  service: osd
                  args:
                    all-available-devices: true
              - config:
                  command: shell
                  args:                             # arguments to ceph orch
                    - "ceph fs volume create cephfs"
              - config:
                  command: apply
                  service: mds
                  base_cmd_args:                    # arguments to ceph orch
                    verbose: true
                  pos_args:
                    - cephfs                        # name of the filesystem
                  args:
                    placement:
                      nodes:
                        - node4
                        - node5
      desc:  CephFS Mirror cluster deployment using cephadm
      destroy-clster: false
      module: test_cephadm.py
      polarion-id: CEPH-83574114
      name: deploy cephfs-mirror
  - test:
        abort-on-fail: true
        clusters:
          ceph1:
            config:
              command: add
              copy_admin_keyring: true
              id: client.1
              install_packages:
                - ceph-common
                - ceph-fuse
              node: node7
          ceph2:
            config:
              command: add
              copy_admin_keyring: true
              id: client.1
              install_packages:
                - ceph-common
                - ceph-fuse
              node: node6
        desc: "Configure the Cephfs client system 1"
        destroy-cluster: false
        module: test_client.py
        name: "configure client"
  - test:
      abort-on-fail: false
      desc: "Configure CephFS Mirroring"
      clusters:
        ceph1:
          config:
            name: Validate the Synchronisation is successful upon enabling fs mirroring
      module: cephfs_mirroring.test_cephfs_mirroring_configure_cephfs_mirroring.py
      polarion-id: "CEPH-83574099"
  - test:
      abort-on-fail: false
      desc: "Configure CephFS Mirroring modification of source and target cluster"
      clusters:
        ceph1:
          config:
            name: Configure CephFS Mirroring modification of source and target cluster
      module: cephfs_mirroring.validate_file_dir_stats_modification_on_mirrored_cluster.py
      polarion-id: "CEPH-83575625"
  - test:
      abort-on-fail: false
      desc: "Configure CephFS Mirroring on multiple FS"
      clusters:
        ceph1:
          config:
            name: Validate the Mirroring is successful upon enabling fs mirroring on multiple FS from source to same destinatio
      module: cephfs_mirroring.test_cephfs_mirroring_validate_cephfs-mirroring_on_multifs_setup.py
      polarion-id: "CEPH-83574107"
  - test:
      abort-on-fail: false
      desc: "modify User caps and validate the mirroring sync."
      clusters:
        ceph1:
          config:
            name: "modify User caps and validate the mirroring sync."
      module: cephfs_mirroring.test_cephfs_mirror_auth_caps.py
      polarion-id: "CEPH-83574109"
  - test:
      abort-on-fail: false
      desc: "Modify the Remote Snap Directories"
      clusters:
        ceph1:
          config:
            name: Modify the Remote Snap Directories
      module: cephfs_mirroring.test_cephfs_mirroring_modify_remote_snapshots.py
      polarion-id: "CEPH-83574120"
  - test:
      abort-on-fail: false
      desc: "Convert single node to HA configuration"
      clusters:
        ceph1:
          config:
            name: Convert single node to HA configuration
      module: cephfs_mirroring.test_cephfs_mirror_ha_conversion.py
      polarion-id: "CEPH-83575340"
  - test:
      abort-on-fail: false
      desc: "Validate all failure scenarios to disconnect the mirroring"
      clusters:
        ceph1:
          config:
            name: Validate all failure scenarios to disconnect the mirroring
      module: cephfs_mirroring.test_cephfs_mirror_disconnect.py
      polarion-id: "CEPH-83574100"
  - test:
      abort-on-fail: false
      desc: "Validate multiple snapshots Synchronization on Mirror setup"
      clusters:
        ceph1:
          config:
            name: Validate multiple snapshots Synchronization on Mirror setup
      module: cephfs_mirroring.test_cephfs_mirroring_multiple_snapshots.py
      polarion-id: "CEPH-83580026"
  - test:
      abort-on-fail: false
      desc: "Validate Snapshot Sync status using asok file"
      clusters:
        ceph1:
          config:
            name: Validate the Synchronisation Status using asok file
      module: cephfs_mirroring.test_cephfs_mirroring_snap_status_using_asok.py
      polarion-id: "CEPH-83593134"
  - test:
      abort-on-fail: false
      desc: "Validate snapshot synchronisation using SnapSchedule"
      clusters:
        ceph1:
          config:
            name: Validate snapshot synchronisation using SnapSchedul
      module: cephfs_mirroring.test_cephfs_mirroring_with_snap_schedule.py
      polarion-id: "CEPH-83598968"
  - test:
      abort-on-fail: false
      desc: "cephfs basic operations"
      clusters:
        ceph1:
          config:
            name: "cephfs basic operations"
      module: cephfs_basic_tests.py
      polarion-id: "CEPH-11293"
  - test:
      desc: cephfs_snapshot_management
      module: cephfs_snapshot_management.py
      clusters:
        ceph1:
          config:
            name: "cephfs_snapshot_management.py"
      polarion-id: CEPH-83571366
      abort-on-fail: false
  - test:
      clusters:
        ceph1:
          config:
            name: "Clone Operations"
      module: bug-1980920.py
      desc: Cancel the clone operation and validate error message
      polarion-id: CEPH-83574681
      abort-on-fail: false
  - test:
      clusters:
        ceph1:
          config:
            name: "nfs-ganesha_with_cephfs"
      module: nfs-ganesha_basics.py
      desc: Configure nfs-ganesha on nfs server,do mount on any client and do IOs
      polarion-id: CEPH-83574439
      abort-on-fail: false
  - test:
      clusters:
        ceph1:
          config:
            name: "Run fsstress on kernel and fuse mounts"
      module: cephfs_bugs.fsstress_kernel_verification.py
      polarion-id: CEPH-83575623
      desc: Run fsstress on kernel and fuse mounts
      abort-on-fail: false
  - test:
      clusters:
        ceph1:
          config:
            name: "Run xfs test on kernel"
      module: xfs_test.py
      polarion-id: CEPH-83575623
      desc: Run xfs test on kernel
      abort-on-fail: false
  - test:
       clusters:
         ceph1:
           config:
             name: "Files-quota-test"
       module: quota.quota_files.py
       polarion-id: CEPH-83573399
       desc: Tests the file attributes on the directory
       abort-on-fail: false
  - test:
       clusters:
         ceph1:
           config:
             name: "Bytes-quota-test"
       module: quota.quota_bytes.py
       polarion-id: CEPH-83573399
       desc: Tests the Byte attr
  - test:
       clusters:
         ceph1:
           config:
             name: "Running basic bash commands on fuse,Kernel and NFS mounts"
       module: clients.fs_basic_bash_cmds.py
       polarion-id: CEPH-11300
       desc: Running basic bash commands on fuse,Kernel and NFS mounts
       abort-on-fail: false
       config:
         no_of_files: 1000
         size_of_files: 1
         num_dir: 100
  - test:
      abort-on-fail: false
      desc: "Running basic bash commands on fuse,Kernel and NFS mounts"
      clusters:
        ceph1:
          config:
            name: "Running basic bash commands on fuse,Kernel and NFS mounts"
      module: clients.verify_root_squash_in_caps.py
      polarion-id: "CEPH-83573868"
  - test:
      desc: Create 4-5 Filesystem randomly on different MDS daemons
      abort-on-fail: false
      clusters:
        ceph1:
          config:
            name: "creation of multiple file systems"
      module: multifs.multifs_multiplefs.py
      polarion-id: CEPH-83573867
  - test:
      desc: cephfs-top metrcis verification using --dump
      abort-on-fail: false
      clusters:
        ceph1:
          config:
            name: "cephfs-top metrcis verification using --dump"
      module: cephfs_top.cephfs_top_dump.py
      polarion-id: CEPH-83573848
  - test:
      abort-on-fail: false
      clusters:
        ceph1:
          config:
            name: "Taking the cephfs down with down flag"
      desc: "Taking the cephfs down with down flag"
      module: cephfs_recovery.fs_down_flag.py
      polarion-id: CEPH-83573264
      comments: "Product bug - BZ-2225891"
  - test:
      abort-on-fail: false
      desc: "Execute FS Scrub Commands on cephfs"
      clusters:
        ceph1:
          config:
            name: "Execute FS Scrub Commands on cephfs"
      module: cephfs_recovery.fs_scrub.py
      polarion-id: CEPH-11267
