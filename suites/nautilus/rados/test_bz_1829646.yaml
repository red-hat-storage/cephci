tests:
   - test:
       name: install ceph pre-requisites
       module: install_prereq.py
       abort-on-fail: true

   - test:
       name: ceph ansible
       polarion-id: CEPH-83571467
       module: test_ansible.py
       config:
         is_mixed_lvm_configs: True
         ansi_config:
           ceph_test: True
           ceph_origin: distro
           ceph_stable_release: nautilus
           ceph_repository: rhcs
           osd_scenario: lvm
           journal_size: 1024
           ceph_stable: True
           ceph_stable_rh_storage: True
           fetch_directory: ~/fetch
           copy_admin_key: true
           dashboard_enabled: False
           ceph_conf_overrides:
             global:
               osd_pool_default_pg_num: 64
               osd_default_pool_size: 2
               osd_pool_default_pgp_num: 64
               mon_max_pg_per_osd: 1024
             mon:
               mon_allow_pool_delete: true
             client:
               rgw crypt require ssl: false
               rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                 testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
           cephfs_pools:
             - name: "cephfs_data"
               pgs: "8"
             - name: "cephfs_metadata"
               pgs: "8"
       desc: osd with lvm scenario
       destroy-cluster: False
       abort-on-fail: true

   - test:
      name: auto trimming of ceph osd maps
      module: test_bz_1829646.py
      abort-on-fail: true