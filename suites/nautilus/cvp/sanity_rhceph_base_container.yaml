tests:
  - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

  - test:
       name: containerized ceph ansible
       polarion-id: CEPH-83571503
       module: test_ansible.py
       config:
         is_mixed_lvm_configs: True
         ansi_config:
             ceph_test: True
             ceph_origin: distro
             ceph_stable_release: nautilus
             ceph_repository: rhcs
             osd_scenario: lvm
             osd_auto_discovery: False
             journal_size: 1024
             ceph_stable: True
             ceph_stable_rh_storage: True
             ceph_docker_image: rhceph/rhceph-3-rhel7
             ceph_docker_image_tag: latest
             containerized_deployment: true
             ceph_docker_registry: registry.access.redhat.com
             copy_admin_key: true
             dashboard_enabled: False
             ceph_conf_overrides:
                 global:
                   osd_pool_default_pg_num: 64
                   osd_default_pool_size: 2
                   osd_pool_default_pgp_num: 64
                   mon_max_pg_per_osd: 1024
                 mon:
                   mon_allow_pool_delete: true
                 client:
                   rgw crypt require ssl: false
                   rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                     testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
             cephfs_pools:
               - name: "cephfs_data"
                 pgs: "8"
               - name: "cephfs_metadata"
                 pgs: "8"
       desc: osd with collocated journal
       destroy-cluster: False
       abort-on-fail: true

  - test:
     name: check-ceph-health
     module: exec.py
     config:
           cmd: ceph -s
           sudo: True
     desc: Check for ceph health debug info
# basic ansible tests
  - test:
     name: config roll over
     polarion-id: CEPH-9581
     module: test_ansible_roll_over.py
     config:
         is_mixed_lvm_configs: True
         ansi_config:
           ceph_test: True
           ceph_origin: distro
           ceph_stable_release: nautilus
           ceph_repository: rhcs
           osd_scenario: lvm
           osd_auto_discovery: False
           journal_size: 1024
           ceph_stable: True
           ceph_stable_rh_storage: True
           ceph_docker_image: rhceph/rhceph-3-rhel7
           ceph_docker_image_tag: latest
           containerized_deployment: true
           ceph_docker_registry: registry.access.redhat.com
           copy_admin_key: true
           dashboard_enabled: False
           ceph_conf_overrides:
               global:
                 osd_pool_default_pg_num: 64
                 osd_default_pool_size: 2
                 osd_pool_default_pgp_num: 64
                 mon_max_pg_per_osd: 1024
               mon:
                 mon_allow_pool_delete: true
               client:
                 rgw crypt require ssl: false
                 rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                   testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
           cephfs_pools:
             - name: "cephfs_data"
               pgs: "8"
             - name: "cephfs_metadata"
               pgs: "8"
         add:
             - node:
                 node-name: .*node10.*
                 demon:
                     - mon
     desc: add containerized monitor

  - test:
     name: config roll over
     polarion-id: CEPH-9583
     module: test_ansible_roll_over.py
     config:
         is_mixed_lvm_configs: True
         ansi_config:
           ceph_test: True
           ceph_origin: distro
           ceph_stable_release: nautilus
           ceph_repository: rhcs
           osd_scenario: lvm
           osd_auto_discovery: False
           journal_size: 1024
           ceph_stable: True
           ceph_stable_rh_storage: True
           ceph_docker_image: rhceph/rhceph-3-rhel7
           ceph_docker_image_tag: latest
           containerized_deployment: true
           ceph_docker_registry: registry.access.redhat.com
           copy_admin_key: true
           dashboard_enabled: False
           ceph_conf_overrides:
               global:
                 osd_pool_default_pg_num: 64
                 osd_default_pool_size: 2
                 osd_pool_default_pgp_num: 64
                 mon_max_pg_per_osd: 1024
               mon:
                 mon_allow_pool_delete: true
               client:
                 rgw crypt require ssl: false
                 rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                   testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
           cephfs_pools:
             - name: "cephfs_data"
               pgs: "8"
             - name: "cephfs_metadata"
               pgs: "8"
         add:
             - node:
                 node-name: .*node11.*
                 demon:
                     - osd
     desc: add new containerized osd node

  - test:
     name: config roll over
     polarion-id: CEPH-9582
     module: test_ansible_roll_over.py
     config:
         is_mixed_lvm_configs: True
         ansi_config:
           ceph_test: True
           ceph_origin: distro
           ceph_stable_release: nautilus
           ceph_repository: rhcs
           osd_scenario: lvm
           osd_auto_discovery: False
           journal_size: 1024
           ceph_stable: True
           ceph_stable_rh_storage: True
           ceph_docker_image: rhceph/rhceph-3-rhel7
           ceph_docker_image_tag: latest
           containerized_deployment: true
           ceph_docker_registry: registry.access.redhat.com
           copy_admin_key: true
           dashboard_enabled: False
           ceph_conf_overrides:
               global:
                 osd_pool_default_pg_num: 64
                 osd_default_pool_size: 2
                 osd_pool_default_pgp_num: 64
                 mon_max_pg_per_osd: 1024
               mon:
                 mon_allow_pool_delete: true
               client:
                 rgw crypt require ssl: false
                 rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                   testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
           cephfs_pools:
             - name: "cephfs_data"
               pgs: "8"
             - name: "cephfs_metadata"
               pgs: "8"
         add:
             - node:
                 node-name: .*node11.*
                 demon:
                     - osd
     desc: add containerized osd to existing node
# basic cephfs tests

  - test:
    name: cephfs-lifecycle ops
    module: CEPH-11333.py
    polarion-id: CEPH-11333
    desc: Perfrom cephfs lifecycle ops like delete cephfs,recreate cephfs
    abort-on-fail: false

  - test:
    name: multi-client-rw-io
    module: CEPH-10528_10529.py
    polarion-id: CEPH-10528,CEPH-10529
    desc: Single CephFS on multiple clients,performing IOs and checking file locking mechanism
    abort-on-fail: false

  - test:
    name: cephfs-basics
    module: cephfs_basic_tests.py
    polarion-id: CEPH-11293,CEPH-11296,CEPH-11297,CEPH-11295
    desc: cephfs basic operations
    abort-on-fail: false

 #  - test:
 #     name: ceph ansible purge
 #     polarion-id: CEPH-83571493
 #     module: purge_cluster.py
 #     config:
 #           ansible-dir: /usr/share/ceph-ansible
 #           playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
 #     desc: Purge ceph cluster
 #     destroy-cluster: True
