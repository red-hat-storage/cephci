#===============================================================================================
#-------------------------------------
#-----    Sanity Test suite     ------
#-------------------------------------
#===============================================================================================
tests:
  - test:
      abort-on-fail: true
      module: install_prereq.py
      name: "install ceph pre-requisites"
  - test:
      abort-on-fail: true
      config:
        ansi_config:
          alertmanager_container_image: "registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6"
          ceph_conf_overrides:
            client:
              rgw crypt require ssl: false
              rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            global:
              mon_max_pg_per_osd: 1024
              osd_default_pool_size: 2
              osd_pool_default_pg_num: 64
              osd_pool_default_pgp_num: 64
            mon:
              mon_allow_pool_delete: true
          ceph_docker_image: rhceph/rhceph-4-rhel8
          ceph_docker_image_tag: latest
          ceph_docker_registry: registry.redhat.io
          ceph_origin: distro
          ceph_repository: rhcs
          ceph_stable: true
          ceph_stable_release: nautilus
          ceph_stable_rh_storage: true
          ceph_test: true
          cephfs_pools:
            -
              name: cephfs_data
              pgs: "8"
            -
              name: cephfs_metadata
              pgs: "8"
          containerized_deployment: true
          copy_admin_key: true
          dashboard_admin_password: p@ssw0rd
          dashboard_admin_user: admin
          dashboard_enabled: true
          grafana_admin_password: p@ssw0rd
          grafana_admin_user: admin
          grafana_container_image: "registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4"
          journal_size: 1024
          node_exporter_container_image: "registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6"
          osd_auto_discovery: false
          osd_scenario: collocated
          prometheus_container_image: "registry.redhat.io/openshift4/ose-prometheus:v4.6"
      desc: "ceph deployment using site.yaml(container) playbook"
      destroy-cluster: false
      module: test_ansible.py
      name: "containerized ceph ansible"
      polarion-id: CEPH-83571503

  # Testing stage
  - test:
      name: Executes RGW, RBD and FS operations
      desc: Run object, block and filesystem basic operations parallelly.
      module: test_parallel.py
      parallel:
        - test:
            config:
              script-name: test_Mbuckets_with_Nobjects.py
              config-file-name: test_Mbuckets_with_Nobjects.yaml
              timeout: 300
            desc: test to create "M" no of buckets and "N" no of objects
            module: sanity_rgw.py
            name: Test M buckets with N objects
            polarion-id: CEPH-9789
        - test:
            config:
              branch: nautilus
              script_path: qa/workunits/rbd
              script: cli_generic.sh
            desc: Executig upstream RBD CLI Generic scenarios
            module: test_rbd.py
            name: 1_rbd_cli_generic
            polarion-id: CEPH-83574241
        - test:
            name: cephfs-basics
            module: cephfs_basic_tests.py
            polarion-id: CEPH-11293,CEPH-11296,CEPH-11297,CEPH-11295
            desc: cephfs basic operations
            abort-on-fail: false
