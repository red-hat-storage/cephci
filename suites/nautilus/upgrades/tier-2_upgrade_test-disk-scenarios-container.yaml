#############################################################################
# - Automation support for container based cluster with ceph disk osd scenario
#
# Cluster configuration: (conf/nautilus/upgrades/tier-2_upgrade.yaml)
# --------------------------------------------------------------------------
# Nodes:
#   - 3 MONS, 1 MGRs, 3 OSDs, 1 RGW, 2 ISCSIGW's, 2 MDS's, 1 NFS, 1 GRAFANA, 1 CLIENT
#
# Test steps:
# ---------------------------------------------------------------------
# - Deploy containerised Nautilus Ceph cluster using CDN RPMS with ceph disk osd scenario
# - Run some I/O's
# - Upgrade to Nautilus using rolling upgrade
#
#############################################################################
tests:
- test:
    name: install ceph pre-requisites
    module: install_prereq.py
    abort-on-fail: True

- test:
    name: ceph ansible install rhcs 3.x cdn
    module: test_ansible.py
    polarion-id: CEPH-83574033
    config:
      use_cdn: True
      build: '3.x'
      ansi_config:
        ceph_origin: repository
        ceph_repository: rhcs
        ceph_repository_type: cdn
        ceph_rhcs_version: 3
        osd_scenario: collocated
        osd_auto_discovery: False
        copy_admin_key: True
        containerized_deployment: true
        ceph_docker_image: "rhceph/rhceph-3-rhel7"
        ceph_docker_image_tag: "latest"
        ceph_docker_registry: "registry.redhat.io"
    desc: test cluster ceph-disk 3.x cdn setup using ceph-ansible
    abort-on-fail: True

- test:
      name: Parallel run
      module: test_parallel.py
      parallel:
        - test:
            name: librbd workunit
            module: test_workunit.py
            config:
              test_name: rbd/test_librbd_python.sh
              branch: nautilus
              role: mon
            desc: Test librbd unit tests
        - test:
            name: rados_bench_test
            module: radosbench.py
            config:
              pg_num: '128'
              pool_type: 'normal'
            desc: run rados bench for 360 - normal profile
        - test:
            name: Upgrade containerized ceph to 4.x latest
            polarion-id: CEPH-83574033
            module: test_ansible_upgrade.py
            config:
              build: '4.x'
              ansi_config:
                ceph_origin: distro
                ceph_stable_release: nautilus
                ceph_repository: rhcs
                ceph_rhcs_version: 4
                osd_scenario: lvm
                osd_auto_discovery: False
                ceph_stable: True
                ceph_stable_rh_storage: True
                fetch_directory: ~/fetch
                copy_admin_key: true
                containerized_deployment: true
                upgrade_ceph_packages: True
                dashboard_enabled: True
                dashboard_admin_user: admin
                dashboard_admin_password: p@ssw0rd
                grafana_admin_user: admin
                grafana_admin_password: p@ssw0rd
                node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
                grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
                prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
                alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
            desc: Ceph-Ansible container upgrade 3.x cdn to 4.x latest
            abort-on-fail: True

- test:
    name: librbd workunit
    module: test_workunit.py
    config:
      test_name: rbd/test_librbd_python.sh
      branch: nautilus
      role: mon
    desc: Test librbd unit tests

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      sudo: True
    desc: Check for ceph health debug info

- test:
    name: rados_bench_test
    module: radosbench.py
    config:
      pg_num: '128'
      pool_type: 'normal'
    desc: run rados bench for 360 - normal profile

