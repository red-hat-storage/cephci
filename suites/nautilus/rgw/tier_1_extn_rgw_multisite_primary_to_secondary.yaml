# Below are the multi-site test scenarios run on the master and verified the sync/io on the slave
# ceph-rgw1 is master/primary site
# ceph-rgw2 is slave/secondary site

tests:
  - test:
      name: pre-req
      module: install_prereq.py
      abort-on-fail: true
      desc: install ceph pre requisites

  - test:
      name: ceph ansible
      module: test_ansible.py
      clusters:
        ceph-rgw1:
          config:
            ansi_config:
              ceph_test: True
              ceph_origin: distro
              ceph_repository: rhcs
              osd_scenario: lvm
              osd_auto_discovery: False
              journal_size: 1024
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              dashboard_enabled: False
              rgw_multisite: true
              rgw_zone: US_EAST
              rgw_zonegroup: US
              rgw_realm: USA
              rgw_zonemaster: true
              rgw_zonesecondary: false
              rgw_zonegroupmaster: true
              rgw_zone_user: synchronization-user
              rgw_zone_user_display_name: "Synchronization User"
              rgw_multisite_proto: "http"
              system_access_key: 86nBoQOGpQgKxh4BLMyq
              system_secret_key: NTnkbmkMuzPjgwsBpJ6o
              ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
        ceph-rgw2:
          config:
            ansi_config:
              ceph_test: True
              ceph_origin: distro
              ceph_repository: rhcs
              osd_scenario: lvm
              osd_auto_discovery: False
              journal_size: 1024
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              dashboard_enabled: False
              rgw_multisite: true
              rgw_zone: US_WEST
              rgw_zonegroup: US
              rgw_realm: USA
              rgw_zonemaster: false
              rgw_zonesecondary: true
              rgw_zonegroupmaster: false
              rgw_zone_user: synchronization-user
              rgw_zone_user_display_name: "Synchronization User"
              system_access_key: 86nBoQOGpQgKxh4BLMyq
              system_secret_key: NTnkbmkMuzPjgwsBpJ6o
              rgw_multisite_proto: "http"
              rgw_pull_proto: http
              rgw_pull_port: 8080
              ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
      desc: setup multisite cluster using ceph-ansible
      abort-on-fail: true
  - test:
      name: create user
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      clusters:
        ceph-rgw1:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-rgw2
            timeout: 300

  # datlog omap offload tests
  - test:
      name: datalog omap offload
      desc: Execute datalog omap offload on primary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-rgw1:
          config:
            script-name: test_data_omap_offload.py
            config-file-name: test_data_omap_offload.yaml
            verify-io-on-site: ["ceph-rgw2"]
            timeout: 300

  - test:
      name: datalog omap offload
      desc: Execute datalog omap offload change default datatype to fifo on primary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-rgw1:
          config:
            script-name: test_data_omap_offload.py
            config-file-name: test_data_omap_offload_change_datatype_fifo.yaml
            verify-io-on-site: ["ceph-rgw2"]
            timeout: 300

  - test:
      name: datalog omap offload
      desc: Execute datalog omap offload with multipart objects on primary 
      module: sanity_rgw_multisite.py
      clusters:
        ceph-rgw1:
          config:
            script-name: test_data_omap_offload.py
            config-file-name: test_data_omap_offload_multipart.yaml
            verify-io-on-site: ["ceph-rgw2"]
            timeout: 300

  - test:
      name: datalog omap offload
      desc: Execute datalog omap offload change default datatype to omap on primary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-rgw1:
          config:
            script-name: test_data_omap_offload.py
            config-file-name: test_data_omap_offload_change_datatype_omap.yaml
            verify-io-on-site: ["ceph-rgw2"]
            timeout: 300

  - test:
      name: datalog omap offload
      desc: Execute datalog omap offload on versioned bucket on primary
      module: sanity_rgw_multisite.py
      clusters:
        ceph-rgw1:
          config:
            script-name: test_data_omap_offload.py
            config-file-name: test_data_omap_offload_versioned_bucket.yaml
            verify-io-on-site: ["ceph-rgw2"]
            timeout: 300
