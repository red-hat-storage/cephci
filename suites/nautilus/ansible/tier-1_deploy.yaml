#===============================================================================================
# Tier-level: 1
# Test-Suite: tier-1_deploy.yaml
# Test-Case: Deployment and rollover tests
#
# Cluster Configuration:
#    cephci/conf/nautilus/ansible/tier-1_deploy.yaml
#
#    7-Node cluster(RHEL-7.9 and above)
#    3 MONS, 2 MDS, 1 MGR, 3 OSD and 2 RGW service daemon(s)
#     Node1 - Mon, Mgr, Installer, OSD
#     Node2 - Mon, Mgr, OSD
#     Node3 - Mon, OSD
#     Node4 - Client, Grafana
#     Node5 - Pool , used for roll-over test case
#     Node6 - Pool , used for roll-over test case
#     Node7 - Pool , used for roll-over test case
#
# Test Steps:
#   (1) Install Pre-requisites, and Deploy Ceph using ceph-ansible
#   (2) Use pool nodes to add/remove MON, OSD, RGW and MDS using roll-over playbook.
#   (3) Playbook Switch RPM to containerized
#   (4) Playbook Docker to Podman (Since RHEL 7.9 should be used here)
#===============================================================================================
tests:
   - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571467
      module: test_ansible.py
      config:
        is_mixed_lvm_configs: True
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: nautilus
            ceph_repository: rhcs
            osd_scenario: lvm
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_docker_image: rhceph/rhceph-4-rhel8
            ceph_docker_image_tag: latest
            ceph_docker_registry: registry.redhat.io
            dashboard_enabled: True
            dashboard_admin_user: admin
            dashboard_admin_password: p@ssw0rd
            grafana_admin_user: admin
            grafana_admin_password: p@ssw0rd
            node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
            grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
            prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
            alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with 6 osd scenarios with lvm
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: config roll over mon daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          is_mixed_lvm_configs: True
          add:
              - node:
                  node-name: .*node5.*
                  daemon:
                      - mon
      desc: add mon
      abort-on-fail: true

   - test:
      name: shrink mon
      polarion-id: CEPH-9584
      module: shrink_mon.py
      config:
           mon-to-kill:
            - .*node3.*
      desc: remove monitor
      abort-on-fail: true

   - test:
      name: config roll over new osd daemon
      polarion-id: CEPH-9583
      module: test_ansible_roll_over.py
      abort-on-fail: true
      config:
          is_mixed_lvm_configs: True
          add:
              - node:
                  node-name: .*node6.*
                  daemon:
                      - osd
      desc: add new osd node with lvm scenario


   - test:
      name: config roll over rgw daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      abort-on-fail: true
      config:
          is_mixed_lvm_configs: True
          add:
              - node:
                  node-name: .*node6.*
                  daemon:
                      - rgw
      desc: add rgw

   - test:
      name: config roll over mds daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      abort-on-fail: true
      config:
          is_mixed_lvm_configs: True
          add:
              - node:
                  node-name: .*node7.*
                  daemon:
                      - mds
      desc: add mds

   - test:
      name: switch-from-non-containerized-to-containerized-ceph-daemons
      polarion-id: CEPH-83573510
      module: switch_rpm_to_container.py
      abort-on-fail: true

   - test:
       name: docker to podman with osd with collocated journal
       polarion-id: CEPH-83573538
       abort-on-fail: true
       module: docker_to_podman.py
       desc: docker to podman with osd with collocated journal
